{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPWTAEq_aunE"
   },
   "outputs": [],
   "source": [
    "#options: cc200, dosenbach160, aal, cc400 , power264\n",
    "p_ROI = \"cc400\"\n",
    "p_fold = 10\n",
    "p_metadata_feature = 5\n",
    "p_center = \"Caltech\"\n",
    "p_mode = \"whole\"\n",
    "p_augmentation = True\n",
    "p_selective_regs = False\n",
    "p_metadata = False\n",
    "p_Method = \"DNN\"\n",
    "bad_samples = ['UM_1_0050315', 'Pitt_0050026', 'NYU_0050983', 'OHSU_0050150', 'Trinity_0050247', 'NYU_0050955', 'NYU_0050974', 'Leuven_1_0050694', 'USM_0050514', 'UCLA_1_0051234', 'Olin_0050132', 'MaxMun_b_0051325', 'UCLA_2_0051299', 'NYU_0051027', 'SBL_0051584', 'UCLA_1_0051205', 'UM_2_0050412', 'SBL_0051576', 'Caltech_0051465', 'Yale_0050626', 'Yale_0050619', 'NYU_0050956', 'NYU_0051007', 'OHSU_0050144', 'NYU_0050987', 'UM_2_0050402', 'NYU_0051003', 'SBL_0051572', 'Olin_0050134', 'NYU_0050966', 'NYU_0051029', 'NYU_0051020', 'USM_0050482', 'NYU_0051035', 'MaxMun_a_0051606', 'UM_1_0050272', 'OHSU_0050162', 'USM_0050468', 'Olin_0050107', 'NYU_0051147', 'Caltech_0051491', 'Leuven_2_0050740', 'NYU_0051077', 'NYU_0051090', 'MaxMun_a_0051373', 'CMU_b_0050669', 'Yale_0050568', 'Stanford_0051180', 'NYU_0051149', 'Stanford_0051192', 'UM_1_0050351', 'NYU_0051088', 'UCLA_1_0051256', 'Olin_0050117', 'UM_1_0050362', 'UM_1_0050355', 'UM_2_0050422', 'MaxMun_d_0051360', 'MaxMun_c_0051333', 'Leuven_2_0050731', 'NYU_0051154', 'UCLA_1_0051279', 'USM_0050433', 'SDSU_0050204', 'OHSU_0050158', 'NYU_0051082', 'UCLA_2_0051311', 'NYU_0051151', 'USM_0050447', 'KKI_0050774', 'CMU_a_0050659', 'Yale_0050561', 'UCLA_1_0051262', 'Pitt_0050039', 'Yale_0050552']\n",
    "useless_samples = ['NYU_0050977', 'NYU_0050995', 'MaxMun_a_0051321', 'UCLA_1_0051241', 'NYU_0050985', 'NYU_0051020', 'Trinity_0050232', 'MaxMun_d_0051349', 'Leuven_1_0050694', 'Trinity_0050250', 'Olin_0050130', 'UM_1_0050294', 'Leuven_1_0050690', 'UM_2_0050408', 'Leuven_1_0050696', 'SBL_0051579', 'UM_2_0050405', 'MaxMun_d_0051351', 'NYU_0050981', 'Leuven_1_0050705', 'Stanford_0051171', 'MaxMun_a_0051318', 'NYU_0050961', 'Pitt_0050008', 'Leuven_1_0050704', 'MaxMun_a_0051607', 'Leuven_2_0050756', 'Pitt_0050022', 'Pitt_0050057', 'USM_0050496', 'Pitt_0050016', 'UCLA_1_0051208', 'NYU_0051008', 'UCLA_1_0051214', 'SDSU_0050188', 'Caltech_0051465', 'Yale_0050623', 'UCLA_1_0051220', 'MaxMun_b_0051325', 'SDSU_0050182', 'Leuven_2_0050751', 'UM_1_0050285', 'Trinity_0050253', 'OHSU_0050153', 'Leuven_2_0050757', 'Stanford_0051163', 'NYU_0051035', 'Yale_0050619', 'MaxMun_d_0051329', 'NYU_0050966', 'KKI_0050824', 'UM_1_0050319', 'Leuven_2_0050754', 'SBL_0051580', 'Pitt_0050023', 'OHSU_0050145', 'USM_0050485', 'Pitt_0050005', 'UM_1_0050272', 'Trinity_0050251', 'UM_1_0050297', 'Pitt_0050026', 'Yale_0050601', 'CMU_b_0050648', 'SBL_0051573', 'NYU_0051018', 'UCLA_1_0051224', 'SDSU_0050187', 'SBL_0051582', 'NYU_0050991', 'SBL_0051576', 'Trinity_0050254', 'Trinity_0050237', 'Leuven_1_0050708', 'UCLA_1_0051248', 'USM_0050527', 'NYU_0051034', 'CMU_a_0050654', 'NYU_0051025', 'Leuven_2_0050748', 'UCLA_1_0051226', 'Leuven_2_0050755', 'UM_1_0050287', 'Caltech_0051463', 'Leuven_1_0050711', 'Leuven_1_0050693', 'NYU_0051006', 'USM_0050507', 'Leuven_1_0050697', 'Leuven_2_0050745', 'Yale_0050612', 'USM_0050499', 'Stanford_0051168', 'KKI_0050798', 'UCLA_1_0051230', 'KKI_0050801', 'NYU_0050954', 'SBL_0051572', 'SBL_0051585', 'Pitt_0050012', 'UM_1_0050293', 'OHSU_0050146', 'MaxMun_a_0051606', 'UM_1_0050295', 'MaxMun_b_0051322', 'UCLA_1_0051237', 'CMU_a_0050642', 'NYU_0050982', 'NYU_0051027', 'NYU_0050992', 'Caltech_0051473', 'NYU_0051029', 'Olin_0050127', 'Pitt_0050053', 'UM_1_0050315', 'SDSU_0050183', 'Leuven_2_0050752', 'USM_0050490', 'NYU_0050960', 'NYU_0050955']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhjFE8C9cNh_",
    "outputId": "86364ff0-942d-4e71-cfd9-cd78b49452ff"
   },
   "outputs": [],
   "source": [
    "parameter_list = [p_ROI,p_fold,p_center,p_mode,p_augmentation,p_Method]\n",
    "print(\"*****List of patameters****\")\n",
    "print(\"ROI atlas: \",p_ROI)\n",
    "print(\"per Center or whole: \",p_mode)\n",
    "if p_mode == 'percenter':\n",
    "    print(\"Center's name: \",p_center)\n",
    "print(\"Method's name: \",p_Method)\n",
    "if p_Method == \"ASD-DiagNet\":\n",
    "    print(\"Augmentation: \",p_augmentation)\n",
    "print(\"using metadata: \",p_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gta-2MoG-yzM",
    "outputId": "6a139d3b-431e-4097-8815-c6bce38c9948"
   },
   "outputs": [],
   "source": [
    "pip install nilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMZp1DlM-2cs",
    "outputId": "31a5913b-e17c-4a27-eb73-735bf6b9b20d"
   },
   "outputs": [],
   "source": [
    "pip install pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1DQOg1ZsdqIj",
    "outputId": "e6e35268-f98c-45ba-e851-2e37ce9a5b85"
   },
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlP0aXmBg26E",
    "outputId": "b3f11e0c-b1b9-47f6-e3ca-3d3e6b3b7426"
   },
   "outputs": [],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmxHlj-bdW8k"
   },
   "outputs": [],
   "source": [
    "from nilearn import datasets, connectome, plotting, input_data, image\n",
    "#import pymrmr\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from functools import reduce\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "#from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import pyprind\n",
    "import sys\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import stats\n",
    "from sklearn import tree\n",
    "import functools\n",
    "import numpy.ma as ma # for masked arrays\n",
    "import pyprind\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter\n",
    "from mrmr import mrmr_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zW-ekz0B-u5N",
    "outputId": "71b808c9-d8ab-44b6-ddd4-4fa22017faaa"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jv7tcLvPUWuM"
   },
   "outputs": [],
   "source": [
    "if p_selective_regs:\n",
    "  data_main_path = '/content/drive/My Drive/selected_ccprecdata'\n",
    "  selective_regs_dict = {}\n",
    "  for file in os.listdir(data_main_path):\n",
    "    #print(file)\n",
    "    data = np.loadtxt(os.path.join(data_main_path, file))\n",
    "    df = pd.DataFrame(data)\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
    "        mask = np.invert(np.tri(corr.shape[0], k=-1, dtype=bool))\n",
    "        m = ma.masked_where(mask == 1, mask)\n",
    "        compressed_data = ma.masked_where(m, corr).compressed()\n",
    "\n",
    "    parts = file.split(\"_\")\n",
    "    #print(parts)\n",
    "    if len(parts) == 4:\n",
    "      filename = '_'.join(parts[0:2])\n",
    "    else:\n",
    "      filename = '_'.join(parts[0:3])\n",
    "\n",
    "    selective_regs_dict[filename] = compressed_data\n",
    "\n",
    "else:\n",
    "  selective_regs_dict = None\n",
    "\n",
    "selective_regs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUudhpQ496W_"
   },
   "outputs": [],
   "source": [
    "if p_metadata:\n",
    "  meta_data_dict = {}\n",
    "  meta_data_df = pd.read_csv('/content/drive/My Drive/uncorolated_meta_data_preprocessed.csv')\n",
    "  meta_data_df['FILE_ID'] = pd.Categorical(meta_data_df['FILE_ID'], ordered=True, categories=['Pitt_0050003', 'Pitt_0050004', 'Pitt_0050005', 'Pitt_0050006', 'Pitt_0050007', 'Pitt_0050008', 'Pitt_0050010', 'Pitt_0050011', 'Pitt_0050012', 'Pitt_0050013', 'Pitt_0050014', 'Pitt_0050015', 'Pitt_0050016', 'Pitt_0050020', 'Pitt_0050022', 'Pitt_0050023', 'Pitt_0050024', 'Pitt_0050025', 'Pitt_0050026', 'Pitt_0050027', 'Pitt_0050028', 'Pitt_0050030', 'Pitt_0050031', 'Pitt_0050032', 'Pitt_0050033', 'Pitt_0050034', 'Pitt_0050035', 'Pitt_0050036', 'Pitt_0050037', 'Pitt_0050038', 'Pitt_0050039', 'Pitt_0050040', 'Pitt_0050041', 'Pitt_0050042', 'Pitt_0050043', 'Pitt_0050044', 'Pitt_0050045', 'Pitt_0050046', 'Pitt_0050047', 'Pitt_0050048', 'Pitt_0050049', 'Pitt_0050050', 'Pitt_0050051', 'Pitt_0050052', 'Pitt_0050053', 'Pitt_0050054', 'Pitt_0050056', 'Pitt_0050057', 'Pitt_0050059', 'Pitt_0050060', 'Olin_0050102', 'Olin_0050103', 'Olin_0050104', 'Olin_0050105', 'Olin_0050106', 'Olin_0050107', 'Olin_0050109', 'Olin_0050111', 'Olin_0050112', 'Olin_0050113', 'Olin_0050114', 'Olin_0050115', 'Olin_0050116', 'Olin_0050117', 'Olin_0050118', 'Olin_0050119', 'Olin_0050121', 'Olin_0050123', 'Olin_0050124', 'Olin_0050125', 'Olin_0050127', 'Olin_0050128', 'Olin_0050129', 'Olin_0050130', 'Olin_0050131', 'Olin_0050132', 'Olin_0050134', 'Olin_0050135', 'OHSU_0050142', 'OHSU_0050143', 'OHSU_0050144', 'OHSU_0050145', 'OHSU_0050146', 'OHSU_0050147', 'OHSU_0050148', 'OHSU_0050149', 'OHSU_0050150', 'OHSU_0050152', 'OHSU_0050153', 'OHSU_0050156', 'OHSU_0050157', 'OHSU_0050158', 'OHSU_0050159', 'OHSU_0050160', 'OHSU_0050161', 'OHSU_0050162', 'OHSU_0050163', 'OHSU_0050164', 'OHSU_0050167', 'OHSU_0050168', 'OHSU_0050169', 'OHSU_0050170', 'OHSU_0050171', 'SDSU_0050182', 'SDSU_0050183', 'SDSU_0050184', 'SDSU_0050186', 'SDSU_0050187', 'SDSU_0050188', 'SDSU_0050189', 'SDSU_0050190', 'SDSU_0050193', 'SDSU_0050194', 'SDSU_0050195', 'SDSU_0050196', 'SDSU_0050198', 'SDSU_0050199', 'SDSU_0050200', 'SDSU_0050201', 'SDSU_0050202', 'SDSU_0050203', 'SDSU_0050204', 'SDSU_0050205', 'SDSU_0050206', 'SDSU_0050208', 'SDSU_0050210', 'SDSU_0050213', 'SDSU_0050214', 'SDSU_0050215', 'SDSU_0050217', 'Trinity_0050232', 'Trinity_0050233', 'Trinity_0050234', 'Trinity_0050236', 'Trinity_0050237', 'Trinity_0050239', 'Trinity_0050240', 'Trinity_0050241', 'Trinity_0050243', 'Trinity_0050245', 'Trinity_0050247', 'Trinity_0050248', 'Trinity_0050249', 'Trinity_0050250', 'Trinity_0050251', 'Trinity_0050252', 'Trinity_0050253', 'Trinity_0050254', 'Trinity_0050255', 'Trinity_0050257', 'Trinity_0050259', 'Trinity_0050260', 'Trinity_0050261', 'Trinity_0050262', 'Trinity_0050263', 'Trinity_0050264', 'Trinity_0050265', 'Trinity_0050266', 'Trinity_0050267', 'Trinity_0050268', 'Trinity_0050269', 'Trinity_0050270', 'Trinity_0050271', 'UM_1_0050272', 'UM_1_0050273', 'UM_1_0050274', 'UM_1_0050275', 'UM_1_0050276', 'UM_1_0050278', 'UM_1_0050282', 'UM_1_0050284', 'UM_1_0050285', 'UM_1_0050287', 'UM_1_0050289', 'UM_1_0050290', 'UM_1_0050291', 'UM_1_0050292', 'UM_1_0050293', 'UM_1_0050294', 'UM_1_0050295', 'UM_1_0050297', 'UM_1_0050298', 'UM_1_0050300', 'UM_1_0050301', 'UM_1_0050302', 'UM_1_0050304', 'UM_1_0050308', 'UM_1_0050310', 'UM_1_0050312', 'UM_1_0050314', 'UM_1_0050315', 'UM_1_0050318', 'UM_1_0050319', 'UM_1_0050320', 'UM_1_0050321', 'UM_1_0050324', 'UM_1_0050325', 'UM_1_0050327', 'UM_1_0050329', 'UM_1_0050330', 'UM_1_0050331', 'UM_1_0050332', 'UM_1_0050333', 'UM_1_0050334', 'UM_1_0050335', 'UM_1_0050336', 'UM_1_0050337', 'UM_1_0050338', 'UM_1_0050339', 'UM_1_0050340', 'UM_1_0050341', 'UM_1_0050342', 'UM_1_0050343', 'UM_1_0050344', 'UM_1_0050345', 'UM_1_0050346', 'UM_1_0050347', 'UM_1_0050348', 'UM_1_0050349', 'UM_1_0050350', 'UM_1_0050351', 'UM_1_0050352', 'UM_1_0050353', 'UM_1_0050354', 'UM_1_0050355', 'UM_1_0050356', 'UM_1_0050357', 'UM_1_0050358', 'UM_1_0050359', 'UM_1_0050360', 'UM_1_0050361', 'UM_1_0050362', 'UM_1_0050363', 'UM_1_0050364', 'UM_1_0050365', 'UM_1_0050366', 'UM_1_0050367', 'UM_1_0050368', 'UM_1_0050369', 'UM_1_0050370', 'UM_1_0050372', 'UM_1_0050373', 'UM_1_0050374', 'UM_1_0050375', 'UM_1_0050376', 'UM_1_0050377', 'UM_1_0050379', 'UM_1_0050380', 'UM_1_0050381', 'UM_2_0050382', 'UM_2_0050383', 'UM_2_0050385', 'UM_2_0050386', 'UM_2_0050387', 'UM_2_0050388', 'UM_2_0050390', 'UM_2_0050391', 'UM_2_0050397', 'UM_2_0050399', 'UM_2_0050402', 'UM_2_0050403', 'UM_2_0050404', 'UM_2_0050405', 'UM_2_0050406', 'UM_2_0050407', 'UM_2_0050408', 'UM_2_0050410', 'UM_2_0050411', 'UM_2_0050412', 'UM_2_0050413', 'UM_2_0050414', 'UM_2_0050415', 'UM_2_0050416', 'UM_2_0050417', 'UM_2_0050418', 'UM_2_0050419', 'UM_2_0050421', 'UM_2_0050422', 'UM_2_0050424', 'UM_2_0050425', 'UM_2_0050426', 'UM_2_0050427', 'UM_2_0050428', 'USM_0050433', 'USM_0050434', 'USM_0050435', 'USM_0050436', 'USM_0050437', 'USM_0050438', 'USM_0050439', 'USM_0050440', 'USM_0050441', 'USM_0050442', 'USM_0050443', 'USM_0050444', 'USM_0050445', 'USM_0050446', 'USM_0050447', 'USM_0050448', 'USM_0050449', 'USM_0050453', 'USM_0050463', 'USM_0050466', 'USM_0050467', 'USM_0050468', 'USM_0050469', 'USM_0050470', 'USM_0050477', 'USM_0050480', 'USM_0050481', 'USM_0050482', 'USM_0050483', 'USM_0050485', 'USM_0050486', 'USM_0050487', 'USM_0050488', 'USM_0050490', 'USM_0050491', 'USM_0050492', 'USM_0050493', 'USM_0050494', 'USM_0050496', 'USM_0050497', 'USM_0050498', 'USM_0050499', 'USM_0050500', 'USM_0050501', 'USM_0050502', 'USM_0050503', 'USM_0050504', 'USM_0050507', 'USM_0050509', 'USM_0050510', 'USM_0050514', 'USM_0050515', 'USM_0050516', 'USM_0050518', 'USM_0050519', 'USM_0050520', 'USM_0050521', 'USM_0050523', 'USM_0050524', 'USM_0050525', 'USM_0050526', 'USM_0050527', 'USM_0050528', 'USM_0050529', 'USM_0050530', 'USM_0050531', 'USM_0050532', 'Yale_0050551', 'Yale_0050552', 'Yale_0050555', 'Yale_0050557', 'Yale_0050558', 'Yale_0050561', 'Yale_0050563', 'Yale_0050565', 'Yale_0050568', 'Yale_0050569', 'Yale_0050570', 'Yale_0050571', 'Yale_0050572', 'Yale_0050573', 'Yale_0050574', 'Yale_0050575', 'Yale_0050576', 'Yale_0050577', 'Yale_0050578', 'Yale_0050601', 'Yale_0050602', 'Yale_0050603', 'Yale_0050604', 'Yale_0050606', 'Yale_0050607', 'Yale_0050608', 'Yale_0050612', 'Yale_0050613', 'Yale_0050614', 'Yale_0050615', 'Yale_0050616', 'Yale_0050619', 'Yale_0050620', 'Yale_0050621', 'Yale_0050622', 'Yale_0050623', 'Yale_0050624', 'Yale_0050625', 'Yale_0050626', 'Yale_0050627', 'Yale_0050628', 'CMU_a_0050642', 'CMU_b_0050644', 'CMU_a_0050647', 'CMU_b_0050648', 'CMU_a_0050649', 'CMU_a_0050654', 'CMU_a_0050656', 'CMU_a_0050659', 'CMU_a_0050664', 'CMU_a_0050665', 'CMU_b_0050669', 'Leuven_1_0050682', 'Leuven_1_0050683', 'Leuven_1_0050685', 'Leuven_1_0050686', 'Leuven_1_0050687', 'Leuven_1_0050688', 'Leuven_1_0050689', 'Leuven_1_0050690', 'Leuven_1_0050691', 'Leuven_1_0050692', 'Leuven_1_0050693', 'Leuven_1_0050694', 'Leuven_1_0050695', 'Leuven_1_0050696', 'Leuven_1_0050697', 'Leuven_1_0050698', 'Leuven_1_0050699', 'Leuven_1_0050700', 'Leuven_1_0050701', 'Leuven_1_0050702', 'Leuven_1_0050703', 'Leuven_1_0050704', 'Leuven_1_0050705', 'Leuven_1_0050706', 'Leuven_1_0050707', 'Leuven_1_0050708', 'Leuven_1_0050709', 'Leuven_1_0050711', 'Leuven_2_0050722', 'Leuven_2_0050723', 'Leuven_2_0050724', 'Leuven_2_0050725', 'Leuven_2_0050726', 'Leuven_2_0050728', 'Leuven_2_0050730', 'Leuven_2_0050731', 'Leuven_2_0050733', 'Leuven_2_0050735', 'Leuven_2_0050737', 'Leuven_2_0050738', 'Leuven_2_0050739', 'Leuven_2_0050740', 'Leuven_2_0050741', 'Leuven_2_0050742', 'Leuven_2_0050743', 'Leuven_2_0050744', 'Leuven_2_0050745', 'Leuven_2_0050748', 'Leuven_2_0050749', 'Leuven_2_0050750', 'Leuven_2_0050751', 'Leuven_2_0050752', 'Leuven_2_0050754', 'Leuven_2_0050755', 'Leuven_2_0050756', 'Leuven_2_0050757', 'KKI_0050772', 'KKI_0050773', 'KKI_0050774', 'KKI_0050775', 'KKI_0050776', 'KKI_0050777', 'KKI_0050778', 'KKI_0050780', 'KKI_0050781', 'KKI_0050782', 'KKI_0050783', 'KKI_0050786', 'KKI_0050790', 'KKI_0050791', 'KKI_0050792', 'KKI_0050796', 'KKI_0050797', 'KKI_0050798', 'KKI_0050799', 'KKI_0050800', 'KKI_0050801', 'KKI_0050803', 'KKI_0050807', 'KKI_0050812', 'KKI_0050814', 'KKI_0050816', 'KKI_0050817', 'KKI_0050818', 'KKI_0050820', 'KKI_0050821', 'KKI_0050822', 'KKI_0050823', 'KKI_0050824', 'NYU_0050952', 'NYU_0050954', 'NYU_0050955', 'NYU_0050956', 'NYU_0050957', 'NYU_0050958', 'NYU_0050959', 'NYU_0050960', 'NYU_0050961', 'NYU_0050962', 'NYU_0050964', 'NYU_0050965', 'NYU_0050966', 'NYU_0050967', 'NYU_0050968', 'NYU_0050969', 'NYU_0050970', 'NYU_0050972', 'NYU_0050973', 'NYU_0050974', 'NYU_0050976', 'NYU_0050977', 'NYU_0050978', 'NYU_0050979', 'NYU_0050981', 'NYU_0050982', 'NYU_0050983', 'NYU_0050984', 'NYU_0050985', 'NYU_0050986', 'NYU_0050987', 'NYU_0050988', 'NYU_0050989', 'NYU_0050990', 'NYU_0050991', 'NYU_0050992', 'NYU_0050993', 'NYU_0050994', 'NYU_0050995', 'NYU_0050996', 'NYU_0050997', 'NYU_0050999', 'NYU_0051000', 'NYU_0051001', 'NYU_0051002', 'NYU_0051003', 'NYU_0051006', 'NYU_0051007', 'NYU_0051008', 'NYU_0051009', 'NYU_0051010', 'NYU_0051011', 'NYU_0051012', 'NYU_0051013', 'NYU_0051014', 'NYU_0051015', 'NYU_0051016', 'NYU_0051017', 'NYU_0051018', 'NYU_0051019', 'NYU_0051020', 'NYU_0051021', 'NYU_0051023', 'NYU_0051024', 'NYU_0051025', 'NYU_0051026', 'NYU_0051027', 'NYU_0051028', 'NYU_0051029', 'NYU_0051030', 'NYU_0051032', 'NYU_0051033', 'NYU_0051034', 'NYU_0051035', 'NYU_0051036', 'NYU_0051038', 'NYU_0051039', 'NYU_0051040', 'NYU_0051041', 'NYU_0051042', 'NYU_0051044', 'NYU_0051045', 'NYU_0051046', 'NYU_0051047', 'NYU_0051048', 'NYU_0051049', 'NYU_0051050', 'NYU_0051051', 'NYU_0051052', 'NYU_0051053', 'NYU_0051054', 'NYU_0051055', 'NYU_0051056', 'NYU_0051057', 'NYU_0051058', 'NYU_0051059', 'NYU_0051060', 'NYU_0051061', 'NYU_0051062', 'NYU_0051063', 'NYU_0051064', 'NYU_0051065', 'NYU_0051066', 'NYU_0051067', 'NYU_0051068', 'NYU_0051069', 'NYU_0051070', 'NYU_0051072', 'NYU_0051073', 'NYU_0051074', 'NYU_0051075', 'NYU_0051076', 'NYU_0051077', 'NYU_0051078', 'NYU_0051079', 'NYU_0051080', 'NYU_0051081', 'NYU_0051082', 'NYU_0051083', 'NYU_0051084', 'NYU_0051085', 'NYU_0051086', 'NYU_0051087', 'NYU_0051088', 'NYU_0051089', 'NYU_0051090', 'NYU_0051091', 'NYU_0051093', 'NYU_0051094', 'NYU_0051095', 'NYU_0051096', 'NYU_0051097', 'NYU_0051098', 'NYU_0051099', 'NYU_0051100', 'NYU_0051101', 'NYU_0051102', 'NYU_0051103', 'NYU_0051104', 'NYU_0051105', 'NYU_0051106', 'NYU_0051107', 'NYU_0051109', 'NYU_0051110', 'NYU_0051111', 'NYU_0051112', 'NYU_0051113', 'NYU_0051114', 'NYU_0051116', 'NYU_0051117', 'NYU_0051118', 'NYU_0051122', 'NYU_0051123', 'NYU_0051124', 'NYU_0051126', 'NYU_0051127', 'NYU_0051128', 'NYU_0051129', 'NYU_0051130', 'NYU_0051131', 'Trinity_0051132', 'Trinity_0051133', 'Trinity_0051134', 'Trinity_0051135', 'Trinity_0051136', 'Trinity_0051137', 'Trinity_0051138', 'Trinity_0051139', 'Trinity_0051140', 'Trinity_0051141', 'Trinity_0051142', 'NYU_0051146', 'NYU_0051147', 'NYU_0051148', 'NYU_0051149', 'NYU_0051150', 'NYU_0051151', 'NYU_0051152', 'NYU_0051153', 'NYU_0051154', 'NYU_0051155', 'NYU_0051156', 'NYU_0051159', 'Stanford_0051161', 'Stanford_0051162', 'Stanford_0051163', 'Stanford_0051164', 'Stanford_0051168', 'Stanford_0051169', 'Stanford_0051170', 'Stanford_0051171', 'Stanford_0051173', 'Stanford_0051177', 'Stanford_0051178', 'Stanford_0051179', 'Stanford_0051180', 'Stanford_0051181', 'Stanford_0051182', 'Stanford_0051183', 'Stanford_0051184', 'Stanford_0051185', 'Stanford_0051187', 'Stanford_0051188', 'Stanford_0051189', 'Stanford_0051192', 'Stanford_0051194', 'Stanford_0051197', 'Stanford_0051198', 'UCLA_1_0051201', 'UCLA_1_0051202', 'UCLA_1_0051203', 'UCLA_1_0051204', 'UCLA_1_0051205', 'UCLA_1_0051206', 'UCLA_1_0051207', 'UCLA_1_0051208', 'UCLA_1_0051210', 'UCLA_1_0051211', 'UCLA_1_0051212', 'UCLA_1_0051214', 'UCLA_1_0051215', 'UCLA_1_0051216', 'UCLA_1_0051217', 'UCLA_1_0051218', 'UCLA_1_0051219', 'UCLA_1_0051220', 'UCLA_1_0051221', 'UCLA_1_0051222', 'UCLA_1_0051223', 'UCLA_1_0051224', 'UCLA_1_0051225', 'UCLA_1_0051226', 'UCLA_1_0051228', 'UCLA_1_0051229', 'UCLA_1_0051230', 'UCLA_1_0051231', 'UCLA_1_0051234', 'UCLA_1_0051235', 'UCLA_1_0051236', 'UCLA_1_0051237', 'UCLA_1_0051239', 'UCLA_1_0051240', 'UCLA_1_0051241', 'UCLA_1_0051248', 'UCLA_1_0051249', 'UCLA_1_0051250', 'UCLA_1_0051251', 'UCLA_1_0051252', 'UCLA_1_0051253', 'UCLA_1_0051254', 'UCLA_1_0051255', 'UCLA_1_0051256', 'UCLA_1_0051257', 'UCLA_1_0051260', 'UCLA_1_0051261', 'UCLA_1_0051262', 'UCLA_1_0051264', 'UCLA_1_0051265', 'UCLA_1_0051266', 'UCLA_1_0051267', 'UCLA_1_0051268', 'UCLA_1_0051269', 'UCLA_1_0051271', 'UCLA_1_0051272', 'UCLA_1_0051273', 'UCLA_1_0051275', 'UCLA_1_0051276', 'UCLA_1_0051277', 'UCLA_1_0051278', 'UCLA_1_0051279', 'UCLA_1_0051280', 'UCLA_1_0051281', 'UCLA_2_0051291', 'UCLA_2_0051292', 'UCLA_2_0051293', 'UCLA_2_0051294', 'UCLA_2_0051295', 'UCLA_2_0051297', 'UCLA_2_0051298', 'UCLA_2_0051299', 'UCLA_2_0051300', 'UCLA_2_0051301', 'UCLA_2_0051302', 'UCLA_2_0051303', 'UCLA_2_0051304', 'UCLA_2_0051305', 'UCLA_2_0051306', 'UCLA_2_0051307', 'UCLA_2_0051308', 'UCLA_2_0051309', 'UCLA_2_0051311', 'UCLA_2_0051313', 'UCLA_2_0051315', 'MaxMun_a_0051318', 'MaxMun_a_0051319', 'MaxMun_a_0051320', 'MaxMun_a_0051321', 'MaxMun_b_0051322', 'MaxMun_b_0051323', 'MaxMun_b_0051325', 'MaxMun_b_0051326', 'MaxMun_b_0051327', 'MaxMun_c_0051328', 'MaxMun_d_0051329', 'MaxMun_d_0051330', 'MaxMun_d_0051331', 'MaxMun_c_0051332', 'MaxMun_c_0051333', 'MaxMun_c_0051334', 'MaxMun_c_0051335', 'MaxMun_c_0051336', 'MaxMun_c_0051338', 'MaxMun_c_0051339', 'MaxMun_c_0051340', 'MaxMun_c_0051341', 'MaxMun_c_0051342', 'MaxMun_c_0051343', 'MaxMun_c_0051344', 'MaxMun_c_0051345', 'MaxMun_c_0051346', 'MaxMun_c_0051347', 'MaxMun_d_0051349', 'MaxMun_d_0051350', 'MaxMun_d_0051351', 'MaxMun_d_0051354', 'MaxMun_d_0051356', 'MaxMun_d_0051357', 'MaxMun_d_0051359', 'MaxMun_d_0051360', 'MaxMun_d_0051361', 'MaxMun_a_0051362', 'MaxMun_a_0051363', 'MaxMun_a_0051364', 'MaxMun_a_0051365', 'MaxMun_a_0051369', 'MaxMun_a_0051370', 'MaxMun_a_0051373', 'Caltech_0051461', 'Caltech_0051463', 'Caltech_0051464', 'Caltech_0051465', 'Caltech_0051473', 'Caltech_0051477', 'Caltech_0051479', 'Caltech_0051480', 'Caltech_0051481', 'Caltech_0051482', 'Caltech_0051484', 'Caltech_0051487', 'Caltech_0051488', 'Caltech_0051491', 'Caltech_0051493', 'SBL_0051556', 'SBL_0051557', 'SBL_0051558', 'SBL_0051559', 'SBL_0051560', 'SBL_0051562', 'SBL_0051563', 'SBL_0051564', 'SBL_0051565', 'SBL_0051566', 'SBL_0051567', 'SBL_0051568', 'SBL_0051569', 'SBL_0051570', 'SBL_0051572', 'SBL_0051573', 'SBL_0051574', 'SBL_0051576', 'SBL_0051577', 'SBL_0051578', 'SBL_0051579', 'SBL_0051580', 'SBL_0051582', 'SBL_0051583', 'SBL_0051584', 'SBL_0051585', 'MaxMun_a_0051606', 'MaxMun_a_0051607'])\n",
    "\n",
    "  sorted_meta_data_df = meta_data_df.sort_values('FILE_ID')\n",
    "\n",
    "  for index, row in sorted_meta_data_df.iterrows():\n",
    "    file_id = row['FILE_ID']\n",
    "    other_values = row.drop('FILE_ID').tolist()\n",
    "    meta_data_dict[file_id] = other_values\n",
    "\n",
    "else:\n",
    "  meta_data_dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LA1cRqa6Snq"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1 is for asd and 0 is for healthy\n",
    "df_labels = pd.read_csv('/content/drive/My Drive/Phenotypic_V1_0b_preprocessed1.csv')#path\n",
    "df_labels.DX_GROUP = df_labels.DX_GROUP.map({1: 1, 2:0})\n",
    "\n",
    "\n",
    "labels = {}\n",
    "for row in df_labels.iterrows():\n",
    "    file_id = row[1]['FILE_ID']\n",
    "    y_label = row[1]['DX_GROUP']\n",
    "    if file_id == 'no_filename':\n",
    "        continue\n",
    "    assert(file_id not in labels)\n",
    "    labels[file_id] = y_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kboevt-byNYH"
   },
   "outputs": [],
   "source": [
    "def get_key(filename):\n",
    "    f_split = filename.split('_')\n",
    "    if f_split[3] == 'rois':\n",
    "        key = '_'.join(f_split[0:3])\n",
    "    else:\n",
    "        key = '_'.join(f_split[0:2])\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6TlspmJ_yRKw",
    "outputId": "030ce414-671a-453c-f63f-bc6ffa54bba3"
   },
   "outputs": [],
   "source": [
    "data_main_path = '/content/drive/My Drive/cc400precdata/ABIDE_pcp/cpac/filt_global' #path to time series data\n",
    "#data_main_path = '/content/drive/My Drive/power264'\n",
    "flist = os.listdir(data_main_path)\n",
    "print(len(flist))\n",
    "\n",
    "for f in range(len(flist)):\n",
    "    flist[f] = get_key(flist[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PaPvGDn2NQz"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVxEdhqkAmLH"
   },
   "outputs": [],
   "source": [
    "def get_label(filename):\n",
    "    assert (filename in labels)\n",
    "    return labels[filename]\n",
    "\n",
    "\n",
    "def get_corr_data(filename):\n",
    "    for file in os.listdir(data_main_path):\n",
    "        if file.startswith(filename):\n",
    "            if p_ROI == \"power\":\n",
    "              data = np.loadtxt(os.path.join(data_main_path, file))\n",
    "              if p_selective_regs:\n",
    "                excluded_column_indices = [1, 2, 17, 54, 57, 108, 147, 173]\n",
    "                data = np.delete(data, excluded_column_indices, axis=1)\n",
    "              df = pd.DataFrame(data)\n",
    "            else:\n",
    "              df = pd.read_csv(os.path.join(data_main_path, file), sep='\\t')\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
    "        mask = np.invert(np.tri(corr.shape[0], k=-1, dtype=bool))\n",
    "        m = ma.masked_where(mask == 1, mask)\n",
    "        return ma.masked_where(m, corr).compressed()\n",
    "\n",
    "def get_corr_matrix(filename):\n",
    "    for file in os.listdir(data_main_path):\n",
    "        if file.startswith(filename):\n",
    "          if p_ROI == \"power\":\n",
    "            data = np.loadtxt(os.path.join(data_main_path, file))\n",
    "            if p_selective_regs:\n",
    "              excluded_column_indices = [1, 2, 17, 54, 57, 108, 147, 173]\n",
    "              data = np.delete(data, excluded_column_indices, axis=1)\n",
    "            df = pd.DataFrame(data)\n",
    "          else:\n",
    "            df = pd.read_csv(os.path.join(data_main_path, file), sep='\\t')\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
    "        return corr\n",
    "\n",
    "def confusion(g_turth,predictions):\n",
    "    tn, fp, fn, tp = confusion_matrix(g_turth,predictions).ravel()\n",
    "    accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
    "    sensitivity = (tp)/(tp+fn)\n",
    "    specificty = (tn)/(tn+fp)\n",
    "    return accuracy,sensitivity,specificty\n",
    "\n",
    "def get_regs(samplesnames,regnum):\n",
    "    datas = []\n",
    "    for sn in samplesnames:\n",
    "        datas.append(all_corr[sn][0])\n",
    "    datas = np.array(datas)\n",
    "    #print('datas.shape is equal to:')\n",
    "    #print(datas.shape)\n",
    "    avg=[]\n",
    "    for ie in range(datas.shape[1]):\n",
    "        avg.append(np.mean(datas[:,ie]))\n",
    "    avg=np.array(avg)\n",
    "    highs=avg.argsort()[-regnum:][::-1]\n",
    "    lows=avg.argsort()[:regnum][::-1]\n",
    "    regions=np.concatenate((highs,lows),axis=0)\n",
    "    return regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frP24RtRjaMF",
    "outputId": "592b776e-e242-42da-cc13-21591a803c86"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./correlations_file'+p_ROI+'.pkl'):\n",
    "    print(2)\n",
    "    pbar=pyprind.ProgBar(len(flist))\n",
    "    all_corr = {}\n",
    "    for f in flist:\n",
    "\n",
    "        lab = get_label(f)\n",
    "        all_corr[f] = (get_corr_data(f), lab)\n",
    "        pbar.update()\n",
    "\n",
    "    print('Corr-computations finished')\n",
    "\n",
    "    pickle.dump(all_corr, open('./correlations_file'+p_ROI+'.pkl', 'wb'))\n",
    "    print('Saving to file finished')\n",
    "\n",
    "else:\n",
    "    print(3)\n",
    "    all_corr = pickle.load(open('./correlations_file'+p_ROI+'.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46uy1V1X7xVp",
    "outputId": "3171d404-8603-496b-c04c-d8ff1caa7ac9"
   },
   "outputs": [],
   "source": [
    "lst = []\n",
    "for f in flist:\n",
    "  lab = all_corr[f][1]\n",
    "  lst.append(lab)\n",
    "num_ones = lst.count(1)\n",
    "num_zeros = lst.count(0)\n",
    "\n",
    "print(\"Number of ones:\", num_ones)\n",
    "print(\"Number of zeros:\", num_zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7FGpEgReu90"
   },
   "source": [
    "**Selecting 1935 best features with extra tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glp7W4kLqEfG",
    "outputId": "6f6043c3-2ab3-4759-b58c-2e5ad4f73e90"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Create the ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create a pipeline with feature selection and classifier\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(ExtraTreesClassifier(n_estimators=100, random_state=42),max_features=1935)),\n",
    "    ('classification', clf)\n",
    "])\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "selected_features = []\n",
    "lens = []\n",
    "\n",
    "y_arr = np.array([get_label(f) for f in flist])\n",
    "for train_index, test_index in cv.split(flist, y_arr):\n",
    "    train_data = []\n",
    "    train_y = []\n",
    "    X_train, X_test = np.array(flist)[train_index], np.array(flist)[test_index]\n",
    "    y_train, y_test = y_arr[train_index], y_arr[test_index]\n",
    "\n",
    "    for i in X_train:\n",
    "      train_data.append(all_corr[i][0])\n",
    "      train_y.append(all_corr[i][1])\n",
    "\n",
    "    data_train = np.vstack(train_data)\n",
    "    y_train = np.vstack(train_y)\n",
    "    pipeline.fit(data_train, y_train)\n",
    "    selected = pipeline.named_steps['feature_selection'].get_support(indices=True)\n",
    "    selected_features.append(selected)\n",
    "    lens.append(len(selected))\n",
    "\n",
    "# Find the minimum and even length of selected features\n",
    "min_even_length = min(x for x in lens)\n",
    "#min_even_length = min(x for x in lens if x % 2 == 0)\n",
    "\n",
    "# Filter selected features to get the ones with the minimum and even length\n",
    "final_selected_features = [feat for feat in selected_features if len(feat) == min_even_length]\n",
    "\n",
    "# Convert selected features to a numpy array\n",
    "#final_selected_features = np.array(final_selected_features)\n",
    "\n",
    "# Print selected features\n",
    "print(\"Selected features with minimum and even length after all folds:\")\n",
    "print(final_selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRbwn4uON4j7",
    "outputId": "8c9dae6f-c8c8-4dab-ba96-ef2fc8aa2f3b"
   },
   "outputs": [],
   "source": [
    "final_selected_features = final_selected_features[0].tolist()\n",
    "print(len(final_selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jx3YMAjQBu4Y",
    "outputId": "513ae24c-059b-4efd-9006-f4aaeca79dd5"
   },
   "outputs": [],
   "source": [
    "#for i in range(final_selected_features.shape[1]):\n",
    " # print(final_selected_features[0][i])\n",
    "#print(final_selected_features.shape)\n",
    "\n",
    "for f in flist:\n",
    "  print(f)\n",
    "  data = all_corr[f][0]\n",
    "  all_corr[f] = (data[final_selected_features] , all_corr[f][1])\n",
    "  print(all_corr[f][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4PJ2kA-fChM"
   },
   "source": [
    "**calculating eigen vectors and eigen values for each item as a dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WHe82SA3BwrP",
    "outputId": "4f8c0575-1cba-46c1-ba00-15bd78c0c377"
   },
   "outputs": [],
   "source": [
    "if p_Method==\"ASD-DiagNet\":\n",
    "    eig_data = {}\n",
    "    pbar = pyprind.ProgBar(len(flist))\n",
    "    for f in flist:\n",
    "        d = get_corr_matrix(f)\n",
    "        eig_vals, eig_vecs = np.linalg.eig(d)\n",
    "\n",
    "        for ev in eig_vecs.T:\n",
    "            np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
    "\n",
    "        sum_eigvals = np.sum(np.abs(eig_vals))\n",
    "        # Make a list of (eigenvalue, eigenvector, norm_eigval) tuples\n",
    "        eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i], np.abs(eig_vals[i])/sum_eigvals)\n",
    "                     for i in range(len(eig_vals))]\n",
    "\n",
    "        # Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "        eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        eig_data[f] = {'eigvals':np.array([ep[0] for ep in eig_pairs]),\n",
    "                       'norm-eigvals':np.array([ep[2] for ep in eig_pairs]),\n",
    "                       'eigvecs':[ep[1] for ep in eig_pairs]}\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vc1fm0ctfQZn"
   },
   "source": [
    "**using SMOTE with EROS in augmenting data for second model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEJfKV_BB5Fw"
   },
   "outputs": [],
   "source": [
    "def norm_weights(sub_flist):\n",
    "    num_dim = len(eig_data[flist[0]]['eigvals'])\n",
    "    norm_weights = np.zeros(shape=num_dim)\n",
    "    for f in sub_flist:\n",
    "        norm_weights += eig_data[f]['norm-eigvals']\n",
    "    return norm_weights\n",
    "\n",
    "def cal_similarity(d1, d2, weights, lim=None):\n",
    "    res = 0.0\n",
    "    if lim is None:\n",
    "        weights_arr = weights.copy()\n",
    "    else:\n",
    "        weights_arr = weights[:lim].copy()\n",
    "        weights_arr /= np.sum(weights_arr)\n",
    "    for i,w in enumerate(weights_arr):\n",
    "        res += w*np.inner(d1[i], d2[i])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hr6N6F_phBSn"
   },
   "outputs": [],
   "source": [
    "#augment_data_select_1935_features_power264\n",
    "def augment_extra_tree(sub_flist):\n",
    "  neighbors = {}\n",
    "  neighbor_weights = {}\n",
    "  train_aug_labels = []\n",
    "  train_aug_data = []\n",
    "  weights = norm_weights(sub_flist)\n",
    "  #print(weights)\n",
    "  #print('size wights' , weights.shape)\n",
    "  flist1 = [f for f in sub_flist]\n",
    "  labels = np.array([all_corr[f][1] for f in flist1])\n",
    "  list_of_data = [all_corr[f][0] for f in flist1]\n",
    "  all_labels = [all_corr[f][1] for f in flist1]\n",
    "  current_flist = np.array(flist1.copy())\n",
    "  current_lab0_flist = current_flist[labels == 0]\n",
    "  current_lab1_flist = current_flist[labels == 1]\n",
    "\n",
    "  for f in sub_flist:\n",
    "    label = all_corr[f][1]\n",
    "    candidates = (set(current_lab0_flist) if label == 0 else set(current_lab1_flist))\n",
    "    candidates.remove(f)\n",
    "    eig_f = eig_data[f]['eigvecs']\n",
    "    sim_list = []\n",
    "    for cand in candidates:\n",
    "      total = 0\n",
    "      eig_cand = eig_data[cand]['eigvecs']\n",
    "      sim = cal_similarity(eig_f, eig_cand,weights,2)\n",
    "      sim_list.append((sim, cand))\n",
    "      sim_list.sort(key=lambda x: x[0], reverse=True)\n",
    "      neighbors[f] = [item[1] for item in sim_list[:5]]\n",
    "      total = sum(item[0] for item in sim_list[:5])\n",
    "      total = np.round(total, 2)\n",
    "      #for item in sim_list[:5]:\n",
    "       # result = item[0]/total\n",
    "        #neighbor_weights[f] = np.round(result,2)\n",
    "      neighbor_weights[f] = [np.round((item[0]/total),2) for item in sim_list[:5]]\n",
    "      #print(neighbor_weights)\n",
    "      #print(neighbors[f])\n",
    "  for f in sub_flist:\n",
    "    #print('train_data is:' , f)\n",
    "    d1, y1 = all_corr[f][0], all_corr[f][1]\n",
    "    f2 = random.choices(neighbors[f], weights=neighbor_weights[f], k=1)[0]\n",
    "    #print('its neighbor is:' , f2)\n",
    "    d2, y2 = all_corr[f2][0], all_corr[f2][1]\n",
    "    assert y1 == y2\n",
    "    r = np.random.uniform(low=0, high=1)\n",
    "    train_aug_labels.append(y1)\n",
    "    data = r*d1 + (1-r)*d2\n",
    "    train_aug_data.append(data)\n",
    "\n",
    "  list_of_data.extend(train_aug_data)\n",
    "  all_labels.extend(train_aug_labels)\n",
    "  return  list_of_data, all_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiNVna7jZPwW"
   },
   "source": [
    "**succesfull structures of GAN for augmenting data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PM7okCyLC9Cd"
   },
   "source": [
    "using conditional Wgan with GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDJfJEeHDAO2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def augment_fmri_data_cw_gan_gp(sub_flist_samples):\n",
    "\n",
    "    num_epochs = 300\n",
    "    batch_size = 128\n",
    "    latent_dim = 128\n",
    "    lambda_gp = 20\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, latent_dim, label_dim, output_dim):\n",
    "            super(Generator, self).__init__()\n",
    "            self.label_embedding = nn.Embedding(label_dim, latent_dim)\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(latent_dim + latent_dim, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(512, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 2048),  # New layer\n",
    "                nn.ReLU(inplace=True),  # New layer\n",
    "                nn.Linear(2048, output_dim),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "        def forward(self, noise, labels):\n",
    "            c = self.label_embedding(labels)\n",
    "            x = torch.cat((noise, c), dim=1)\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, input_dim, label_dim):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.label_embedding = nn.Embedding(label_dim, input_dim)\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim + input_dim, 1024),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(256, 128),  # New layer\n",
    "                nn.LeakyReLU(0.2, inplace=True),  # New layer\n",
    "                nn.Linear(128, 1),\n",
    "            )\n",
    "\n",
    "        def forward(self, img, labels):\n",
    "            c = self.label_embedding(labels)\n",
    "            x = torch.cat((img, c), dim=1)\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "\n",
    "    def compute_gradient_penalty(D, real_samples, fake_samples, labels, device):\n",
    "        alpha = torch.rand(real_samples.size(0), 1).to(device)\n",
    "        alpha = alpha.expand(real_samples.size())\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "        d_interpolates = D(interpolates, labels)\n",
    "        fake = torch.ones(d_interpolates.size(), requires_grad=False).to(device)\n",
    "        gradients = grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "\n",
    "    input_dim = 1935\n",
    "    label_dim = 2\n",
    "    num_samples = len(sub_flist_samples)\n",
    "\n",
    "    # Instantiate models\n",
    "    generator = Generator(latent_dim, label_dim, input_dim).to(device)\n",
    "    discriminator = Discriminator(input_dim, label_dim).to(device)\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_data = np.array([all_corr[f][0] for f in flist1])\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32).to(device)\n",
    "    labels = torch.tensor(array_of_labels, dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_data, labels) in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = torch.ones(batch_size, 1).to(device)\n",
    "            fake = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Sample noise and labels as generator input\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            gen_labels = labels\n",
    "\n",
    "            # Generate a batch of data\n",
    "            gen_data = generator(z, gen_labels)\n",
    "\n",
    "            # Real data\n",
    "            real_validity = discriminator(real_data, labels)\n",
    "            # Fake data\n",
    "            fake_validity = discriminator(gen_data.detach(), gen_labels)\n",
    "\n",
    "            # Gradient penalty\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_data.data, gen_data.data, labels.data, device)\n",
    "\n",
    "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_data = generator(z, gen_labels)\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            fake_validity = discriminator(gen_data, gen_labels)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        #print(f\"Epoch [{epoch+1}/{num_epochs}] D loss: {d_loss.item()} G loss: {g_loss.item()}\")\n",
    "\n",
    "    # Generate synthetic samples\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        gen_labels = torch.tensor(array_of_labels, dtype=torch.long).to(device)\n",
    "        synthetic_data = generator(z, gen_labels).cpu().numpy()\n",
    "        synthetic_labels = gen_labels.cpu().numpy()\n",
    "        print(synthetic_labels.shape[0])\n",
    "\n",
    "    # Ensure synthetic_data has the correct shape (num_samples, input_dim)\n",
    "    if synthetic_data.shape[1] != input_dim:\n",
    "        raise ValueError(f\"Generated synthetic_data has incorrect shape: {synthetic_data.shape}. Expected ({num_samples}, {input_dim}).\")\n",
    "\n",
    "    # Concatenate data and labels\n",
    "    augmented_data = np.vstack((data.cpu().numpy(), synthetic_data))\n",
    "    augmented_labels = np.hstack((synthetic_labels, synthetic_labels))\n",
    "\n",
    "\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4XTeDOP5GsH"
   },
   "source": [
    "using simple gan two layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRoYqKqH5I_4"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim2, hidden_dim1, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n",
    "\n",
    "class CodeDiscriminator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim1, hidden_dim2):\n",
    "        super(CodeDiscriminator, self).__init__()\n",
    "        self.code_discriminator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.code_discriminator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmCphSKr5MRK"
   },
   "outputs": [],
   "source": [
    "def generate_data(num_samples, decoder, latent_dim):\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_codes = torch.randn(num_samples, latent_dim)\n",
    "        generated_data = decoder(latent_codes)\n",
    "    return generated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D169evWS5NRy"
   },
   "outputs": [],
   "source": [
    "def augment_data_alpha_gan_2_layer(sub_flist_samples):\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    input_dim = 1935\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    latent_dim = 128\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "\n",
    "    # Instantiate models\n",
    "    encoder = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "    decoder = Decoder(latent_dim, hidden_dim2, hidden_dim1, input_dim)\n",
    "    discriminator = Discriminator(input_dim, hidden_dim1, hidden_dim2)\n",
    "    code_discriminator = CodeDiscriminator(latent_dim, hidden_dim1, hidden_dim2)\n",
    "\n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "    code_loss = nn.BCELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    optimizer_D = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    optimizer_Dis = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "    optimizer_CodeDis = optim.Adam(code_discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = []\n",
    "    for f in sub_flist_samples:\n",
    "      list_of_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "    #usefull data and labels\n",
    "    useful_samples = [item for item in sub_flist_samples if item not in useless_samples]\n",
    "    flist1 = [f for f in useful_samples]\n",
    "    array_of_useful_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_useful_labels = array_of_useful_labels.tolist()\n",
    "    list_useful_data = []\n",
    "\n",
    "    for f in useful_samples:\n",
    "      list_useful_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "    data = torch.tensor(list_of_data)\n",
    "    labels = torch.tensor(list_of_labels)\n",
    "\n",
    "    #usefull all data and labels\n",
    "    useful_data = torch.tensor(list_useful_data)\n",
    "    useful_labels = torch.tensor(list_of_useful_labels)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "    useful_dataloader = DataLoader(TensorDataset(useful_data, useful_labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data, _ in dataloader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Prepare real and fake labels\n",
    "            real_labels = torch.ones(batch_size, 1)\n",
    "            fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "            # Train the Discriminator\n",
    "            latent_code = encoder(real_data)\n",
    "            reconstructed_data = decoder(latent_code)\n",
    "\n",
    "            outputs_real = discriminator(real_data)\n",
    "            outputs_fake = discriminator(reconstructed_data.detach())\n",
    "\n",
    "            dis_loss_real = adversarial_loss(outputs_real, real_labels)\n",
    "            dis_loss_fake = adversarial_loss(outputs_fake, fake_labels)\n",
    "            dis_loss = dis_loss_real + dis_loss_fake\n",
    "\n",
    "            optimizer_Dis.zero_grad()\n",
    "            dis_loss.backward(retain_graph=True)\n",
    "            optimizer_Dis.step()\n",
    "\n",
    "            # Train the Code Discriminator\n",
    "            outputs_code_real = code_discriminator(latent_code)\n",
    "            outputs_code_fake = code_discriminator(torch.randn(batch_size, latent_dim))\n",
    "\n",
    "            code_loss_real = code_loss(outputs_code_real, real_labels)\n",
    "            code_loss_fake = code_loss(outputs_code_fake, fake_labels)\n",
    "            code_dis_loss = code_loss_real + code_loss_fake\n",
    "\n",
    "            optimizer_CodeDis.zero_grad()\n",
    "            code_dis_loss.backward(retain_graph=True)\n",
    "            optimizer_CodeDis.step()\n",
    "\n",
    "            # Train the Encoder and Decoder (Generator)\n",
    "            outputs_fake = discriminator(reconstructed_data)\n",
    "            gen_loss = adversarial_loss(outputs_fake, real_labels)\n",
    "            rec_loss = reconstruction_loss(reconstructed_data, real_data)\n",
    "            total_gen_loss = gen_loss + rec_loss\n",
    "\n",
    "            optimizer_E.zero_grad()\n",
    "            optimizer_D.zero_grad()\n",
    "            total_gen_loss.backward(retain_graph=True)\n",
    "            optimizer_E.step()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_gen_loss.item()}')\n",
    "\n",
    "    # Generate additional samples\n",
    "    synthetic_data = generate_data(3 * num_samples, decoder, latent_dim)\n",
    "\n",
    "    # Generating labels for synthetic data\n",
    "    synthetic_labels = labels[:3 * num_samples]\n",
    "    synthetic_labels = synthetic_labels.repeat(3)\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    augmented_labels_list = augmented_labels.tolist()\n",
    "    print('number of ones is:', augmented_labels_list.count(1))\n",
    "    print('number of zeros is:', augmented_labels_list.count(0))\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "    # DataLoader for training\n",
    "    augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inspect the augmented dataset\n",
    "    print(augmented_data.shape)  # Should be (2*num_samples, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (2*num_samples,)\n",
    "\n",
    "    return augmented_data, augmented_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-b-8F6wVNtOB"
   },
   "source": [
    "suimultaneously train AutoEncoder after layer with 128 features and DNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmM3s1P18B79"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def create_Autoencoder_DNN(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)  # Use transform instead of fit_transform for test data\n",
    "\n",
    "    # Convert y_train and y_test to numpy arrays\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # Define the dimensions\n",
    "    input_dim = len(x_train[0])\n",
    "    encoding_dim = 128\n",
    "    hidden_dim_1 = 64\n",
    "    output_dim = 1\n",
    "\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,), name='input_layer')\n",
    "\n",
    "    # Encoding layer (Autoencoder)\n",
    "    encoded = Dense(encoding_dim, activation='relu', kernel_regularizer=l2(0.01), name='encoded_layer')(input_layer)\n",
    "\n",
    "    # Decoding layer (Autoencoder)\n",
    "    decoded = Dense(input_dim, activation='sigmoid', name='decoded_output')(encoded)\n",
    "\n",
    "    # Classification layers (DNN)\n",
    "    classification_hidden_1 = Dense(hidden_dim_1, activation='relu', kernel_regularizer=l2(0.01), name='classification_hidden_1')(encoded)\n",
    "    classification_output = Dense(output_dim, activation='sigmoid', name='classification_output')(classification_hidden_1)\n",
    "\n",
    "    # Combined model\n",
    "    combined_model = Model(inputs=input_layer, outputs=[decoded, classification_output])\n",
    "\n",
    "    # Compile the model\n",
    "    combined_model.compile(optimizer='adam',\n",
    "                           loss={'decoded_output': 'mse', 'classification_output': 'binary_crossentropy'},\n",
    "                           loss_weights={'decoded_output': 1.0, 'classification_output': 1.0})\n",
    "\n",
    "    # Print model summary\n",
    "    combined_model.summary()\n",
    "\n",
    "    # Train the combined model\n",
    "    history = combined_model.fit(x_train, {'decoded_output': x_train, 'classification_output': y_train},\n",
    "                                 epochs=500,\n",
    "                                 batch_size=64,\n",
    "                                 shuffle=True,\n",
    "                                 validation_data=(x_test, {'decoded_output': x_test, 'classification_output': y_test}),\n",
    "                                 verbose=0)\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    losses = combined_model.evaluate(x_test, {'decoded_output': x_test, 'classification_output': y_test}, verbose=0)\n",
    "    total_loss = losses[0]\n",
    "    mse_loss = losses[1]\n",
    "    bce_loss = losses[2]\n",
    "\n",
    "    print(f'Test Total Loss: {total_loss}')\n",
    "    print(f'Test MSE Loss: {mse_loss}')\n",
    "    print(f'Test BCE Loss: {bce_loss}')\n",
    "\n",
    "    # Predict the labels\n",
    "    y_pred_probs = combined_model.predict(x_test, verbose=0)[1]\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "    # Calculate accuracy, sensitivity, and specificity\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Sensitivity: {sensitivity}')\n",
    "    print(f'Specificity: {specificity}')\n",
    "\n",
    "    return accuracy, sensitivity, specificity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46tU7HuCfHH7"
   },
   "source": [
    "simultanously autoencoder and DNN after 64 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8CWzmzgfTXi"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def create_Autoencoder_DNN2(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)  # Use transform instead of fit_transform for test data\n",
    "\n",
    "    # Convert y_train and y_test to numpy arrays\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # Define the dimensions\n",
    "    input_dim = len(x_train[0])\n",
    "    encoding_dim = 128\n",
    "    hidden_dim_1 = 64\n",
    "    output_dim = 1\n",
    "\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,), name='input_layer')\n",
    "\n",
    "    # Encoding layer (Autoencoder)\n",
    "    encoded = Dense(encoding_dim, activation='relu', kernel_regularizer=l2(0.01), name='encoded_layer')(input_layer)\n",
    "\n",
    "    # Intermediate hidden layer for features\n",
    "    hidden_layer = Dense(hidden_dim_1, activation='relu', kernel_regularizer=l2(0.01), name='hidden_layer')(encoded)\n",
    "\n",
    "    # Decoding layer (Autoencoder)\n",
    "    decoded = Dense(input_dim, activation='sigmoid', name='decoded_output')(hidden_layer)\n",
    "\n",
    "    # Classification output layer\n",
    "    classification_output = Dense(output_dim, activation='sigmoid', name='classification_output')(hidden_layer)\n",
    "\n",
    "    # Combined model\n",
    "    combined_model = Model(inputs=input_layer, outputs=[decoded, classification_output])\n",
    "\n",
    "    # Compile the model\n",
    "    combined_model.compile(optimizer='adam',\n",
    "                           loss={'decoded_output': 'mse', 'classification_output': 'binary_crossentropy'},\n",
    "                           loss_weights={'decoded_output': 1.0, 'classification_output': 1.0})\n",
    "\n",
    "    # Print model summary\n",
    "    combined_model.summary()\n",
    "\n",
    "    # Train the combined model\n",
    "    history = combined_model.fit(x_train, {'decoded_output': x_train, 'classification_output': y_train},\n",
    "                                 epochs=500,\n",
    "                                 batch_size=64,\n",
    "                                 shuffle=True,\n",
    "                                 validation_data=(x_test, {'decoded_output': x_test, 'classification_output': y_test}),\n",
    "                                 verbose=0)\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    losses = combined_model.evaluate(x_test, {'decoded_output': x_test, 'classification_output': y_test}, verbose=0)\n",
    "    total_loss = losses[0]\n",
    "    mse_loss = losses[1]\n",
    "    bce_loss = losses[2]\n",
    "\n",
    "    print(f'Test Total Loss: {total_loss}')\n",
    "    print(f'Test MSE Loss: {mse_loss}')\n",
    "    print(f'Test BCE Loss: {bce_loss}')\n",
    "\n",
    "    # Predict the labels\n",
    "    y_pred_probs = combined_model.predict(x_test, verbose=0)[1]\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "    # Calculate accuracy, sensitivity, and specificity\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Sensitivity: {sensitivity}')\n",
    "    print(f'Specificity: {specificity}')\n",
    "\n",
    "    return accuracy, sensitivity, specificity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYwOF9SuXfbT",
    "outputId": "2efe956c-ac17-4859-a4cc-b995f7235228"
   },
   "outputs": [],
   "source": [
    "if p_Method == \"DNN\":\n",
    "\n",
    "    overall_res_auc = []\n",
    "    #clf = SVC(C = 5 , gamma=0.01 , kernel='rbf') if  p_Method == 'SVM' else RandomForestClassifier(n_estimators=100)\n",
    "    overall_result = []\n",
    "    for rp in range(10):\n",
    "        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
    "        np.random.shuffle(flist)\n",
    "        y_arr = np.array([get_label(f) for f in flist])\n",
    "        res = []\n",
    "        for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
    "            train_samples, test_samples = np.array(flist)[train_index], np.array(flist)[test_index]\n",
    "            print(len(train_samples))\n",
    "            print(len(test_samples))\n",
    "            train_data = []\n",
    "            train_labels = []\n",
    "            test_data = []\n",
    "            test_labels = []\n",
    "\n",
    "            #train_data , train_labels = augment_extra_tree(train_samples)\n",
    "            #print(train_data)\n",
    "\n",
    "            for i in train_samples:\n",
    "                train_data.append(all_corr[i][0])\n",
    "                #test_data.append(np.concatenate((all_corr[i][0], meta_data_dict[i]), axis=0))\n",
    "                train_labels.append(all_corr[i][1])\n",
    "\n",
    "            for i in test_samples:\n",
    "                test_data.append(all_corr[i][0])\n",
    "                #test_data.append(np.concatenate((all_corr[i][0], meta_data_dict[i]), axis=0))\n",
    "                test_labels.append(all_corr[i][1])\n",
    "\n",
    "            print(len(train_data))\n",
    "            print(len(train_labels))\n",
    "            print(\"len test is :\" , len(test_data))\n",
    "            #train_labels = np.array(train_labels)\n",
    "            #test_labels = np.array(test_labels)\n",
    "            res.append(create_Autoencoder_DNN2(train_data ,train_labels, test_data, test_labels))\n",
    "            #overall_res_auc.append(create_dnn_model(train_data ,train_labels, test_data, test_labels))\n",
    "            #print('res is:' , res)\n",
    "        print(\"repeat: \",rp,np.mean(res, axis=0).tolist())\n",
    "        overall_result.append(np.mean(res, axis=0).tolist())\n",
    "    print(\"---------------Result of repeating 10 times-------------------\")\n",
    "    print(np.mean(np.array(overall_result), axis=0).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training the svm model in third implementation we have**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxyXVg0YjYF1"
   },
   "source": [
    "svm parameter tuning with GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "id": "-ye_sYTPxaai",
    "outputId": "e142c5c7-1760-4635-f0fc-ec1c584ee782"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if p_Method != \"ASD-DiagNet\" and p_mode == \"whole\":\n",
    "    whole_train_data = []\n",
    "    whole_test_data = []\n",
    "    whole_train_label = []\n",
    "    whole_test_label = []\n",
    "    param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'poly', 'linear']\n",
    "    }\n",
    "\n",
    "    svm = SVC()\n",
    "    grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "    overall_result = []\n",
    "    for rp in range(10):\n",
    "        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
    "        np.random.shuffle(flist)\n",
    "        y_arr = np.array([get_label(f) for f in flist])\n",
    "        res = []\n",
    "        for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
    "            train_samples, test_samples = np.array(flist)[train_index], np.array(flist)[test_index]\n",
    "            train_data = []\n",
    "            train_labels = []\n",
    "            test_data = []\n",
    "            test_labels = []\n",
    "\n",
    "            train_data , train_labels = augment_data_alpha_gan_2_layer(train_samples)\n",
    "\n",
    "\n",
    "\n",
    "            grid_search.fit(train_data,train_labels)\n",
    "            best_params = grid_search.best_params_\n",
    "            best_score = grid_search.best_score_\n",
    "\n",
    "            print(\"Best Parameters:\", best_params)\n",
    "            print(\"Best Score:\", best_score)\n",
    "            #pr = clf.predict(train_data)\n",
    "            #res.append(confusion(train_labels,pr))\n",
    "\n",
    "        #print(\"repeat: \",rp,np.mean(res, axis=0).tolist())\n",
    "        #overall_result.append(np.mean(res, axis=0).tolist())\n",
    "    #print(\"---------------Result of repeating 10 times-------------------\")\n",
    "    #print(np.mean(np.array(overall_result), axis=0).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0y_r9jlwje40"
   },
   "source": [
    "training the model with tunned hyper parameters in 10 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Io86Ywz7mjpL",
    "outputId": "24430ca9-d8fe-4ea9-9ee8-2c5ba6217dfc"
   },
   "outputs": [],
   "source": [
    "if p_Method != \"ASD-DiagNet\" and p_mode == \"whole\":\n",
    "\n",
    "    overall_res_auc = []\n",
    "    all_false_positives = []\n",
    "    all_false_negatives = []\n",
    "    clf = SVC(C = 50 , gamma=0.0001 , kernel='rbf')\n",
    "    overall_result = []\n",
    "    overall_train_result = []\n",
    "    for rp in range(10):\n",
    "        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
    "        np.random.shuffle(flist)\n",
    "        y_arr = np.array([get_label(f) for f in flist])\n",
    "        res = []\n",
    "        train_res = []\n",
    "        for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
    "            train_samples, test_samples = np.array(flist)[train_index], np.array(flist)[test_index]\n",
    "            print(len(train_samples))\n",
    "            print(len(test_samples))\n",
    "            train_data = []\n",
    "            train_labels = []\n",
    "            test_data = []\n",
    "            test_labels = []\n",
    "            #duplics = find_duplicates(concatenated_list)\n",
    "            #concatenated_list = train_samples + test_samples\n",
    "            #list_counts = Counter(concatenated_list)\n",
    "            #print('duplics is:' , duplics)\n",
    "\n",
    "            train_data , train_labels = augment_data_alpha_gan_2_layer(train_samples)\n",
    "            #print(train_data)\n",
    "\n",
    "            for i in test_samples:\n",
    "                test_data.append(all_corr[i][0])\n",
    "                #test_data.append(np.concatenate((all_corr[i][0], meta_data_dict[i]), axis=0))\n",
    "                test_labels.append(all_corr[i][1])\n",
    "\n",
    "            #normalizing test data\n",
    "            #test_data = torch.tensor(test_data, dtype=torch.float)\n",
    "            #test_data = (test_data - test_data.mean(dim=0)) / test_data.std(dim=0)\n",
    "            #test_labels = torch.tensor(test_labels, dtype=torch.float32).view(-1, 1)\n",
    "            print(len(train_data))\n",
    "            print(\"len test is :\" , len(test_data))\n",
    "            clf.fit(train_data,train_labels)\n",
    "            pr = clf.predict(test_data)\n",
    "            train_pr = clf.predict(train_data)\n",
    "\n",
    "            train_labels = np.array(train_labels)\n",
    "            pr = np.array(pr)\n",
    "            train_data = np.array(train_data)\n",
    "\n",
    "            test_labels = np.array(test_labels)\n",
    "            train_pr = np.array(train_pr)\n",
    "            test_data = np.array(test_data)\n",
    "\n",
    "            # Identify False Positives (FP) and False Negatives (FN)\n",
    "            false_positives = test_samples[(test_labels == 0) & (pr == 1)]\n",
    "            false_negatives = test_samples[(test_labels == 1) & (pr == 0)]\n",
    "\n",
    "            all_false_positives.extend(false_positives)\n",
    "            all_false_negatives.extend(false_negatives)\n",
    "\n",
    "            # Print False Positives and False Negatives\n",
    "            print(\"False Positives (Predicted 1, Actual 0):\")\n",
    "            print(false_positives)\n",
    "            print(\"False Negatives (Predicted 0, Actual 1):\")\n",
    "            print(false_negatives)\n",
    "            res.append(confusion(test_labels,pr))\n",
    "            train_res.append(confusion(train_labels,train_pr))\n",
    "            overall_res_auc.append(confusion(test_labels,pr))\n",
    "            #print('res is:' , res)\n",
    "        print(\"repeat: \",rp,np.mean(res, axis=0).tolist())\n",
    "        print(\"repeat: \",rp,np.mean(train_res, axis=0).tolist())\n",
    "        overall_result.append(np.mean(res, axis=0).tolist())\n",
    "        overall_train_result.append(np.mean(train_res, axis=0).tolist())\n",
    "    print(\"---------------Result of repeating 10 times-------------------\")\n",
    "    print(np.mean(np.array(overall_result), axis=0).tolist())\n",
    "    print(np.mean(np.array(overall_train_result), axis=0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_T6ItbVAb1OH"
   },
   "outputs": [],
   "source": [
    "print(overall_res_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUV9Lp5Ujt4S"
   },
   "source": [
    "selcting best features , augmenting data and training the neural network model to train the model in first implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGqoYq55B7Jv"
   },
   "outputs": [],
   "source": [
    "class CC200Dataset(Dataset):\n",
    "    def __init__(self, pkl_filename=None, data=None, metadata=None,selective_regs=None, samples_list=None,\n",
    "                 augmentation=False, aug_factor=1, num_neighbs=5,\n",
    "                 eig_data=None, similarity_fn=None, verbose=False,regs=None):\n",
    "        self.regs=regs\n",
    "        if pkl_filename is not None:\n",
    "            if verbose:\n",
    "                print ('Loading ..!', end=' ')\n",
    "            self.data = pickle.load(open(pkl_filename, 'rb')) #all_corr dictionary that saved in pkl file earlier\n",
    "        elif data is not None:\n",
    "            self.data = data.copy()\n",
    "\n",
    "        else:\n",
    "            sys.stderr.write('Eigther PKL file or data is needed!')\n",
    "            return\n",
    "\n",
    "        if metadata is not None:\n",
    "          self.metadata = metadata.copy()\n",
    "\n",
    "        if selective_regs is not None:\n",
    "          self.selective_regs = selective_regs.copy()\n",
    "\n",
    "        #if verbose:\n",
    "        #    print ('Preprocess..!', end='  ')\n",
    "        if samples_list is None:\n",
    "            self.flist = [f for f in self.data]\n",
    "        else:\n",
    "            self.flist = [f for f in samples_list]\n",
    "        self.labels = np.array([self.data[f][1] for f in self.flist])\n",
    "\n",
    "        current_flist = np.array(self.flist.copy())\n",
    "        current_lab0_flist = current_flist[self.labels == 0]\n",
    "        current_lab1_flist = current_flist[self.labels == 1]\n",
    "        #if verbose:\n",
    "        #    print(' Num Positive : ', len(current_lab1_flist), end=' ')\n",
    "        #    print(' Num Negative : ', len(current_lab0_flist), end=' ')\n",
    "\n",
    "\n",
    "        if augmentation:\n",
    "            self.num_data = aug_factor * len(self.flist)\n",
    "            self.neighbors = {}\n",
    "            pbar = pyprind.ProgBar(len(self.flist))\n",
    "            weights = norm_weights(samples_list)#??\n",
    "            for f in self.flist:\n",
    "                label = self.data[f][1]\n",
    "                candidates = (set(current_lab0_flist) if label == 0 else set(current_lab1_flist))\n",
    "                candidates.remove(f)\n",
    "                eig_f = eig_data[f]['eigvecs']\n",
    "                sim_list = []\n",
    "                for cand in candidates:\n",
    "                    eig_cand = eig_data[cand]['eigvecs']\n",
    "                    sim = similarity_fn(eig_f, eig_cand,weights)\n",
    "                    sim_list.append((sim, cand))\n",
    "                sim_list.sort(key=lambda x: x[0], reverse=True)\n",
    "                self.neighbors[f] = [item[1] for item in sim_list[:num_neighbs]]#list(candidates)#[item[1] for item in sim_list[:num_neighbs]]\n",
    "\n",
    "        else:\n",
    "            self.num_data = len(self.flist)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < len(self.flist):\n",
    "            fname = self.flist[index]\n",
    "            data = self.data[fname][0].copy() #get_corr_data(fname, mode=cal_mode)\n",
    "            data = data[self.regs].copy()\n",
    "            if p_selective_regs:\n",
    "              selective_regs = self.selective_regs[fname].copy()\n",
    "              data = np.concatenate((data, selective_regs), axis=0)\n",
    "            if p_metadata:\n",
    "              metadata = self.metadata[fname].copy()\n",
    "              data = np.concatenate((data, metadata), axis=0)\n",
    "\n",
    "            label = (self.labels[index],)\n",
    "\n",
    "            return torch.FloatTensor(data), torch.FloatTensor(label)\n",
    "        else:\n",
    "            f1 = self.flist[index % len(self.flist)]\n",
    "            d1, y1 = self.data[f1][0], self.data[f1][1]\n",
    "            d1=d1[self.regs]\n",
    "            #metadata_d1 = self.metadata[f1].copy()\n",
    "            f2 = np.random.choice(self.neighbors[f1])\n",
    "            d2, y2 = self.data[f2][0], self.data[f2][1]\n",
    "            d2=d2[self.regs]\n",
    "            #metadata_d2 = self.metadata[f2].copy()\n",
    "            #if metadata is not None:\n",
    "            if p_metadata:\n",
    "              metadata_d1 = self.metadata[f1].copy()\n",
    "              metadata_d2 = self.metadata[f2].copy()\n",
    "              d1 = np.concatenate((d1, metadata_d1), axis=0)\n",
    "              d2 = np.concatenate((d2, metadata_d2), axis=0)\n",
    "\n",
    "            if p_selective_regs:\n",
    "              selective_regs_d1 = self.selective_regs[f1].copy()\n",
    "              selective_regs_d2 = self.selective_regs[f2].copy()\n",
    "              d1 = np.concatenate((d1, selective_regs_d1), axis=0)\n",
    "              d2 = np.concatenate((d2, selective_regs_d2), axis=0)\n",
    "\n",
    "            #d1 = np.concatenate((d1, metadata_d1), axis=0)\n",
    "            #d2 = np.concatenate((d2, metadata_d2), axis=0)\n",
    "            assert y1 == y2\n",
    "            r = np.random.uniform(low=0, high=1)\n",
    "            label = (y1,)\n",
    "            data = r*d1 + (1-r)*d2\n",
    "            return torch.FloatTensor(data), torch.FloatTensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OktBnGy2CBcv"
   },
   "outputs": [],
   "source": [
    "def get_loader(pkl_filename=None, data=None, metadata=None ,selective_regs=None, samples_list=None,\n",
    "               batch_size=64,\n",
    "               num_workers=1, mode='train',\n",
    "               *, augmentation=False, aug_factor=1, num_neighbs=5,\n",
    "                 eig_data=None, similarity_fn=None, verbose=False,regions=None):\n",
    "    \"\"\"Build and return data loader.\"\"\"\n",
    "    if mode == 'train':\n",
    "        shuffle = True\n",
    "    else:\n",
    "        shuffle = False\n",
    "        augmentation=False\n",
    "\n",
    "\n",
    "    dataset = CC200Dataset(pkl_filename=pkl_filename, data=data, metadata=metadata ,selective_regs=selective_regs, samples_list=samples_list,\n",
    "                            augmentation=augmentation, aug_factor=aug_factor,\n",
    "                            eig_data=eig_data, similarity_fn=similarity_fn, verbose=verbose,regs=regions)\n",
    "\n",
    "    data_loader = DataLoader(dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             num_workers=num_workers)\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mOaqqEGKCFN2",
    "outputId": "72033661-e3b6-4b45-a2b3-1b2ab1128ed7"
   },
   "outputs": [],
   "source": [
    "class MTAutoEncoder(nn.Module):\n",
    "    def __init__(self, num_inputs=990,\n",
    "                 num_latent=200, tied=True,\n",
    "                 num_classes=2, use_dropout=False):\n",
    "        super(MTAutoEncoder, self).__init__()\n",
    "        self.tied = tied\n",
    "        self.num_latent = num_latent\n",
    "\n",
    "        self.fc_encoder = nn.Linear(num_inputs, num_latent)\n",
    "\n",
    "        if not tied:\n",
    "            self.fc_decoder = nn.Linear(num_latent, num_inputs)\n",
    "\n",
    "        self.fc_encoder = nn.Linear(num_inputs, num_latent)\n",
    "\n",
    "        if use_dropout:\n",
    "            self.classifier = nn.Sequential (\n",
    "                nn.Dropout(p=0.5),\n",
    "                nn.Linear(self.num_latent, 1),\n",
    "                #nn.Linear(self.num_latent+28, 1),\n",
    "\n",
    "            )\n",
    "        else:\n",
    "            self.classifier = nn.Sequential (\n",
    "                nn.Linear(self.num_latent, 1),\n",
    "                #nn.Linear(self.num_latent+28, 1),\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x, eval_classifier=False):\n",
    "        if p_metadata:\n",
    "          additional_columns = x[:, -5:].clone()\n",
    "          x = x[:, :-5]\n",
    "\n",
    "        if p_selective_regs:\n",
    "          additional_columns = x[:, -28:].clone()\n",
    "          x = x[:, :-28]\n",
    "\n",
    "        x = self.fc_encoder(x)\n",
    "        x = torch.tanh(x)\n",
    "        if p_metadata:\n",
    "          x = torch.cat((x, additional_columns), dim=1)\n",
    "        if p_selective_regs:\n",
    "          x = torch.cat((x, additional_columns), dim=1)\n",
    "\n",
    "        if eval_classifier:\n",
    "            x_logit = self.classifier(x)\n",
    "        else:\n",
    "            x_logit = None\n",
    "\n",
    "        if self.tied:\n",
    "            if p_metadata:\n",
    "              additional_columns = x[:, -5:].clone()\n",
    "              x = x[:, :-5]\n",
    "\n",
    "            if p_selective_regs:\n",
    "              additional_columns = x[:, -28:].clone()\n",
    "              x = x[:, :-28]\n",
    "            x = F.linear(x, self.fc_encoder.weight.t())  #performs the decoder step\n",
    "            if p_metadata:\n",
    "              x = torch.cat((x, additional_columns), dim=1)\n",
    "\n",
    "            if p_selective_regs:\n",
    "              x = torch.cat((x, additional_columns), dim=1)\n",
    "        else:\n",
    "            x = self.fc_decoder(x) #passing through decoder _ reconstruct the original input data from the latent representation.\n",
    "\n",
    "        return x, x_logit\n",
    "\n",
    "mtae = MTAutoEncoder()\n",
    "\n",
    "mtae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zW5xUY3-CLRv"
   },
   "outputs": [],
   "source": [
    "def train(model, epoch, train_loader, p_bernoulli=None, mode='both', lam_factor=1.0):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for i,(batch_x,batch_y) in enumerate(train_loader):\n",
    "        if len(batch_x) != batch_size:\n",
    "            continue\n",
    "        if p_bernoulli is not None:\n",
    "            if i == 0:\n",
    "                p_tensor = torch.ones_like(batch_x).to(device)*p_bernoulli\n",
    "            rand_bernoulli = torch.bernoulli(p_tensor).to(device)\n",
    "\n",
    "\n",
    "        data, target = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if mode in ['both', 'ae']:\n",
    "            if p_bernoulli is not None:\n",
    "                rec_noisy, _ = model(data*rand_bernoulli, False)\n",
    "                loss_ae = criterion_ae(rec_noisy, data) / len(batch_x)\n",
    "            else:\n",
    "                rec, _ = model(data, False)\n",
    "                loss_ae = criterion_ae(rec, data) / len(batch_x)\n",
    "\n",
    "\n",
    "        if mode in ['both', 'clf']:\n",
    "            rec_clean, logits = model(data, True)\n",
    "            loss_clf = criterion_clf(logits, target)\n",
    "\n",
    "        if mode == 'both':\n",
    "            loss_total = loss_ae + lam_factor*loss_clf\n",
    "            train_losses.append([loss_ae.detach().cpu().numpy(),\n",
    "                                 loss_clf.detach().cpu().numpy()])\n",
    "        elif mode == 'ae':\n",
    "            loss_total = loss_ae\n",
    "            train_losses.append([loss_ae.detach().cpu().numpy(),\n",
    "                                 0.0])\n",
    "        elif mode == 'clf':\n",
    "            loss_total = loss_clf\n",
    "            train_losses.append([0.0,\n",
    "                                 loss_clf.detach().cpu().numpy()])\n",
    "\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_losses\n",
    "\n",
    "def test(model, criterion, test_loader,\n",
    "         eval_classifier=False, num_batch=None):\n",
    "    test_loss, n_test, correct = 0.0, 0, 0\n",
    "    all_predss=[]\n",
    "    if eval_classifier:\n",
    "        y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i,(batch_x,batch_y) in enumerate(test_loader, 1):\n",
    "            if num_batch is not None:\n",
    "                if i >= num_batch:\n",
    "                    continue\n",
    "            data = batch_x.to(device)\n",
    "            rec, logits = model(data, eval_classifier)\n",
    "\n",
    "            test_loss += criterion(rec, data).detach().cpu().numpy()\n",
    "            n_test += len(batch_x)\n",
    "            if eval_classifier:\n",
    "                proba = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "                preds = np.ones_like(proba, dtype=np.int32)\n",
    "                preds[proba < 0.5] = 0\n",
    "                all_predss.extend(preds)###????\n",
    "                y_arr = np.array(batch_y, dtype=np.int32)\n",
    "\n",
    "                correct += np.sum(preds == y_arr)\n",
    "                y_true.extend(y_arr.tolist())\n",
    "                y_pred.extend(proba.tolist())\n",
    "        mlp_acc,mlp_sens,mlp_spef = confusion(y_true,all_predss)\n",
    "\n",
    "    #print(\"saraaaa\")\n",
    "    #print(\"accuracy is :\" , mlp_acc)\n",
    "    return  mlp_acc,mlp_sens,mlp_spef#,correct/n_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LL_3lc9WCTr-",
    "outputId": "2a8ba1fb-3e06-4f00-9916-dfd070d2dfad"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nNAIozD6E_1t",
    "outputId": "e19b52dc-39df-4843-81ae-924dd57703c7"
   },
   "outputs": [],
   "source": [
    "\n",
    "if p_Method == \"ASD-DiagNet\" and p_mode == \"whole\":\n",
    "\n",
    "    num_corr = len(all_corr[flist[0]][0])\n",
    "    print(\"num_corr:  \",num_corr)\n",
    "\n",
    "    start =time.time()\n",
    "    batch_size = 8\n",
    "    learning_rate_ae, learning_rate_clf = 0.0001, 0.0001\n",
    "    num_epochs = 25\n",
    "\n",
    "    p_bernoulli = None\n",
    "    augmentation = p_augmentation\n",
    "    use_dropout = False\n",
    "\n",
    "    aug_factor = 2\n",
    "    num_neighbs = 5\n",
    "    lim4sim = 2\n",
    "    n_lat = int(num_corr/4)\n",
    "    print(n_lat)\n",
    "    start= time.time()\n",
    "\n",
    "    print('p_bernoulli: ', p_bernoulli)\n",
    "    print('augmentaiton: ', augmentation, 'aug_factor: ', aug_factor,\n",
    "          'num_neighbs: ', num_neighbs, 'lim4sim: ', lim4sim)\n",
    "    print('use_dropout: ', use_dropout, '\\n')\n",
    "\n",
    "\n",
    "    sim_function = functools.partial(cal_similarity, lim=lim4sim)\n",
    "    crossval_res_kol=[]\n",
    "    y_arr = np.array([get_label(f) for f in flist])\n",
    "    flist = np.array(flist)\n",
    "    kk=0\n",
    "    for rp in range(10):\n",
    "        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
    "        np.random.shuffle(flist)\n",
    "        y_arr = np.array([get_label(f) for f in flist])\n",
    "        for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
    "            train_samples, test_samples = flist[train_index], flist[test_index]\n",
    "\n",
    "\n",
    "            verbose = (True if (kk == 0) else False)\n",
    "\n",
    "            regions_inds = get_regs(train_samples,int(num_corr/4))\n",
    "\n",
    "            num_inpp = len(regions_inds)\n",
    "            n_lat = int(num_inpp/2)\n",
    "            train_loader=get_loader(data=all_corr, samples_list=train_samples,\n",
    "                                    batch_size=batch_size, mode='train',\n",
    "                                    augmentation=augmentation, aug_factor=aug_factor,\n",
    "                                    num_neighbs=num_neighbs, eig_data=eig_data, similarity_fn=sim_function,\n",
    "                                    verbose=verbose,regions=regions_inds)\n",
    "\n",
    "            test_loader=get_loader(data=all_corr, samples_list=test_samples,\n",
    "                                   batch_size=batch_size, mode='test', augmentation=False,\n",
    "                                   verbose=verbose,regions=regions_inds)\n",
    "\n",
    "            model = MTAutoEncoder(tied=True, num_inputs=num_inpp, num_latent=n_lat, use_dropout=use_dropout)\n",
    "            model.to(device)\n",
    "            criterion_ae = nn.MSELoss(reduction='sum')\n",
    "            criterion_clf = nn.BCEWithLogitsLoss()\n",
    "            optimizer = optim.SGD([{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n",
    "                                   {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
    "                                  momentum=0.9)\n",
    "\n",
    "            for epoch in range(1, num_epochs+1):\n",
    "                if epoch <= 20:\n",
    "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='both')\n",
    "                else:\n",
    "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='clf')\n",
    "\n",
    "\n",
    "            res_mlp = test(model, criterion_ae, test_loader, eval_classifier=True)\n",
    "            print(test(model, criterion_ae, test_loader, eval_classifier=True))\n",
    "            crossval_res_kol.append(res_mlp)\n",
    "        print(\"averages:\")\n",
    "        print(np.mean(np.array(crossval_res_kol),axis = 0))\n",
    "        finish= time.time()\n",
    "\n",
    "        print(finish-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zc8YjH0F_qzn",
    "outputId": "f0822ff0-c884-4689-ac8c-160afa473c69"
   },
   "outputs": [],
   "source": [
    "print(crossval_res_kol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qr8MCyHLkgk-"
   },
   "source": [
    "**setting the base lines of first implementation**\n",
    "\n",
    "random forest and svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnGRZDD5kqPV"
   },
   "source": [
    "tunning the hyper parameters of base line svm with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "Rx6ww8PAWboe",
    "outputId": "45bc024a-6a4b-43cf-c81e-6d8c6c01f63b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if p_Method != \"ASD-DiagNet\" and p_mode == \"whole\":\n",
    "    whole_train_data = []\n",
    "    whole_test_data = []\n",
    "    whole_train_label = []\n",
    "    whole_test_label = []\n",
    "    param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'linear', 'poly']\n",
    "    }\n",
    "\n",
    "    svm = SVC()\n",
    "    grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "    overall_result = []\n",
    "    for rp in range(10):\n",
    "        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
    "        np.random.shuffle(flist)\n",
    "        y_arr = np.array([get_label(f) for f in flist])\n",
    "        res = []\n",
    "        for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
    "            train_samples, test_samples = np.array(flist)[train_index], np.array(flist)[test_index]\n",
    "            train_data = []\n",
    "            train_labels = []\n",
    "            test_data = []\n",
    "            test_labels = []\n",
    "            train_data_augmented = []\n",
    "            train_labels_augmented = []\n",
    "\n",
    "            train_data_augmented , train_labels_augmented = augment_extra_tree(train_samples)\n",
    "            for i in train_samples:\n",
    "                #whole_train_data.append(all_corr[i][0])\n",
    "                #whole_train_label.append(all_corr[i][1])\n",
    "                train_data.append(all_corr[i][0])\n",
    "                #train_data.append(np.concatenate((all_corr[i][0], meta_data_dict[i]), axis=0))\n",
    "                train_labels.append(all_corr[i][1])\n",
    "\n",
    "            for i in test_samples:\n",
    "                test_data.append(all_corr[i][0])\n",
    "                #test_data.append(np.concatenate((all_corr[i][0], meta_data_dict[i]), axis=0))\n",
    "                test_labels.append(all_corr[i][1])\n",
    "\n",
    "            #train_data.extend(train_data_augmented)\n",
    "            #train_labels.extend(train_labels_augmented)\n",
    "            grid_search.fit(train_data,train_labels)\n",
    "            best_params = grid_search.best_params_\n",
    "            best_score = grid_search.best_score_\n",
    "\n",
    "            print(\"Best Parameters:\", best_params)\n",
    "            print(\"Best Score:\", best_score)\n",
    "            #pr = clf.predict(train_data)\n",
    "            #res.append(confusion(train_labels,pr))\n",
    "\n",
    "        #print(\"repeat: \",rp,np.mean(res, axis=0).tolist())\n",
    "        #overall_result.append(np.mean(res, axis=0).tolist())\n",
    "    #print(\"---------------Result of repeating 10 times-------------------\")\n",
    "    #print(np.mean(np.array(overall_result), axis=0).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qPvNgs4lCvQ"
   },
   "source": [
    "a function for ensuring that there is no same sample in both train set and test set in training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTwzmF04-PUu"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_duplicates(input_list):\n",
    "    # Count the occurrences of each item in the list\n",
    "    counts = Counter(input_list)\n",
    "\n",
    "    # Filter and return the items that occur more than once\n",
    "    duplicates = [item for item, count in counts.items() if count > 1]\n",
    "    return duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeDK5DqplXqy"
   },
   "source": [
    "training the first implementation's base line which is svm with tunned hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kg4xNmKrc_vd",
    "outputId": "59d797d7-2245-4e17-ebec-de887d0d0085"
   },
   "outputs": [],
   "source": [
    "if p_Method != \"ASD-DiagNet\" and p_mode == \"whole\":\n",
    "    whole_train_data = []\n",
    "    whole_test_data = []\n",
    "    whole_train_label = []\n",
    "    whole_test_label = []\n",
    "    clf = SVC(C = 2 , gamma=0.001 , kernel='rbf') if  p_Method == 'SVM' else RandomForestClassifier(n_estimators=100)\n",
    "    overall_result = []\n",
    "    for rp in range(10):\n",
    "        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
    "        np.random.shuffle(flist)\n",
    "        y_arr = np.array([get_label(f) for f in flist])\n",
    "        res = []\n",
    "        for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
    "            train_samples, test_samples = np.array(flist)[train_index], np.array(flist)[test_index]\n",
    "            print(len(train_samples))\n",
    "            print(len(test_samples))\n",
    "            #train_counts = Counter(train_samples)\n",
    "            #print('train_counts is' , train_counts)\n",
    "            #test_counts = Counter(test_samples)\n",
    "            #print('test_counts is' , test_counts)\n",
    "            train_data = []\n",
    "            train_labels = []\n",
    "            test_data = []\n",
    "            test_labels = []\n",
    "            train_data_augmented = []\n",
    "            train_labels_augmented = []\n",
    "            concatenated_list = []\n",
    "            concatenated_list.extend(train_samples)\n",
    "            concatenated_list.extend(test_samples)\n",
    "            duplics = find_duplicates(concatenated_list)\n",
    "            #concatenated_list = train_samples + test_samples\n",
    "            #list_counts = Counter(concatenated_list)\n",
    "            print('duplics is:' , duplics)\n",
    "\n",
    "            train_data_augmented , train_labels_augmented = augment_extra_tree(train_samples)\n",
    "            for i in train_samples:\n",
    "                train_data.append(all_corr[i][0])\n",
    "                #train_data.append(np.concatenate((all_corr[i][0], meta_data_dict[i]), axis=0))\n",
    "                train_labels.append(all_corr[i][1])\n",
    "\n",
    "            for i in test_samples:\n",
    "                test_data.append(all_corr[i][0])\n",
    "                #test_data.append(np.concatenate((all_corr[i][0], meta_data_dict[i]), axis=0))\n",
    "                test_labels.append(all_corr[i][1])\n",
    "\n",
    "            train_data.extend(train_data_augmented)\n",
    "            print(len(train_data))\n",
    "            print(\"len test is :\" , len(test_data))\n",
    "            train_labels.extend(train_labels_augmented)\n",
    "            clf.fit(train_data,train_labels)\n",
    "            pr = clf.predict(train_data)\n",
    "            res.append(confusion(train_labels,pr))\n",
    "\n",
    "        print(\"repeat: \",rp,np.mean(res, axis=0).tolist())\n",
    "        overall_result.append(np.mean(res, axis=0).tolist())\n",
    "    print(\"---------------Result of repeating 10 times-------------------\")\n",
    "    print(np.mean(np.array(overall_result), axis=0).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGS-97Fol6VX"
   },
   "source": [
    "using hyper parameter tunning for finding the best parameters of random forest as the first model's base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "MSAR5qSaTrtX",
    "outputId": "1c2edcb4-1d63-4dd7-dac7-6f0e47de5e92"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if p_Method != \"ASD-DiagNet\" and p_mode == \"whole\":\n",
    "    whole_train_data = []\n",
    "    whole_test_data = []\n",
    "    whole_train_label = []\n",
    "    whole_test_label = []\n",
    "    param_grid = {\n",
    "      'n_estimators': [100, 200, 300],\n",
    "      'max_depth': [None, 10, 20, 30],\n",
    "    }\n",
    "\n",
    "    svm = RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "    overall_result = []\n",
    "    for rp in range(10):\n",
    "        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
    "        np.random.shuffle(flist)\n",
    "        y_arr = np.array([get_label(f) for f in flist])\n",
    "        res = []\n",
    "        for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
    "            train_samples, test_samples = np.array(flist)[train_index], np.array(flist)[test_index]\n",
    "            train_data = []\n",
    "            train_labels = []\n",
    "            test_data = []\n",
    "            test_labels = []\n",
    "            train_data_augmented = []\n",
    "            train_labels_augmented = []\n",
    "\n",
    "            #train_data_augmented , train_labels_augmented = augment_extra_tree(train_samples)\n",
    "            for i in train_samples:\n",
    "                #whole_train_data.append(all_corr[i][0])\n",
    "                #whole_train_label.append(all_corr[i][1])\n",
    "                train_data.append(all_corr[i][0])\n",
    "                #train_data.append(np.concatenate((all_corr[i][0], meta_data_dict[i]), axis=0))\n",
    "                train_labels.append(all_corr[i][1])\n",
    "\n",
    "            for i in test_samples:\n",
    "                test_data.append(all_corr[i][0])\n",
    "                #test_data.append(np.concatenate((all_corr[i][0], meta_data_dict[i]), axis=0))\n",
    "                test_labels.append(all_corr[i][1])\n",
    "\n",
    "            #train_data.extend(train_data_augmented)\n",
    "            #train_labels.extend(train_labels_augmented)\n",
    "            grid_search.fit(train_data,train_labels)\n",
    "            best_params = grid_search.best_params_\n",
    "            best_score = grid_search.best_score_\n",
    "\n",
    "            print(\"Best Parameters:\", best_params)\n",
    "            print(\"Best Score:\", best_score)\n",
    "            #pr = clf.predict(train_data)\n",
    "            #res.append(confusion(train_labels,pr))\n",
    "\n",
    "        #print(\"repeat: \",rp,np.mean(res, axis=0).tolist())\n",
    "        #overall_result.append(np.mean(res, axis=0).tolist())\n",
    "    #print(\"---------------Result of repeating 10 times-------------------\")\n",
    "    #print(np.mean(np.array(overall_result), axis=0).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFotTLBKmWxl"
   },
   "source": [
    "training the base line model, random forest with tunned hyper parameters in above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fr01ZGimM2ol",
    "outputId": "bc479196-fc39-4667-c2da-9e0240614857"
   },
   "outputs": [],
   "source": [
    "if p_Method != \"ASD-DiagNet\" and p_mode == \"whole\":\n",
    "\n",
    "    print(p_Method)\n",
    "    overall_res_auc = []\n",
    "    clf = SVC(C=30 , gamma=0.001 , kernel='rbf') if  p_Method == 'SVM' else RandomForestClassifier(n_estimators=100)\n",
    "    overall_result = []\n",
    "    for rp in range(10):\n",
    "        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
    "        np.random.shuffle(flist)\n",
    "        y_arr = np.array([get_label(f) for f in flist])\n",
    "        res = []\n",
    "        for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
    "            train_samples, test_samples = np.array(flist)[train_index], np.array(flist)[test_index]\n",
    "            train_data = []\n",
    "            train_labels = []\n",
    "            test_data = []\n",
    "            test_labels = []\n",
    "\n",
    "            for i in train_samples:\n",
    "                train_data.append(all_corr[i][0])\n",
    "                train_labels.append(all_corr[i][1])\n",
    "\n",
    "            for i in test_samples:\n",
    "                test_data.append(all_corr[i][0])\n",
    "                test_labels.append(all_corr[i][1])\n",
    "\n",
    "\n",
    "            clf.fit(train_data,train_labels)\n",
    "            pr = clf.predict(test_data)\n",
    "            res.append(confusion(test_labels,pr))\n",
    "            overall_res_auc.append(confusion(test_labels,pr))\n",
    "\n",
    "        print(\"repeat: \",rp,np.mean(res, axis=0).tolist())\n",
    "        overall_result.append(np.mean(res, axis=0).tolist())\n",
    "    print(\"---------------Result of repeating 10 times-------------------\")\n",
    "    print(np.mean(np.array(overall_result), axis=0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fdkn_v-LOlFq",
    "outputId": "71cb36f8-3e06-457f-8527-d32d9ef20b53"
   },
   "outputs": [],
   "source": [
    "print(overall_res_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ru5P2Movp3y9"
   },
   "source": [
    "**all the usefull methods for implementing first and second model finish here**\n",
    "\n",
    "the unseuccesful senarios mentioned below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nhnnsslyfint"
   },
   "source": [
    "**unsuccesfull structures of GAN for augmenting data**\n",
    "these structures couldn't fit on the data and train in some cases and in other cases they couldn't conduct to better results as accuracy , sensitivity , specificity for the second model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3KX2KR3R7sQ"
   },
   "source": [
    "conditional wgan with encoder , decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FC3bUgauSBFz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def augment_fmri_data_icw_gan_additional_layer(sub_flist_samples):\n",
    "\n",
    "    num_epochs = 300\n",
    "    batch_size = 128\n",
    "    latent_dim = 128\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    lambda_gp = 20\n",
    "    input_dim = 1935\n",
    "    label_dim = 2\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    class Encoder(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "            super(Encoder, self).__init__()\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim1, hidden_dim2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim2, latent_dim)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.encoder(x)\n",
    "\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, latent_dim, label_dim, output_dim):\n",
    "            super(Generator, self).__init__()\n",
    "            self.label_embedding = nn.Embedding(label_dim, latent_dim)\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(latent_dim + latent_dim, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(512, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 2048),  # New layer\n",
    "                nn.ReLU(inplace=True),  # New layer\n",
    "                nn.Linear(2048, output_dim),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "        def forward(self, noise, labels):\n",
    "            c = self.label_embedding(labels)\n",
    "            x = torch.cat((noise, c), dim=1)\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, input_dim, label_dim):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.label_embedding = nn.Embedding(label_dim, input_dim)\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim + input_dim, 1024),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(256, 128),  # New layer\n",
    "                nn.LeakyReLU(0.2, inplace=True),  # New layer\n",
    "                nn.Linear(128, 1),\n",
    "            )\n",
    "\n",
    "        def forward(self, img, labels):\n",
    "            c = self.label_embedding(labels)\n",
    "            x = torch.cat((img, c), dim=1)\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "\n",
    "    class CodeDiscriminator(nn.Module):\n",
    "        def __init__(self, latent_dim, hidden_dim1, hidden_dim2):\n",
    "            super(CodeDiscriminator, self).__init__()\n",
    "            self.code_discriminator = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim1, hidden_dim2),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim2, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            return self.code_discriminator(z)\n",
    "\n",
    "    def compute_gradient_penalty(D, real_samples, fake_samples, labels, device):\n",
    "        alpha = torch.rand(real_samples.size(0), 1).to(device)\n",
    "        alpha = alpha.expand(real_samples.size())\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "        d_interpolates = D(interpolates, labels)\n",
    "        fake = torch.ones(d_interpolates.size(), requires_grad=False).to(device)\n",
    "        gradients = grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "\n",
    "\n",
    "    # Instantiate models\n",
    "    print(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "    encoder = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim).to(device)\n",
    "    generator = Generator(latent_dim, label_dim, input_dim).to(device)\n",
    "    discriminator = Discriminator(input_dim, label_dim).to(device)\n",
    "    code_discriminator = CodeDiscriminator(latent_dim, hidden_dim1, hidden_dim2).to(device)\n",
    "\n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "    code_loss = nn.BCELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_E = optim.Adam(encoder.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    optimizer_Dis = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    optimizer_CodeDis = optim.Adam(code_discriminator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_data = np.array([all_corr[f][0] for f in flist1])\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32).to(device)\n",
    "    labels = torch.tensor(array_of_labels, dtype=torch.long).to(device)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_data, labels) in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = torch.ones(batch_size, 1).to(device)\n",
    "            fake = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            optimizer_Dis.zero_grad()\n",
    "\n",
    "            # Encode real data and decode to generate fake data\n",
    "            latent_code = encoder(real_data)\n",
    "            gen_data = generator(latent_code, labels)\n",
    "\n",
    "            # Sample noise and labels as generator input\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            gen_labels = labels\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_data_gan = generator(z, gen_labels)\n",
    "\n",
    "            # Discriminator output\n",
    "            real_validity = discriminator(real_data, labels)\n",
    "            fake_validity = discriminator(gen_data.detach(), labels)\n",
    "            fake_validity_gan = discriminator(gen_data_gan.detach(), gen_labels)\n",
    "\n",
    "            # Gradient penalty\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_data.data, gen_data.data, labels.data, device)\n",
    "\n",
    "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + torch.mean(fake_validity_gan) + lambda_gp * gradient_penalty\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_Dis.step()\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Code Discriminator\n",
    "            # ---------------------\n",
    "            optimizer_CodeDis.zero_grad()\n",
    "\n",
    "            outputs_code_real = code_discriminator(latent_code)\n",
    "            outputs_code_fake = code_discriminator(torch.randn(batch_size, latent_dim).to(device))\n",
    "\n",
    "            code_loss_real = code_loss(outputs_code_real, valid)\n",
    "            code_loss_fake = code_loss(outputs_code_fake, fake)\n",
    "            code_dis_loss = code_loss_real + code_loss_fake\n",
    "\n",
    "            code_dis_loss.backward(retain_graph=True)\n",
    "            optimizer_CodeDis.step()\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Encoder and Generator\n",
    "            # -----------------\n",
    "            optimizer_E.zero_grad()\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            outputs_fake = discriminator(gen_data, labels)\n",
    "            gen_loss = adversarial_loss(outputs_fake, valid)\n",
    "\n",
    "            # Debugging shape issue\n",
    "            print(f\"Shape of gen_data: {gen_data.shape}\")\n",
    "            print(f\"Shape of real_data: {real_data.shape}\")\n",
    "\n",
    "            rec_loss = reconstruction_loss(gen_data, real_data)\n",
    "            total_gen_loss = gen_loss + rec_loss\n",
    "\n",
    "            total_gen_loss.backward(retain_graph=True)\n",
    "            optimizer_E.step()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_data_gan = generator(z, gen_labels)\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            fake_validity_gan = discriminator(gen_data_gan, gen_labels)\n",
    "            g_loss = -torch.mean(fake_validity_gan)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        # print(f\"Epoch [{epoch+1}/{num_epochs}] D loss: {d_loss.item()} G loss: {g_loss.item()}\")\n",
    "\n",
    "    # Generate synthetic samples\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        gen_labels = torch.tensor(array_of_labels, dtype=torch.long).to(device)\n",
    "        synthetic_data = generator(z, gen_labels).cpu().numpy()\n",
    "        synthetic_labels = gen_labels.cpu().numpy()\n",
    "        print(synthetic_labels.shape[0])\n",
    "\n",
    "    # Ensure synthetic_data has the correct shape (num_samples, input_dim)\n",
    "    if synthetic_data.shape[1] != input_dim:\n",
    "        raise ValueError(f\"Generated synthetic_data has incorrect shape: {synthetic_data.shape}. Expected ({num_samples}, {input_dim}).\")\n",
    "\n",
    "    # Concatenate data and labels\n",
    "    augmented_data = np.vstack((data.cpu().numpy(), synthetic_data))\n",
    "    augmented_labels = np.hstack((array_of_labels, synthetic_labels))\n",
    "\n",
    "    return augmented_data, augmented_labels\n",
    "\n",
    "# Example usage\n",
    "# Make sure to define `all_corr` and `train_samples` before calling this function\n",
    "# augmented_data, augmented_labels = augment_fmri_data_icw_gan_additional_layer(train_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmb9JZOnmplW"
   },
   "source": [
    "simple gann\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-BbaMIrmq_9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, output_dim),\n",
    "            nn.Tanh()  # Typically, the output activation is Tanh for GANs\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n",
    "\n",
    "def generate_data(num_samples, generator, latent_dim):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_codes = torch.randn(num_samples, latent_dim)\n",
    "        generated_data = generator(latent_codes)\n",
    "    return generated_data\n",
    "\n",
    "def train_2_layer_gan(sub_flist_samples):\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    input_dim = 1935\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    latent_dim = 128\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "\n",
    "    # Instantiate models\n",
    "    generator = Generator(latent_dim, hidden_dim1, hidden_dim2, input_dim)\n",
    "    discriminator = Discriminator(input_dim, hidden_dim1, hidden_dim2)\n",
    "\n",
    "    # Loss function\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_data = [all_corr[f][0].tolist() for f in sub_flist_samples]\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(array_of_labels, dtype=torch.float32)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data, _ in dataloader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Prepare real and fake labels\n",
    "            real_labels = torch.ones(batch_size, 1)\n",
    "            fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "            # Train the Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Generate fake data\n",
    "            z = torch.randn(batch_size, latent_dim)\n",
    "            fake_data = generator(z)\n",
    "\n",
    "            # Discriminator output\n",
    "            outputs_real = discriminator(real_data)\n",
    "            outputs_fake = discriminator(fake_data.detach())\n",
    "\n",
    "            # Loss for discriminator\n",
    "            dis_loss_real = adversarial_loss(outputs_real, real_labels)\n",
    "            dis_loss_fake = adversarial_loss(outputs_fake, fake_labels)\n",
    "            dis_loss = dis_loss_real + dis_loss_fake\n",
    "\n",
    "            dis_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train the Generator\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate fake data\n",
    "            outputs_fake = discriminator(fake_data)\n",
    "\n",
    "            # Loss for generator\n",
    "            gen_loss = adversarial_loss(outputs_fake, real_labels)\n",
    "\n",
    "            gen_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        # print(f'Epoch [{epoch+1}/{num_epochs}], Loss D: {dis_loss.item()}, Loss G: {gen_loss.item()}')\n",
    "\n",
    "    # Generate additional samples\n",
    "    synthetic_data = generate_data(num_samples, generator, latent_dim)\n",
    "\n",
    "    # Generating labels for synthetic data\n",
    "    synthetic_labels = labels[:num_samples]\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    augmented_labels_list = augmented_labels.tolist()\n",
    "    print('number of ones is:', augmented_labels_list.count(1))\n",
    "    print('number of zeros is:', augmented_labels_list.count(0))\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "    # DataLoader for training\n",
    "    augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inspect the augmented dataset\n",
    "    print(augmented_data.shape)  # Should be (2*num_samples, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (2*num_samples,)\n",
    "\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyz2a5zVM_DM"
   },
   "source": [
    "using 2 layer simple gan with gradient penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcNgEsLCNDXz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n",
    "\n",
    "def generate_data(num_samples, generator, latent_dim):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_codes = torch.randn(num_samples, latent_dim)\n",
    "        generated_data = generator(latent_codes)\n",
    "    return generated_data\n",
    "\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "    alpha = torch.rand(real_samples.size(0), 1)\n",
    "    alpha = alpha.expand_as(real_samples)\n",
    "\n",
    "    interpolates = alpha * real_samples + ((1 - alpha) * fake_samples)\n",
    "    interpolates = interpolates.requires_grad_(True)\n",
    "\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "\n",
    "    fake = torch.ones(real_samples.size(0), 1, requires_grad=False)\n",
    "\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "    return gradient_penalty\n",
    "\n",
    "def augment_data_gan_2_layer_gp(sub_flist_samples):\n",
    "    # Hyperparameters\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    input_dim = 1935\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    latent_dim = 128\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "    lambda_gp = 10  # Gradient penalty weight\n",
    "\n",
    "    # Instantiate models\n",
    "    generator = Generator(latent_dim, hidden_dim2, hidden_dim1, input_dim)\n",
    "    discriminator = Discriminator(input_dim, hidden_dim1, hidden_dim2)\n",
    "\n",
    "    # Loss function\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = []\n",
    "    for f in sub_flist_samples:\n",
    "      list_of_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(list_of_labels, dtype=torch.float32)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data, _ in dataloader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Prepare real and fake labels\n",
    "            real_labels = torch.ones(batch_size, 1)\n",
    "            fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "            # Train the Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            z = torch.randn(batch_size, latent_dim)\n",
    "            fake_data = generator(z)\n",
    "\n",
    "            real_validity = discriminator(real_data)\n",
    "            fake_validity = discriminator(fake_data.detach())\n",
    "\n",
    "            d_loss_real = adversarial_loss(real_validity, real_labels)\n",
    "            d_loss_fake = adversarial_loss(fake_validity, fake_labels)\n",
    "\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_data.data, fake_data.data)\n",
    "\n",
    "            d_loss = d_loss_real + d_loss_fake + lambda_gp * gradient_penalty\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train the Generator\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            generated_data = generator(z)\n",
    "            validity = discriminator(generated_data)\n",
    "\n",
    "            g_loss = adversarial_loss(validity, real_labels)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        # Optionally print the progress\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}], D Loss: {d_loss.item()}, G Loss: {g_loss.item()}')\n",
    "\n",
    "    # Generate additional samples\n",
    "    synthetic_data = generate_data(num_samples, generator, latent_dim)\n",
    "\n",
    "    # Generating labels for synthetic data\n",
    "    synthetic_labels = labels[:num_samples]\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    augmented_labels_list = augmented_labels.tolist()\n",
    "    print('number of ones is:', augmented_labels_list.count(1))\n",
    "    print('number of zeros is:', augmented_labels_list.count(0))\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "    # DataLoader for training\n",
    "    augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inspect the augmented dataset\n",
    "    print(augmented_data.shape)  # Should be (2*num_samples, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (2*num_samples,)\n",
    "\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsFrfVRraP3U"
   },
   "source": [
    "using conditional wGAN with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k5rJuPIsaS3P"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def augment_fmri_data_icw_gan(sub_flist_samples):\n",
    "\n",
    "    num_epochs = 200\n",
    "    batch_size = 64\n",
    "    latent_dim = 128\n",
    "    lambda_gp = 5\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, latent_dim, label_dim, output_dim):\n",
    "            super(Generator, self).__init__()\n",
    "            self.label_embedding = nn.Embedding(label_dim, latent_dim)\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(latent_dim + latent_dim, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(512, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, output_dim),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "        def forward(self, noise, labels):\n",
    "            c = self.label_embedding(labels)\n",
    "            x = torch.cat((noise, c), dim=1)\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, input_dim, label_dim):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.label_embedding = nn.Embedding(label_dim, input_dim)\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim + input_dim, 1024),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(256, 1),\n",
    "            )\n",
    "\n",
    "        def forward(self, img, labels):\n",
    "            c = self.label_embedding(labels)\n",
    "            x = torch.cat((img, c), dim=1)\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "\n",
    "    def compute_gradient_penalty(D, real_samples, fake_samples, labels, device):\n",
    "        alpha = torch.rand(real_samples.size(0), 1).to(device)\n",
    "        alpha = alpha.expand(real_samples.size())\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "        d_interpolates = D(interpolates, labels)\n",
    "        fake = torch.ones(d_interpolates.size(), requires_grad=False).to(device)\n",
    "        gradients = grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "\n",
    "    input_dim = 1935\n",
    "    label_dim = 2\n",
    "    num_samples = len(sub_flist_samples)\n",
    "\n",
    "    # Instantiate models\n",
    "    generator = Generator(latent_dim, label_dim, input_dim).to(device)\n",
    "    discriminator = Discriminator(input_dim, label_dim).to(device)\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_data = np.array([all_corr[f][0] for f in flist1])\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32).to(device)\n",
    "    labels = torch.tensor(array_of_labels, dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_data, labels) in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = torch.ones(batch_size, 1).to(device)\n",
    "            fake = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Sample noise and labels as generator input\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            gen_labels = labels\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_data = generator(z, gen_labels)\n",
    "\n",
    "            # Real data\n",
    "            real_validity = discriminator(real_data, labels)\n",
    "            # Fake data\n",
    "            fake_validity = discriminator(gen_data.detach(), gen_labels)\n",
    "\n",
    "            # Gradient penalty\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_data.data, gen_data.data, labels.data, device)\n",
    "\n",
    "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_data = generator(z, gen_labels)\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            fake_validity = discriminator(gen_data, gen_labels)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        #print(f\"Epoch [{epoch+1}/{num_epochs}] D loss: {d_loss.item()} G loss: {g_loss.item()}\")\n",
    "\n",
    "    # Generate synthetic samples\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        gen_labels = torch.tensor(array_of_labels, dtype=torch.long).to(device)\n",
    "        synthetic_data = generator(z, gen_labels).cpu().numpy()\n",
    "        synthetic_labels = gen_labels.cpu().numpy()\n",
    "        print(synthetic_labels.shape[0])\n",
    "\n",
    "    # Ensure synthetic_data has the correct shape (num_samples, input_dim)\n",
    "    if synthetic_data.shape[1] != input_dim:\n",
    "        raise ValueError(f\"Generated synthetic_data has incorrect shape: {synthetic_data.shape}. Expected ({num_samples}, {input_dim}).\")\n",
    "\n",
    "    # Concatenate data and labels\n",
    "    augmented_data = np.vstack((data.cpu().numpy(), synthetic_data))\n",
    "    augmented_labels = np.hstack((synthetic_labels, synthetic_labels))\n",
    "\n",
    "\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4ZrNmDdO9TD"
   },
   "source": [
    "adding 2 layer to icw_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KATrleJ-PAjF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def augment_fmri_data_icw_gan_2_additional_layer(sub_flist_samples):\n",
    "\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    latent_dim = 128\n",
    "    lambda_gp = 10\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, latent_dim, label_dim, output_dim):\n",
    "            super(Generator, self).__init__()\n",
    "            self.label_embedding = nn.Embedding(label_dim, latent_dim)\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(latent_dim + latent_dim, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(512, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 2048),  # New layer\n",
    "                nn.ReLU(inplace=True),  # New layer\n",
    "                nn.Linear(2048, 4096),  # New layer\n",
    "                nn.ReLU(inplace=True),  # New layer\n",
    "                nn.Linear(4096, output_dim),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "        def forward(self, noise, labels):\n",
    "            c = self.label_embedding(labels)\n",
    "            x = torch.cat((noise, c), dim=1)\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, input_dim, label_dim):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.label_embedding = nn.Embedding(label_dim, input_dim)\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim + input_dim, 1024),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(256, 128),  # New layer\n",
    "                nn.LeakyReLU(0.2, inplace=True),  # New layer\n",
    "                nn.Linear(128, 64),  # New layer\n",
    "                nn.LeakyReLU(0.2, inplace=True),  # New layer\n",
    "                nn.Linear(64, 1),\n",
    "            )\n",
    "\n",
    "        def forward(self, img, labels):\n",
    "            c = self.label_embedding(labels)\n",
    "            x = torch.cat((img, c), dim=1)\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "\n",
    "    def compute_gradient_penalty(D, real_samples, fake_samples, labels, device):\n",
    "        alpha = torch.rand(real_samples.size(0), 1).to(device)\n",
    "        alpha = alpha.expand(real_samples.size())\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "        d_interpolates = D(interpolates, labels)\n",
    "        fake = torch.ones(d_interpolates.size(), requires_grad=False).to(device)\n",
    "        gradients = grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "\n",
    "    input_dim = 1935\n",
    "    label_dim = 2\n",
    "    num_samples = len(sub_flist_samples)\n",
    "\n",
    "    # Instantiate models\n",
    "    generator = Generator(latent_dim, label_dim, input_dim).to(device)\n",
    "    discriminator = Discriminator(input_dim, label_dim).to(device)\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_data = np.array([all_corr[f][0] for f in flist1])\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32).to(device)\n",
    "    labels = torch.tensor(array_of_labels, dtype=torch.long).to(device)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_data, labels) in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = torch.ones(batch_size, 1).to(device)\n",
    "            fake = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Sample noise and labels as generator input\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            gen_labels = labels\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_data = generator(z, gen_labels)\n",
    "\n",
    "            # Real data\n",
    "            real_validity = discriminator(real_data, labels)\n",
    "            # Fake data\n",
    "            fake_validity = discriminator(gen_data.detach(), gen_labels)\n",
    "\n",
    "            # Gradient penalty\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_data.data, gen_data.data, labels.data, device)\n",
    "\n",
    "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_data = generator(z, gen_labels)\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            fake_validity = discriminator(gen_data, gen_labels)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        #print(f\"Epoch [{epoch+1}/{num_epochs}] D loss: {d_loss.item()} G loss: {g_loss.item()}\")\n",
    "\n",
    "    # Generate synthetic samples\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        gen_labels = torch.tensor(array_of_labels, dtype=torch.long).to(device)\n",
    "        synthetic_data = generator(z, gen_labels).cpu().numpy()\n",
    "        synthetic_labels = gen_labels.cpu().numpy()\n",
    "        print(synthetic_labels.shape[0])\n",
    "\n",
    "    # Ensure synthetic_data has the correct shape (num_samples, input_dim)\n",
    "    if synthetic_data.shape[1] != input_dim:\n",
    "        raise ValueError(f\"Generated synthetic_data has incorrect shape: {synthetic_data.shape}. Expected ({num_samples}, {input_dim}).\")\n",
    "\n",
    "    # Concatenate data and labels\n",
    "    augmented_data = np.vstack((data.cpu().numpy(), synthetic_data))\n",
    "    augmented_labels = np.hstack((synthetic_labels, synthetic_labels))\n",
    "\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNEFCBIFieeY"
   },
   "source": [
    "using gmm alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joCp0xEFig3Q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def augment_fmri_data_gmm(sub_flist_samples):\n",
    "\n",
    "    n_components=30\n",
    "    random_state=42\n",
    "    list_of_data_zeros = []\n",
    "    list_of_data_ones = []\n",
    "    #retrive data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    array_of_zeros_labels = array_of_labels[array_of_labels == 0]\n",
    "    array_of_ones_labels = array_of_labels[array_of_labels == 1]\n",
    "    for f in sub_flist_samples:\n",
    "        if all_corr[f][1] == 0:\n",
    "          list_of_data_zeros.append(all_corr[f][0].tolist())\n",
    "        if all_corr[f][1] == 1:\n",
    "          list_of_data_ones.append(all_corr[f][0].tolist())\n",
    "\n",
    "    array_of_data_zeros = np.array(list_of_data_zeros)\n",
    "    array_of_data_ones = np.array(list_of_data_ones)\n",
    "\n",
    "    #print(array_of_labels)\n",
    "    #print(array_of_zeros_labels)\n",
    "    #print(array_of_ones_labels)\n",
    "    #print(array_of_data_zeros)\n",
    "    #print(array_of_data_ones)\n",
    "\n",
    "    # Fit GMMs\n",
    "    gmm_class_0 = GaussianMixture(n_components=n_components, random_state=random_state)\n",
    "    gmm_class_1 = GaussianMixture(n_components=n_components, random_state=random_state)\n",
    "\n",
    "    gmm_class_0.fit(array_of_data_zeros)\n",
    "    gmm_class_1.fit(array_of_data_ones)\n",
    "\n",
    "    # Number of samples to generate\n",
    "    num_samples_zero = array_of_data_zeros.shape[0]\n",
    "    num_samples_one = array_of_data_ones.shape[0]\n",
    "    # Generate synthetic samples\n",
    "    synthetic_samples_class_0, _ = gmm_class_0.sample(num_samples_zero)\n",
    "    synthetic_samples_class_1, _ = gmm_class_1.sample(num_samples_one)\n",
    "\n",
    "    # Assign labels to synthetic samples\n",
    "    synthetic_labels_class_0 = np.zeros(num_samples_zero)\n",
    "    synthetic_labels_class_1 = np.ones(num_samples_one)\n",
    "\n",
    "    # Combine synthetic samples and labels\n",
    "    synthetic_data1 = np.vstack((array_of_data_zeros, array_of_data_ones))\n",
    "    synthetic_data2 = np.vstack((synthetic_samples_class_0, synthetic_samples_class_1))\n",
    "    augmented_data = np.vstack((synthetic_data1, synthetic_data2))\n",
    "\n",
    "    synthetic_labels1 = np.hstack((array_of_zeros_labels, array_of_ones_labels))\n",
    "    synthetic_labels2 = np.hstack((synthetic_labels_class_0, synthetic_labels_class_1))\n",
    "    augmented_labels = np.hstack((synthetic_labels1, synthetic_labels2))\n",
    "\n",
    "    return augmented_data , augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBcqm7xurdTT"
   },
   "source": [
    "using gmm with pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsNPX1IkrfcV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def augment_fmri_data_gmm_pca(sub_flist_samples):\n",
    "\n",
    "    n_components_gmm=25\n",
    "    n_components_pca=100\n",
    "    random_state=42\n",
    "    list_of_data_zeros = []\n",
    "    list_of_data_ones = []\n",
    "    # Retrieve data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    array_of_zeros_labels = array_of_labels[array_of_labels == 0]\n",
    "    array_of_ones_labels = array_of_labels[array_of_labels == 1]\n",
    "    for f in sub_flist_samples:\n",
    "        if all_corr[f][1] == 0:\n",
    "            list_of_data_zeros.append(all_corr[f][0].tolist())\n",
    "        if all_corr[f][1] == 1:\n",
    "            list_of_data_ones.append(all_corr[f][0].tolist())\n",
    "\n",
    "    array_of_data_zeros = np.array(list_of_data_zeros)\n",
    "    array_of_data_ones = np.array(list_of_data_ones)\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    data_zeros_scaled = scaler.fit_transform(array_of_data_zeros)\n",
    "    data_ones_scaled = scaler.fit_transform(array_of_data_ones)\n",
    "\n",
    "    # Apply PCA to reduce dimensions\n",
    "    pca = PCA(n_components=n_components_pca, random_state=random_state)\n",
    "    data_zeros_reduced = pca.fit_transform(data_zeros_scaled)\n",
    "    data_ones_reduced = pca.fit_transform(data_ones_scaled)\n",
    "\n",
    "    # Fit GMMs on the reduced data\n",
    "    gmm_class_0 = GaussianMixture(n_components=n_components_gmm, random_state=random_state)\n",
    "    gmm_class_1 = GaussianMixture(n_components=n_components_gmm, random_state=random_state)\n",
    "\n",
    "    gmm_class_0.fit(data_zeros_reduced)\n",
    "    gmm_class_1.fit(data_ones_reduced)\n",
    "\n",
    "    # Number of samples to generate\n",
    "    num_samples_zero = data_zeros_reduced.shape[0]\n",
    "    num_samples_one = data_ones_reduced.shape[0]\n",
    "\n",
    "    # Generate synthetic samples\n",
    "    synthetic_samples_class_0_reduced, _ = gmm_class_0.sample(num_samples_zero)\n",
    "    synthetic_samples_class_1_reduced, _ = gmm_class_1.sample(num_samples_one)\n",
    "\n",
    "    # Inverse transform the synthetic data back to the original scale\n",
    "    synthetic_samples_class_0_scaled = pca.inverse_transform(synthetic_samples_class_0_reduced)\n",
    "    synthetic_samples_class_1_scaled = pca.inverse_transform(synthetic_samples_class_1_reduced)\n",
    "    synthetic_samples_class_0 = scaler.inverse_transform(synthetic_samples_class_0_scaled)\n",
    "    synthetic_samples_class_1 = scaler.inverse_transform(synthetic_samples_class_1_scaled)\n",
    "\n",
    "    # Assign labels to synthetic samples\n",
    "    synthetic_labels_class_0 = np.zeros(num_samples_zero)\n",
    "    synthetic_labels_class_1 = np.ones(num_samples_one)\n",
    "\n",
    "    # Combine synthetic samples and labels with original data\n",
    "    original_data_combined = np.vstack((array_of_data_zeros, array_of_data_ones))\n",
    "    synthetic_data_combined = np.vstack((synthetic_samples_class_0, synthetic_samples_class_1))\n",
    "    augmented_data = np.vstack((original_data_combined, synthetic_data_combined))\n",
    "\n",
    "    original_labels_combined = np.hstack((array_of_zeros_labels, array_of_ones_labels))\n",
    "    synthetic_labels_combined = np.hstack((synthetic_labels_class_0, synthetic_labels_class_1))\n",
    "    augmented_labels = np.hstack((original_labels_combined, synthetic_labels_combined))\n",
    "\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kW8uQRLvJ9Zm"
   },
   "source": [
    "using gmm and encoder , decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0izYGPwtKCdj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def augment_data_with_gmm_encoder_decoder(sub_flist_samples):\n",
    "\n",
    "    num_components=15\n",
    "    input_dim=1935\n",
    "    hidden_dim1=512\n",
    "    hidden_dim2=256\n",
    "    latent_dim=128\n",
    "    learning_rate=0.001\n",
    "    num_epochs=100\n",
    "    batch_size=64\n",
    "\n",
    "    # Define the original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = []\n",
    "    for f in sub_flist_samples:\n",
    "        list_of_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "    data = np.array(list_of_data)\n",
    "    labels = np.array(list_of_labels)\n",
    "\n",
    "    data = torch.tensor(data, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    # Step 2: Define the Encoder and Decoder\n",
    "    class Encoder(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "            super(Encoder, self).__init__()\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim1, hidden_dim2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim2, latent_dim)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.encoder(x)\n",
    "\n",
    "    class Decoder(nn.Module):\n",
    "        def __init__(self, latent_dim, hidden_dim2, hidden_dim1, output_dim):\n",
    "            super(Decoder, self).__init__()\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim2, hidden_dim1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim1, output_dim)\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            return self.decoder(z)\n",
    "\n",
    "    encoder = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "    decoder = Decoder(latent_dim, hidden_dim2, hidden_dim1, input_dim)\n",
    "\n",
    "    # Step 3: Training the Encoder and Decoder (Autoencoder)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)\n",
    "\n",
    "    dataloader = DataLoader(TensorDataset(data, labels.unsqueeze(1)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, _ in dataloader:\n",
    "            # Forward pass\n",
    "            latent_codes = encoder(inputs)\n",
    "            outputs = decoder(latent_codes)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, inputs)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Step 4: Use GMM to Generate Synthetic Data in Lower Dimensional Space\n",
    "    # Encode the entire dataset to get the latent representation\n",
    "    with torch.no_grad():\n",
    "        encoded_data = encoder(data).numpy()\n",
    "\n",
    "    # Fit a GMM to the latent space\n",
    "    gmm = GaussianMixture(n_components=num_components, random_state=0)\n",
    "    gmm.fit(encoded_data)\n",
    "\n",
    "    # Generate synthetic data in the latent space\n",
    "    synthetic_latent_data = gmm.sample(len(data))[0]\n",
    "\n",
    "    # Step 5: Decode the Synthetic Data Back to Original Dimension\n",
    "    synthetic_latent_data = torch.tensor(synthetic_latent_data, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        synthetic_data = decoder(synthetic_latent_data).numpy()\n",
    "\n",
    "    # Generate labels for synthetic data (same as the original labels)\n",
    "    synthetic_labels = labels.numpy()\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = np.vstack((data.numpy(), synthetic_data))\n",
    "    augmented_labels = np.hstack((labels.numpy(), synthetic_labels))\n",
    "\n",
    "    # Verify the shapes\n",
    "    print('Original data shape:', data.shape)\n",
    "    print('Synthetic data shape:', synthetic_data.shape)\n",
    "    print('Augmented data shape:', augmented_data.shape)\n",
    "    print('Augmented labels shape:', augmented_labels.shape)\n",
    "\n",
    "    return augmented_data, augmented_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5LT1aYvAQRd"
   },
   "source": [
    "using vae and gan together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIw5gC0pAUpO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super(VAE_Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc31 = nn.Linear(hidden_dim2, latent_dim)\n",
    "        self.fc32 = nn.Linear(hidden_dim2, latent_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        h = self.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h)\n",
    "\n",
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim2, hidden_dim1, output_dim):\n",
    "        super(VAE_Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim2)\n",
    "        self.fc2 = nn.Linear(hidden_dim2, hidden_dim1)\n",
    "        self.fc3 = nn.Linear(hidden_dim1, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.relu(self.fc1(z))\n",
    "        h = self.relu(self.fc2(h))\n",
    "        return self.fc3(h)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.MSELoss()(recon_x, x)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "def generate_data(num_samples, decoder, latent_dim):\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_codes = torch.randn(num_samples, latent_dim)\n",
    "        generated_data = decoder(latent_codes)\n",
    "    return generated_data\n",
    "\n",
    "def augment_data_vae_gan(sub_flist_samples):\n",
    "    # Hyperparameters\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    input_dim = 1935\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    latent_dim = 128\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "\n",
    "    # Instantiate models\n",
    "    vae_encoder = VAE_Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "    vae_decoder = VAE_Decoder(latent_dim, hidden_dim2, hidden_dim1, input_dim)\n",
    "    discriminator = Discriminator(input_dim, hidden_dim1, hidden_dim2)\n",
    "\n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_VAE = optim.Adam(list(vae_encoder.parameters()) + list(vae_decoder.parameters()), lr=learning_rate)\n",
    "    optimizer_Dis = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = []\n",
    "    for f in sub_flist_samples:\n",
    "        list_of_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(list_of_labels, dtype=torch.float32)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data, _ in dataloader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Prepare real and fake labels\n",
    "            real_labels = torch.ones(batch_size, 1)\n",
    "            fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "            # VAE forward pass\n",
    "            mu, logvar = vae_encoder(real_data)\n",
    "            z = reparameterize(mu, logvar)\n",
    "            reconstructed_data = vae_decoder(z)\n",
    "\n",
    "            # Train the Discriminator\n",
    "            outputs_real = discriminator(real_data)\n",
    "            outputs_fake = discriminator(reconstructed_data.detach())\n",
    "\n",
    "            dis_loss_real = adversarial_loss(outputs_real, real_labels)\n",
    "            dis_loss_fake = adversarial_loss(outputs_fake, fake_labels)\n",
    "            dis_loss = dis_loss_real + dis_loss_fake\n",
    "\n",
    "            optimizer_Dis.zero_grad()\n",
    "            dis_loss.backward(retain_graph=True)\n",
    "            optimizer_Dis.step()\n",
    "\n",
    "            # Train the VAE (Encoder + Decoder)\n",
    "            outputs_fake = discriminator(reconstructed_data)\n",
    "            vae_adv_loss = adversarial_loss(outputs_fake, real_labels)\n",
    "            vae_rec_loss = loss_function(reconstructed_data, real_data, mu, logvar)\n",
    "            vae_loss = vae_adv_loss + vae_rec_loss\n",
    "\n",
    "            optimizer_VAE.zero_grad()\n",
    "            vae_loss.backward(retain_graph=True)\n",
    "            optimizer_VAE.step()\n",
    "\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {vae_loss.item()}')\n",
    "\n",
    "    # Generate additional samples\n",
    "    synthetic_data = generate_data(num_samples, vae_decoder, latent_dim)\n",
    "\n",
    "    # Generating labels for synthetic data\n",
    "    synthetic_labels = labels[:num_samples]\n",
    "\n",
    "    # If necessary, expand synthetic labels to match synthetic data size\n",
    "    #synthetic_labels = synthetic_labels.repeat(5)\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    augmented_labels_list = augmented_labels.tolist()\n",
    "    print('Number of ones is:', augmented_labels_list.count(1))\n",
    "    print('Number of zeros is:', augmented_labels_list.count(0))\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "    # DataLoader for training\n",
    "    augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inspect the augmented dataset\n",
    "    print(augmented_data.shape)\n",
    "    print(augmented_labels.shape)\n",
    "\n",
    "    return augmented_data, augmented_labels\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'all_corr' is your dictionary with fMRI data and 'sub_flist_samples' is a list of keys\n",
    "# augmented_data, augmented_labels = augment_data_vae_gan(sub_flist_samples, all_corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNPAykH0Lc2m"
   },
   "source": [
    "using vae and gan more complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8g6Y67xjLgAa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim):\n",
    "        super(VAE_Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.fc41 = nn.Linear(hidden_dim3, latent_dim)\n",
    "        self.fc42 = nn.Linear(hidden_dim3, latent_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        h = self.relu(self.fc2(h))\n",
    "        h = self.relu(self.fc3(h))\n",
    "        return self.fc41(h), self.fc42(h)\n",
    "\n",
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim3, hidden_dim2, hidden_dim1, output_dim):\n",
    "        super(VAE_Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim3)\n",
    "        self.fc2 = nn.Linear(hidden_dim3, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim1)\n",
    "        self.fc4 = nn.Linear(hidden_dim1, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.relu(self.fc1(z))\n",
    "        h = self.relu(self.fc2(h))\n",
    "        h = self.relu(self.fc3(h))\n",
    "        return self.fc4(h)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim3, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.MSELoss()(recon_x, x)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "def generate_data(num_samples, decoder, latent_dim):\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_codes = torch.randn(num_samples, latent_dim)\n",
    "        generated_data = decoder(latent_codes)\n",
    "    return generated_data\n",
    "\n",
    "def augment_data_vae_gan2(sub_flist_samples):\n",
    "    # Hyperparameters\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    input_dim = 1935\n",
    "    hidden_dim1 = 1024\n",
    "    hidden_dim2 = 512\n",
    "    hidden_dim3 = 256\n",
    "    latent_dim = 128\n",
    "    num_epochs = 200\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0001\n",
    "\n",
    "    # Instantiate models\n",
    "    vae_encoder = VAE_Encoder(input_dim, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim)\n",
    "    vae_decoder = VAE_Decoder(latent_dim, hidden_dim3, hidden_dim2, hidden_dim1, input_dim)\n",
    "    discriminator = Discriminator(input_dim, hidden_dim1, hidden_dim2, hidden_dim3)\n",
    "\n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_VAE = optim.Adam(list(vae_encoder.parameters()) + list(vae_decoder.parameters()), lr=learning_rate)\n",
    "    optimizer_Dis = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = []\n",
    "    for f in sub_flist_samples:\n",
    "        list_of_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(list_of_labels, dtype=torch.float32)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data, _ in dataloader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Prepare real and fake labels\n",
    "            real_labels = torch.ones(batch_size, 1)\n",
    "            fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "            # VAE forward pass\n",
    "            mu, logvar = vae_encoder(real_data)\n",
    "            z = reparameterize(mu, logvar)\n",
    "            reconstructed_data = vae_decoder(z)\n",
    "\n",
    "            # Train the Discriminator\n",
    "            outputs_real = discriminator(real_data)\n",
    "            outputs_fake = discriminator(reconstructed_data.detach())\n",
    "\n",
    "            dis_loss_real = adversarial_loss(outputs_real, real_labels)\n",
    "            dis_loss_fake = adversarial_loss(outputs_fake, fake_labels)\n",
    "            dis_loss = dis_loss_real + dis_loss_fake\n",
    "\n",
    "            optimizer_Dis.zero_grad()\n",
    "            dis_loss.backward(retain_graph=True)\n",
    "            optimizer_Dis.step()\n",
    "\n",
    "            # Train the VAE (Encoder + Decoder)\n",
    "            outputs_fake = discriminator(reconstructed_data)\n",
    "            vae_adv_loss = adversarial_loss(outputs_fake, real_labels)\n",
    "            vae_rec_loss = loss_function(reconstructed_data, real_data, mu, logvar)\n",
    "            vae_loss = vae_adv_loss + vae_rec_loss\n",
    "\n",
    "            optimizer_VAE.zero_grad()\n",
    "            vae_loss.backward(retain_graph=True)\n",
    "            optimizer_VAE.step()\n",
    "\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}], VAE Loss: {vae_loss.item()}, Discriminator Loss: {dis_loss.item()}')\n",
    "\n",
    "    # Generate additional samples\n",
    "    synthetic_data = generate_data(num_samples, vae_decoder, latent_dim)\n",
    "\n",
    "    # Generating labels for synthetic data\n",
    "    synthetic_labels = labels[:num_samples]\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    augmented_labels_list = augmented_labels.tolist()\n",
    "    print('Number of ones is:', augmented_labels_list.count(1))\n",
    "    print('Number of zeros is:', augmented_labels_list.count(0))\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "    # DataLoader for training\n",
    "    augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inspect the augmented dataset\n",
    "    print(augmented_data.shape)\n",
    "    print(augmented_labels.shape)\n",
    "\n",
    "    return augmented_data, augmented_labels\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'all_corr' is your dictionary with fMRI data and 'sub_flist_samples' is a list of keys\n",
    "# augmented_data, augmented_labels = augment_data_vae_gan(sub_flist_samples, all_corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DB6nb6tx_6JD"
   },
   "source": [
    "using conditional Gan structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WvYMgjGJaMy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "def augment_data_conditional_gan(sub_flist_samples):\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, input_dim, label_dim, hidden_dim, output_dim):\n",
    "            super(Generator, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim + label_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, output_dim)\n",
    "            )\n",
    "\n",
    "        def forward(self, z, labels):\n",
    "            x = torch.cat([z, labels], dim=1)\n",
    "            return self.model(x)\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, input_dim, label_dim, hidden_dim):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim + label_dim, hidden_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x, labels):\n",
    "            x = torch.cat([x, labels], dim=1)\n",
    "            return self.model(x)\n",
    "\n",
    "    # Hyperparameters\n",
    "    input_dim = 1935\n",
    "    label_dim = 1\n",
    "    latent_dim = 128\n",
    "    hidden_dim = 256\n",
    "    output_dim = input_dim\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Instantiate models\n",
    "    generator = Generator(latent_dim, label_dim, hidden_dim, output_dim).to(device)\n",
    "    discriminator = Discriminator(input_dim, label_dim, hidden_dim).to(device)\n",
    "\n",
    "    # Loss and optimizers\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = []\n",
    "    for f in sub_flist_samples:\n",
    "      list_of_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "    data = torch.tensor(list_of_data)\n",
    "    labels = torch.tensor(list_of_labels)\n",
    "\n",
    "    batch_size = 64\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_data, real_labels) in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            real_data = real_data.to(device)\n",
    "            real_labels = real_labels.to(device).unsqueeze(1)\n",
    "\n",
    "            # Create labels\n",
    "            real_targets = torch.ones(batch_size, 1).to(device)\n",
    "            fake_targets = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_labels = (torch.rand(batch_size, 1) > 0.5).float().to(device)\n",
    "            fake_data = generator(z, fake_labels)\n",
    "\n",
    "            outputs_real = discriminator(real_data, real_labels)\n",
    "            outputs_fake = discriminator(fake_data.detach(), fake_labels)\n",
    "\n",
    "            d_loss_real = criterion(outputs_real, real_targets)\n",
    "            d_loss_fake = criterion(outputs_fake, fake_targets)\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            fake_data = generator(z, fake_labels)\n",
    "            outputs = discriminator(fake_data, fake_labels)\n",
    "\n",
    "            g_loss = criterion(outputs, real_targets)\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item()}, g_loss: {g_loss.item()}')\n",
    "\n",
    "    # Generate additional samples\n",
    "    def generate_data(num_samples, generator, latent_dim):\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(num_samples, latent_dim).to(device)\n",
    "            labels = (torch.rand(num_samples, 1) > 0.5).float().to(device)\n",
    "            generated_data = generator(z, labels)\n",
    "        return generated_data, labels\n",
    "\n",
    "    # Generating synthetic data\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    synthetic_data, synthetic_labels = generate_data(num_samples, generator, latent_dim)\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data.cpu()], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels.cpu().squeeze()], dim=0)\n",
    "\n",
    "    # Verify the shape of augmented data and labels\n",
    "    print(augmented_data.shape)  # Should be (2 * num_samples, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (2 * num_samples,)\n",
    "\n",
    "    return augmented_data, augmented_labels\n",
    "\n",
    "# Example usage:\n",
    "# augmented_data, augmented_labels = augment_data_conditional_gan(data, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6kqwpDe043_"
   },
   "source": [
    "using least square gan structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKI_CN3l030G"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def augment_data_lsgan(sub_flist_samples):\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, latent_dim, output_dim):\n",
    "            super(Generator, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(latent_dim, 256),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(256, 512),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(512, 1024),\n",
    "                nn.ReLU(True),\n",
    "                nn.Linear(1024, output_dim),\n",
    "                nn.Tanh()  # Output values in the range [-1, 1]\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            return self.model(z)\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim, 1024),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(256, 1)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "    def train_lsgan(data, labels):\n",
    "\n",
    "        latent_dim=100\n",
    "        num_epochs=100\n",
    "        batch_size=64\n",
    "        lr=0.0002\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        data = data.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        dataset = TensorDataset(data, labels)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        generator = Generator(latent_dim, data.size(1)).to(device)\n",
    "        discriminator = Discriminator(data.size(1)).to(device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "        optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (real_data, _) in enumerate(dataloader):\n",
    "                batch_size = real_data.size(0)\n",
    "\n",
    "                real_data = real_data.to(device)\n",
    "\n",
    "                # Train Discriminator\n",
    "                optimizer_D.zero_grad()\n",
    "                valid = torch.ones(batch_size, 1).to(device)\n",
    "                fake = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "                real_loss = criterion(discriminator(real_data), valid)\n",
    "\n",
    "                z = torch.randn(batch_size, latent_dim).to(device)\n",
    "                fake_data = generator(z)\n",
    "                fake_loss = criterion(discriminator(fake_data.detach()), fake)\n",
    "\n",
    "                d_loss = 0.5 * (real_loss + fake_loss)\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "\n",
    "                # Train Generator\n",
    "                optimizer_G.zero_grad()\n",
    "                g_loss = criterion(discriminator(fake_data), valid)\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}] | D Loss: {d_loss.item()} | G Loss: {g_loss.item()}')\n",
    "\n",
    "        return generator\n",
    "\n",
    "    def generate_synthetic_data(generator, num_samples):\n",
    "        latent_dim=100\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        synthetic_data = generator(z).detach().cpu().numpy()\n",
    "        return synthetic_data\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = []\n",
    "    for f in sub_flist_samples:\n",
    "      list_of_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "    data = torch.tensor(list_of_data)\n",
    "    labels = torch.tensor(list_of_labels)\n",
    "\n",
    "    # Normalize data to be in range [-1, 1] as LSGAN output uses Tanh activation\n",
    "    data = (data - data.min()) / (data.max() - data.min()) * 2 - 1\n",
    "\n",
    "    generator = train_lsgan(data, labels)\n",
    "    synthetic_data = generate_synthetic_data(generator, len(sub_flist_samples))\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat((data, torch.tensor(synthetic_data, dtype=torch.float32)), dim=0)\n",
    "    augmented_labels = torch.cat((labels, labels), dim=0)\n",
    "\n",
    "    print(\"Augmented data shape:\", augmented_data.shape)\n",
    "    print(\"Augmented labels shape:\", augmented_labels.shape)\n",
    "\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyLI14GhkAdN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "def augment_data_lsgan(sub_flist_samples):\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "            super(Generator, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, output_dim)\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            return self.model(z)\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim, 1)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "    def generate_data(num_samples, generator, input_dim):\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(num_samples, input_dim).to(device)\n",
    "            generated_data = generator(z)\n",
    "        return generated_data\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "    n_critic = 5\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = [all_corr[f][0].tolist() for f in sub_flist_samples]\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(list_of_labels, dtype=torch.float32)\n",
    "\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Hyperparameters\n",
    "    input_dim = 1935\n",
    "    latent_dim = 128\n",
    "    hidden_dim = 256\n",
    "    output_dim = input_dim\n",
    "\n",
    "    # Instantiate models\n",
    "    generator = Generator(latent_dim, hidden_dim, output_dim).to(device)\n",
    "    discriminator = Discriminator(input_dim, hidden_dim).to(device)\n",
    "\n",
    "    # Loss and optimizers\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_data, _) in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            real_data = real_data.to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_data = generator(z)\n",
    "\n",
    "            real_validity = discriminator(real_data)\n",
    "            fake_validity = discriminator(fake_data.detach())\n",
    "\n",
    "            real_loss = criterion(real_validity, torch.ones_like(real_validity))\n",
    "            fake_loss = criterion(fake_validity, torch.zeros_like(fake_validity))\n",
    "            d_loss = 0.5 * (real_loss + fake_loss)\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train the generator every n_critic steps\n",
    "            if i % n_critic == 0:\n",
    "                optimizer_G.zero_grad()\n",
    "\n",
    "                generated_data = generator(z)\n",
    "                fake_validity = discriminator(generated_data)\n",
    "\n",
    "                g_loss = criterion(fake_validity, torch.ones_like(fake_validity))\n",
    "\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item()}, g_loss: {g_loss.item()}')\n",
    "\n",
    "    # Generating synthetic data\n",
    "    num_samples = len(sub_flist_samples) * 5  # Generating 5 times the original samples\n",
    "    synthetic_data = generate_data(num_samples, generator, latent_dim)\n",
    "\n",
    "    # Generating synthetic labels (assuming balanced classes)\n",
    "    ones = torch.ones(num_samples // 2)\n",
    "    zeros = torch.zeros(num_samples // 2)\n",
    "    synthetic_labels = torch.cat((ones, zeros), dim=0).float()\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data.cpu()], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    # Verify the shape of augmented data and labels\n",
    "    print(augmented_data.shape)  # Should be (num_samples * 6, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (num_samples * 6,)\n",
    "\n",
    "    return augmented_data, augmented_labels\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'all_corr' is a dictionary with your fMRI dataset and labels\n",
    "# sub_flist_samples is a list of keys to access the data from 'all_corr'\n",
    "# augmented_data, augmented_labels = augment_data_lsgan(sub_flist_samples, all_corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYh8tN1BVde-"
   },
   "source": [
    "using two layer conditional gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRT05svUVgEc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "def augment_data_2_layer_conditional_gan(sub_flist_samples):\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, input_dim, label_dim, hidden_dim, output_dim):\n",
    "            super(Generator, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim + label_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),  # Additional hidden layer\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, output_dim)\n",
    "            )\n",
    "\n",
    "        def forward(self, z, labels):\n",
    "            x = torch.cat([z, labels], dim=1)\n",
    "            return self.model(x)\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, input_dim, label_dim, hidden_dim):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim + label_dim, hidden_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim, hidden_dim),  # Additional hidden layer\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x, labels):\n",
    "            x = torch.cat([x, labels], dim=1)\n",
    "            return self.model(x)\n",
    "\n",
    "    # Hyperparameters\n",
    "    input_dim = 1935\n",
    "    label_dim = 1\n",
    "    latent_dim = 128\n",
    "    hidden_dim = 256\n",
    "    output_dim = input_dim\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Instantiate models\n",
    "    generator = Generator(latent_dim, label_dim, hidden_dim, output_dim).to(device)\n",
    "    discriminator = Discriminator(input_dim, label_dim, hidden_dim).to(device)\n",
    "\n",
    "    # Loss and optimizers\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = []\n",
    "    for f in sub_flist_samples:\n",
    "        list_of_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "    data = torch.tensor(list_of_data)\n",
    "    labels = torch.tensor(list_of_labels)\n",
    "\n",
    "    batch_size = 64\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_data, real_labels) in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            real_data = real_data.to(device)\n",
    "            real_labels = real_labels.to(device).unsqueeze(1)\n",
    "\n",
    "            # Create labels\n",
    "            real_targets = torch.ones(batch_size, 1).to(device)\n",
    "            fake_targets = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_labels = (torch.rand(batch_size, 1) > 0.5).float().to(device)\n",
    "            fake_data = generator(z, fake_labels)\n",
    "\n",
    "            outputs_real = discriminator(real_data, real_labels)\n",
    "            outputs_fake = discriminator(fake_data.detach(), fake_labels)\n",
    "\n",
    "            d_loss_real = criterion(outputs_real, real_targets)\n",
    "            d_loss_fake = criterion(outputs_fake, fake_targets)\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            fake_data = generator(z, fake_labels)\n",
    "            outputs = discriminator(fake_data, fake_labels)\n",
    "\n",
    "            g_loss = criterion(outputs, real_targets)\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "    # Generate additional samples\n",
    "    def generate_data(num_samples, generator, latent_dim):\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(num_samples, latent_dim).to(device)\n",
    "            labels = (torch.rand(num_samples, 1) > 0.5).float().to(device)\n",
    "            generated_data = generator(z, labels)\n",
    "        return generated_data, labels\n",
    "\n",
    "    # Generating synthetic data\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    synthetic_data, synthetic_labels = generate_data(num_samples, generator, latent_dim)\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data.cpu()], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels.cpu().squeeze()], dim=0)\n",
    "\n",
    "    # Verify the shape of augmented data and labels\n",
    "    print(augmented_data.shape)  # Should be (2 * num_samples, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (2 * num_samples,)\n",
    "\n",
    "    return augmented_data, augmented_labels\n",
    "\n",
    "# Example usage:\n",
    "# augmented_data, augmented_labels = augment_data_conditional_gan(data, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PC0lAO4_FKe"
   },
   "source": [
    "using conditional wgan without gradient penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-8K5kzM-2uw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "def augment_data_cwgan(sub_flist_samples):\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, latent_dim, label_dim, hidden_dim, output_dim):\n",
    "            super(Generator, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(latent_dim + label_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, output_dim)\n",
    "            )\n",
    "\n",
    "        def forward(self, z, labels):\n",
    "            x = torch.cat([z, labels], dim=1)\n",
    "            return self.model(x)\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, input_dim, label_dim, hidden_dim):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim + label_dim, hidden_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim, 1)\n",
    "            )\n",
    "\n",
    "        def forward(self, x, labels):\n",
    "            x = torch.cat([x, labels], dim=1)\n",
    "            return self.model(x)\n",
    "\n",
    "    def generate_data(num_samples, generator, latent_dim):\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(num_samples, latent_dim).to(device)\n",
    "            labels = (torch.rand(num_samples, 1) > 0.5).float().to(device)\n",
    "            generated_data = generator(z, labels)\n",
    "        return generated_data, labels\n",
    "\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "    n_critic = 5\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = [all_corr[f][0].tolist() for f in sub_flist_samples]\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(list_of_labels, dtype=torch.float32)\n",
    "\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Hyperparameters\n",
    "    input_dim = 1935\n",
    "    label_dim = 1\n",
    "    latent_dim = 128\n",
    "    hidden_dim = 256\n",
    "    output_dim = input_dim\n",
    "\n",
    "    # Instantiate models\n",
    "    generator = Generator(latent_dim, label_dim, hidden_dim, output_dim).to(device)\n",
    "    discriminator = Discriminator(input_dim, label_dim, hidden_dim).to(device)\n",
    "\n",
    "    # Loss and optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_data, real_labels) in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            real_data = real_data.to(device)\n",
    "            real_labels = real_labels.to(device).unsqueeze(1)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_labels = (torch.rand(batch_size, 1) > 0.5).float().to(device)\n",
    "            fake_data = generator(z, fake_labels)\n",
    "\n",
    "            real_validity = discriminator(real_data, real_labels)\n",
    "            fake_validity = discriminator(fake_data.detach(), fake_labels)\n",
    "\n",
    "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity)\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train the generator every n_critic steps\n",
    "            if i % n_critic == 0:\n",
    "                optimizer_G.zero_grad()\n",
    "\n",
    "                generated_data = generator(z, fake_labels)\n",
    "                g_loss = -torch.mean(discriminator(generated_data, fake_labels))\n",
    "\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item()}, g_loss: {g_loss.item()}')\n",
    "\n",
    "    # Generating synthetic data\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    synthetic_data, synthetic_labels = generate_data(num_samples, generator, latent_dim)\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data.cpu()], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels.cpu().squeeze()], dim=0)\n",
    "\n",
    "    # Verify the shape of augmented data and labels\n",
    "    print(augmented_data.shape)  # Should be (2 * num_samples, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (2 * num_samples,)\n",
    "\n",
    "    return augmented_data, augmented_labels\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'data' is your fMRI dataset with shape (871, 1935) and 'labels' is the corresponding labels with shape (871,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwCDYWBVW-PD"
   },
   "source": [
    "using alph gan 2 layer with temporal transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MfGWj_TXC2C"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim2, hidden_dim1, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n",
    "\n",
    "class CodeDiscriminator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim1, hidden_dim2):\n",
    "        super(CodeDiscriminator, self).__init__()\n",
    "        self.code_discriminator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.code_discriminator(z)\n",
    "\n",
    "def generate_data(num_samples, decoder, latent_dim):\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_codes = torch.randn(num_samples, latent_dim)\n",
    "        generated_data = decoder(latent_codes)\n",
    "    return generated_data\n",
    "\n",
    "def time_shift(data, shift):\n",
    "    return np.roll(data, shift)\n",
    "\n",
    "def add_temporal_noise(data, noise_level=0.1):\n",
    "    noise = noise_level * np.random.randn(*data.shape)\n",
    "    return data + noise\n",
    "\n",
    "def augment_data_alpha_gan_temporal(sub_flist_samples):\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    input_dim = 1935\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    latent_dim = 128\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "\n",
    "    # Instantiate models\n",
    "    encoder = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "    decoder = Decoder(latent_dim, hidden_dim2, hidden_dim1, input_dim)\n",
    "    discriminator = Discriminator(input_dim, hidden_dim1, hidden_dim2)\n",
    "    code_discriminator = CodeDiscriminator(latent_dim, hidden_dim1, hidden_dim2)\n",
    "\n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "    code_loss = nn.BCELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    optimizer_D = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    optimizer_Dis = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "    optimizer_CodeDis = optim.Adam(code_discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = []\n",
    "    for f in sub_flist_samples:\n",
    "        list_of_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(list_of_labels, dtype=torch.float32)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data, _ in dataloader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Prepare real and fake labels\n",
    "            real_labels = torch.ones(batch_size, 1)\n",
    "            fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "            # Train the Discriminator\n",
    "            latent_code = encoder(real_data)\n",
    "            reconstructed_data = decoder(latent_code)\n",
    "\n",
    "            outputs_real = discriminator(real_data)\n",
    "            outputs_fake = discriminator(reconstructed_data.detach())\n",
    "\n",
    "            dis_loss_real = adversarial_loss(outputs_real, real_labels)\n",
    "            dis_loss_fake = adversarial_loss(outputs_fake, fake_labels)\n",
    "            dis_loss = dis_loss_real + dis_loss_fake\n",
    "\n",
    "            optimizer_Dis.zero_grad()\n",
    "            dis_loss.backward(retain_graph=True)\n",
    "            optimizer_Dis.step()\n",
    "\n",
    "            # Train the Code Discriminator\n",
    "            outputs_code_real = code_discriminator(latent_code)\n",
    "            outputs_code_fake = code_discriminator(torch.randn(batch_size, latent_dim))\n",
    "\n",
    "            code_loss_real = code_loss(outputs_code_real, real_labels)\n",
    "            code_loss_fake = code_loss(outputs_code_fake, fake_labels)\n",
    "            code_dis_loss = code_loss_real + code_loss_fake\n",
    "\n",
    "            optimizer_CodeDis.zero_grad()\n",
    "            code_dis_loss.backward(retain_graph=True)\n",
    "            optimizer_CodeDis.step()\n",
    "\n",
    "            # Train the Encoder and Decoder (Generator)\n",
    "            outputs_fake = discriminator(reconstructed_data)\n",
    "            gen_loss = adversarial_loss(outputs_fake, real_labels)\n",
    "            rec_loss = reconstruction_loss(reconstructed_data, real_data)\n",
    "            total_gen_loss = gen_loss + rec_loss\n",
    "\n",
    "            optimizer_E.zero_grad()\n",
    "            optimizer_D.zero_grad()\n",
    "            total_gen_loss.backward(retain_graph=True)\n",
    "            optimizer_E.step()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_gen_loss.item()}')\n",
    "\n",
    "    # Generate additional samples\n",
    "    synthetic_data = generate_data(num_samples, decoder, latent_dim)\n",
    "\n",
    "    # Apply temporal transformations\n",
    "    augmented_synthetic_data = []\n",
    "    for sample in synthetic_data:\n",
    "        sample = sample.numpy()\n",
    "        sample = time_shift(sample, shift=np.random.randint(-10, 10))\n",
    "        sample = add_temporal_noise(sample, noise_level=np.random.uniform(0.01, 0.1))\n",
    "        augmented_synthetic_data.append(sample)\n",
    "\n",
    "    augmented_synthetic_data = torch.tensor(augmented_synthetic_data, dtype=torch.float32)\n",
    "\n",
    "    # Generating labels for synthetic data\n",
    "    synthetic_labels = labels[:num_samples]\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, augmented_synthetic_data], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    augmented_labels_list = augmented_labels.tolist()\n",
    "    print('number of ones is:', augmented_labels_list.count(1))\n",
    "    print('number of zeros is:', augmented_labels_list.count(0))\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "    # DataLoader for training\n",
    "    augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inspect the augmented dataset\n",
    "    print(augmented_data.shape)  # Should be (2*num_samples, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (2*num_samples,)\n",
    "\n",
    "    return augmented_data, augmented_labels\n",
    "\n",
    "# Example usage:\n",
    "# all_corr should be defined with your actual data\n",
    "# sub_flist_samples = [list of sample identifiers]\n",
    "# augmented_data, augmented_labels = augment_data_alpha_gan(sub_flist_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H870GSQWV3Vn"
   },
   "source": [
    "using alph gan 2 layer with spatial and temporal transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8iKdve8tV9vl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from scipy.ndimage import affine_transform\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim2, hidden_dim1, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n",
    "\n",
    "class CodeDiscriminator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim1, hidden_dim2):\n",
    "        super(CodeDiscriminator, self).__init__()\n",
    "        self.code_discriminator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.code_discriminator(z)\n",
    "\n",
    "def generate_data(num_samples, decoder, latent_dim):\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_codes = torch.randn(num_samples, latent_dim)\n",
    "        generated_data = decoder(latent_codes)\n",
    "    return generated_data\n",
    "\n",
    "def apply_affine_transform(sample, scale=1.0):\n",
    "    # Identity matrix with scaling factor for 1D data\n",
    "    affine_matrix = np.array([[scale]])\n",
    "    transformed_sample = affine_transform(sample, affine_matrix)\n",
    "    return transformed_sample\n",
    "\n",
    "def time_shift(sample, shift):\n",
    "    shifted_sample = np.roll(sample, shift)\n",
    "    return shifted_sample\n",
    "\n",
    "def add_temporal_noise(sample, noise_level=0.05):\n",
    "    noise = np.random.normal(0, noise_level, sample.shape)\n",
    "    noisy_sample = sample + noise\n",
    "    return noisy_sample\n",
    "\n",
    "def augment_data_alpha_gan_temporal_spatial(sub_flist_samples):\n",
    "    # Hyperparameters\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    input_dim = 1935\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    latent_dim = 128\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "\n",
    "    # Instantiate models\n",
    "    encoder = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "    decoder = Decoder(latent_dim, hidden_dim2, hidden_dim1, input_dim)\n",
    "    discriminator = Discriminator(input_dim, hidden_dim1, hidden_dim2)\n",
    "    code_discriminator = CodeDiscriminator(latent_dim, hidden_dim1, hidden_dim2)\n",
    "\n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "    code_loss = nn.BCELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    optimizer_D = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    optimizer_Dis = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "    optimizer_CodeDis = optim.Adam(code_discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = []\n",
    "    for f in sub_flist_samples:\n",
    "        list_of_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "    data = torch.tensor(list_of_data)\n",
    "    labels = torch.tensor(list_of_labels)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data, _ in dataloader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Prepare real and fake labels\n",
    "            real_labels = torch.ones(batch_size, 1)\n",
    "            fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "            # Train the Discriminator\n",
    "            latent_code = encoder(real_data)\n",
    "            reconstructed_data = decoder(latent_code)\n",
    "\n",
    "            outputs_real = discriminator(real_data)\n",
    "            outputs_fake = discriminator(reconstructed_data.detach())\n",
    "\n",
    "            dis_loss_real = adversarial_loss(outputs_real, real_labels)\n",
    "            dis_loss_fake = adversarial_loss(outputs_fake, fake_labels)\n",
    "            dis_loss = dis_loss_real + dis_loss_fake\n",
    "\n",
    "            optimizer_Dis.zero_grad()\n",
    "            dis_loss.backward(retain_graph=True)\n",
    "            optimizer_Dis.step()\n",
    "\n",
    "            # Train the Code Discriminator\n",
    "            outputs_code_real = code_discriminator(latent_code)\n",
    "            outputs_code_fake = code_discriminator(torch.randn(batch_size, latent_dim))\n",
    "\n",
    "            code_loss_real = code_loss(outputs_code_real, real_labels)\n",
    "            code_loss_fake = code_loss(outputs_code_fake, fake_labels)\n",
    "            code_dis_loss = code_loss_real + code_loss_fake\n",
    "\n",
    "            optimizer_CodeDis.zero_grad()\n",
    "            code_dis_loss.backward(retain_graph=True)\n",
    "            optimizer_CodeDis.step()\n",
    "\n",
    "            # Train the Encoder and Decoder (Generator)\n",
    "            outputs_fake = discriminator(reconstructed_data)\n",
    "            gen_loss = adversarial_loss(outputs_fake, real_labels)\n",
    "            rec_loss = reconstruction_loss(reconstructed_data, real_data)\n",
    "            total_gen_loss = gen_loss + rec_loss\n",
    "\n",
    "            optimizer_E.zero_grad()\n",
    "            optimizer_D.zero_grad()\n",
    "            total_gen_loss.backward(retain_graph=True)\n",
    "            optimizer_E.step()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_gen_loss.item()}')\n",
    "\n",
    "    # Generate additional samples\n",
    "    synthetic_data = generate_data(num_samples, decoder, latent_dim)\n",
    "\n",
    "    # Apply spatial and temporal transformations\n",
    "    transformed_synthetic_data = []\n",
    "    for sample in synthetic_data:\n",
    "        transformed_sample = sample.numpy().reshape(-1)  # Ensure sample is 1D\n",
    "\n",
    "        # Apply spatial transformation (scaling)\n",
    "        transformed_sample = apply_affine_transform(transformed_sample, scale=np.random.uniform(0.9, 1.1))\n",
    "\n",
    "        # Apply temporal transformation (time shift)\n",
    "        transformed_sample = time_shift(transformed_sample, shift=np.random.randint(-5, 5))\n",
    "\n",
    "        # Apply temporal noise\n",
    "        transformed_sample = add_temporal_noise(transformed_sample, noise_level=np.random.uniform(0.01, 0.1))\n",
    "\n",
    "        transformed_synthetic_data.append(torch.tensor(transformed_sample))\n",
    "\n",
    "    transformed_synthetic_data = torch.stack(transformed_synthetic_data)\n",
    "\n",
    "    # Generating labels for synthetic data\n",
    "    synthetic_labels = labels[:num_samples]\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, transformed_synthetic_data], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    augmented_labels_list = augmented_labels.tolist()\n",
    "    print('number of ones is:', augmented_labels_list.count(1))\n",
    "    print('number of zeros is:', augmented_labels_list.count(0))\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "    # DataLoader for training\n",
    "    augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inspect the augmented dataset\n",
    "    print(augmented_data.shape)  # Should be (2*num_samples, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (2*num_samples,)\n",
    "\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgtk0c2sS3yk"
   },
   "source": [
    "using VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4J9lNoOH2Bm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the VAE components\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2_mu = nn.Linear(hidden_size, latent_size)\n",
    "        self.fc2_logvar = nn.Linear(hidden_size, latent_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        mu = self.fc2_mu(x)\n",
    "        logvar = self.fc2_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.relu(self.fc1(z))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size, latent_size)\n",
    "        self.decoder = Decoder(latent_size, hidden_size, input_size)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, mu, logvar\n",
    "\n",
    "# Updated VAE loss function\n",
    "def vae_loss(x, x_reconstructed, mu, logvar):\n",
    "    MSE = nn.functional.mse_loss(x_reconstructed, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + KLD\n",
    "\n",
    "# Function to train VAE\n",
    "def train_vae(x_data):\n",
    "    input_size = 1935\n",
    "    hidden_size = 400\n",
    "    latent_size = 20\n",
    "    batch_size = 64\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    vae = VAE(input_size, hidden_size, latent_size)\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "    data_loader = DataLoader(x_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in data_loader:\n",
    "            real_data = batch.float()\n",
    "            reconstructed_data, mu, logvar = vae(real_data)\n",
    "            loss = vae_loss(real_data, reconstructed_data, mu, logvar)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #if (epoch+1) % 10 == 0:\n",
    "            #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    return vae\n",
    "\n",
    "# Function to generate new data\n",
    "def generate_data(vae, num_samples, latent_size):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_size)\n",
    "        generated_data = vae.decoder(z).numpy()\n",
    "    return generated_data\n",
    "\n",
    "def aug_with_vae(sub_flist_samples):\n",
    "    input_size = 1935\n",
    "    hidden_size = 400\n",
    "    latent_size = 20\n",
    "    batch_size = 64\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = [all_corr[f][0].tolist() for f in sub_flist_samples]\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(list_of_labels, dtype=torch.float32)\n",
    "\n",
    "    list_of_data_zero = [all_corr[f][0].tolist() for f in sub_flist_samples if all_corr[f][1] == 0]\n",
    "    print('list_of_data_zero is:' , len(list_of_data_zero))\n",
    "    list_of_data_one = [all_corr[f][0].tolist() for f in sub_flist_samples if all_corr[f][1] == 1]\n",
    "    print('list_of_data_one is:' , len(list_of_data_one))\n",
    "\n",
    "    x_class0_tensor = torch.tensor(list_of_data_zero, dtype=torch.float32)\n",
    "    x_class1_tensor = torch.tensor(list_of_data_one, dtype=torch.float32)\n",
    "\n",
    "    # Train VAEs for each class\n",
    "    vae_class0 = train_vae(x_class0_tensor)\n",
    "    vae_class1 = train_vae(x_class1_tensor)\n",
    "\n",
    "    # Generate new samples\n",
    "    num_new_samples = len(sub_flist_samples)  # Define the number of new samples you want to generate\n",
    "    augmented_data_class0 = generate_data(vae_class0, len(list_of_data_zero), latent_size)\n",
    "    augmented_data_class1 = generate_data(vae_class1, len(list_of_data_one), latent_size)\n",
    "\n",
    "    # Create labels for the new samples\n",
    "    augmented_labels_class0 = np.zeros(len(list_of_data_zero))\n",
    "    augmented_labels_class1 = np.ones(len(list_of_data_one))\n",
    "\n",
    "    # Combine original and augmented data\n",
    "    augmented_data = np.vstack((augmented_data_class0, augmented_data_class1))\n",
    "    augmented_labels = np.hstack((augmented_labels_class0, augmented_labels_class1))\n",
    "\n",
    "    # Convert numpy arrays back to tensors\n",
    "    augmented_data_tensor = torch.tensor(augmented_data, dtype=torch.float32)\n",
    "    augmented_labels_tensor = torch.tensor(augmented_labels, dtype=torch.float32)\n",
    "\n",
    "    # Combine with original data\n",
    "    whole_data = torch.cat([data, augmented_data_tensor], dim=0)\n",
    "    whole_labels = torch.cat([labels, augmented_labels_tensor], dim=0)\n",
    "\n",
    "    return whole_data, whole_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEtwId8oE1Dt"
   },
   "source": [
    "VAE two layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oE736LlQE45S"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the VAE components\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3_mu = nn.Linear(hidden_size2, latent_size)\n",
    "        self.fc3_logvar = nn.Linear(hidden_size2, latent_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        mu = self.fc3_mu(x)\n",
    "        logvar = self.fc3_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size2, hidden_size1, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_size, hidden_size2)\n",
    "        self.fc2 = nn.Linear(hidden_size2, hidden_size1)\n",
    "        self.fc3 = nn.Linear(hidden_size1, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.relu(self.fc1(z))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size1, hidden_size2, latent_size)\n",
    "        self.decoder = Decoder(latent_size, hidden_size2, hidden_size1, input_size)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, mu, logvar\n",
    "\n",
    "# Updated VAE loss function\n",
    "def vae_loss(x, x_reconstructed, mu, logvar):\n",
    "    MSE = nn.functional.mse_loss(x_reconstructed, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + KLD\n",
    "\n",
    "# Function to train VAE\n",
    "def train_vae(x_data, input_size, hidden_size1, hidden_size2, latent_size):\n",
    "    batch_size = 64\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    vae = VAE(input_size, hidden_size1, hidden_size2, latent_size)\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "    data_loader = DataLoader(x_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in data_loader:\n",
    "            real_data = batch.float()\n",
    "            reconstructed_data, mu, logvar = vae(real_data)\n",
    "            loss = vae_loss(real_data, reconstructed_data, mu, logvar)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #if (epoch+1) % 10 == 0:\n",
    "         #   print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    return vae\n",
    "\n",
    "# Function to generate new data\n",
    "def generate_data(vae, num_samples, latent_size):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_size)\n",
    "        generated_data = vae.decoder(z).numpy()\n",
    "    return generated_data\n",
    "\n",
    "def aug_with_vae(sub_flist_samples):\n",
    "    input_size = 1935\n",
    "    hidden_size1 = 400\n",
    "    hidden_size2 = 200\n",
    "    latent_size = 20\n",
    "    batch_size = 64\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = [all_corr[f][0].tolist() for f in sub_flist_samples]\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(list_of_labels, dtype=torch.float32)\n",
    "\n",
    "    list_of_data_zero = [all_corr[f][0].tolist() for f in sub_flist_samples if all_corr[f][1] == 0]\n",
    "    print('list_of_data_zero is:', len(list_of_data_zero))\n",
    "    list_of_data_one = [all_corr[f][0].tolist() for f in sub_flist_samples if all_corr[f][1] == 1]\n",
    "    print('list_of_data_one is:', len(list_of_data_one))\n",
    "\n",
    "    x_class0_tensor = torch.tensor(list_of_data_zero, dtype=torch.float32)\n",
    "    x_class1_tensor = torch.tensor(list_of_data_one, dtype=torch.float32)\n",
    "\n",
    "    # Train VAEs for each class\n",
    "    vae_class0 = train_vae(x_class0_tensor, input_size, hidden_size1, hidden_size2, latent_size)\n",
    "    vae_class1 = train_vae(x_class1_tensor, input_size, hidden_size1, hidden_size2, latent_size)\n",
    "\n",
    "    # Generate new samples\n",
    "    num_new_samples = len(sub_flist_samples)  # Define the number of new samples you want to generate\n",
    "    augmented_data_class0 = generate_data(vae_class0, len(list_of_data_zero), latent_size)\n",
    "    augmented_data_class1 = generate_data(vae_class1, len(list_of_data_one), latent_size)\n",
    "\n",
    "    # Create labels for the new samples\n",
    "    augmented_labels_class0 = np.zeros(len(list_of_data_zero))\n",
    "    augmented_labels_class1 = np.ones(len(list_of_data_one))\n",
    "\n",
    "    # Combine original and augmented data\n",
    "    augmented_data = np.vstack((augmented_data_class0, augmented_data_class1))\n",
    "    augmented_labels = np.hstack((augmented_labels_class0, augmented_labels_class1))\n",
    "\n",
    "    # Convert numpy arrays back to tensors\n",
    "    augmented_data_tensor = torch.tensor(augmented_data, dtype=torch.float32)\n",
    "    augmented_labels_tensor = torch.tensor(augmented_labels, dtype=torch.float32)\n",
    "\n",
    "    # Combine with original data\n",
    "    whole_data = torch.cat([data, augmented_data_tensor], dim=0)\n",
    "    whole_labels = torch.cat([labels, augmented_labels_tensor], dim=0)\n",
    "\n",
    "    return whole_data, whole_labels\n",
    "\n",
    "# Example usage:\n",
    "# sub_flist_samples = list of samples\n",
    "# all_corr = dictionary containing your data\n",
    "# train_data, train_labels = aug_with_vae(sub_flist_samples, all_corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjwY26O5VvUL"
   },
   "source": [
    "using VAE with 5 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AMcLIdfVyra"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the VAE components\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        self.fc_layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            self.fc_layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "        self.fc_mu = nn.Linear(hidden_sizes[-1], latent_size)\n",
    "        self.fc_logvar = nn.Linear(hidden_sizes[-1], latent_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.fc_layers:\n",
    "            x = self.relu(layer(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size, hidden_sizes, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        self.fc_layers.append(nn.Linear(latent_size, hidden_sizes[-1]))\n",
    "        for i in range(len(hidden_sizes)-1, 0, -1):\n",
    "            self.fc_layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i-1]))\n",
    "        self.fc_layers.append(nn.Linear(hidden_sizes[0], output_size))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, z):\n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            if i < len(self.fc_layers) - 1:\n",
    "                z = self.relu(layer(z))\n",
    "            else:\n",
    "                z = layer(z)  # no activation on the output layer\n",
    "        return z\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_sizes, latent_size)\n",
    "        self.decoder = Decoder(latent_size, hidden_sizes, input_size)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, mu, logvar\n",
    "\n",
    "# Updated VAE loss function\n",
    "def vae_loss(x, x_reconstructed, mu, logvar):\n",
    "    MSE = nn.functional.mse_loss(x_reconstructed, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + KLD\n",
    "\n",
    "# Function to train VAE\n",
    "def train_vae(x_data, input_size, hidden_sizes, latent_size):\n",
    "    batch_size = 64\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    vae = VAE(input_size, hidden_sizes, latent_size)\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "    data_loader = DataLoader(x_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in data_loader:\n",
    "            real_data = batch.float()\n",
    "            reconstructed_data, mu, logvar = vae(real_data)\n",
    "            loss = vae_loss(real_data, reconstructed_data, mu, logvar)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #if (epoch+1) % 10 == 0:\n",
    "         #   print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    return vae\n",
    "\n",
    "# Function to generate new data\n",
    "def generate_data(vae, num_samples, latent_size):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_size)\n",
    "        generated_data = vae.decoder(z).numpy()\n",
    "    return generated_data\n",
    "\n",
    "def aug_with_vae_5_layer(sub_flist_samples):\n",
    "    input_size = 1935\n",
    "    hidden_sizes = [800, 600, 400, 200, 100]  # Example sizes for the 5 layers\n",
    "    latent_size = 20\n",
    "    batch_size = 64\n",
    "    num_epochs = 100\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = [all_corr[f][0].tolist() for f in sub_flist_samples]\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(list_of_labels, dtype=torch.float32)\n",
    "\n",
    "    list_of_data_zero = [all_corr[f][0].tolist() for f in sub_flist_samples if all_corr[f][1] == 0]\n",
    "    print('list_of_data_zero is:', len(list_of_data_zero))\n",
    "    list_of_data_one = [all_corr[f][0].tolist() for f in sub_flist_samples if all_corr[f][1] == 1]\n",
    "    print('list_of_data_one is:', len(list_of_data_one))\n",
    "\n",
    "    x_class0_tensor = torch.tensor(list_of_data_zero, dtype=torch.float32)\n",
    "    x_class1_tensor = torch.tensor(list_of_data_one, dtype=torch.float32)\n",
    "\n",
    "    # Train VAEs for each class\n",
    "    vae_class0 = train_vae(x_class0_tensor, input_size, hidden_sizes, latent_size)\n",
    "    vae_class1 = train_vae(x_class1_tensor, input_size, hidden_sizes, latent_size)\n",
    "\n",
    "    # Generate new samples\n",
    "    num_new_samples = len(sub_flist_samples)  # Define the number of new samples you want to generate\n",
    "    augmented_data_class0 = generate_data(vae_class0, len(list_of_data_zero), latent_size)\n",
    "    augmented_data_class1 = generate_data(vae_class1, len(list_of_data_one), latent_size)\n",
    "\n",
    "    # Create labels for the new samples\n",
    "    augmented_labels_class0 = np.zeros(len(list_of_data_zero))\n",
    "    augmented_labels_class1 = np.ones(len(list_of_data_one))\n",
    "\n",
    "    # Combine original and augmented data\n",
    "    augmented_data = np.vstack((augmented_data_class0, augmented_data_class1))\n",
    "    augmented_labels = np.hstack((augmented_labels_class0, augmented_labels_class1))\n",
    "\n",
    "    # Convert numpy arrays back to tensors\n",
    "    augmented_data_tensor = torch.tensor(augmented_data, dtype=torch.float32)\n",
    "    augmented_labels_tensor = torch.tensor(augmented_labels, dtype=torch.float32)\n",
    "\n",
    "    # Combine with original data\n",
    "    whole_data = torch.cat([data, augmented_data_tensor], dim=0)\n",
    "    whole_labels = torch.cat([labels, augmented_labels_tensor], dim=0)\n",
    "\n",
    "    return whole_data, whole_labels\n",
    "\n",
    "# Example usage:\n",
    "# sub_flist_samples = list of samples\n",
    "# all_corr = dictionary containing your data\n",
    "# train_data, train_labels = aug_with_vae(sub_flist_samples, all_corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6eFD4FG3J9N"
   },
   "source": [
    "use of the alpha_GAN to augment data weights initiaization and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6XDelnsNBoJ"
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8t8iHBprrXeY"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        #self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(1)\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        #self.apply(weights_init)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        #self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n",
    "\n",
    "class CodeDiscriminator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim):\n",
    "        super(CodeDiscriminator, self).__init__()\n",
    "        self.code_discriminator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        #self.apply(weights_init)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.code_discriminator(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8f8WviqK_2D"
   },
   "outputs": [],
   "source": [
    "def generate_data(num_samples, decoder, latent_dim):\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_codes = torch.randn(num_samples, latent_dim)\n",
    "        generated_data = decoder(latent_codes)\n",
    "    return generated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bn0tFchNrpln"
   },
   "outputs": [],
   "source": [
    "def augment_data_alpha_gan(sub_flist_samples):\n",
    "\n",
    "  # Hyperparameters\n",
    "  num_samples = len(sub_flist_samples)\n",
    "  input_dim = 1935\n",
    "  hidden_dim = 512\n",
    "  latent_dim = 128\n",
    "  dis_hidden_dim = 256\n",
    "  num_epochs = 100\n",
    "  batch_size = 64\n",
    "  learning_rate = 0.0002\n",
    "  step_size = 2  # Number of epochs after which to decrease the learning rate\n",
    "  gamma = 0.5  # Multiplicative factor of learning rate decay\n",
    "  #print(learning_rate)\n",
    "\n",
    "  # Instantiate models\n",
    "  encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "  #print(encoder)\n",
    "  decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "  discriminator = Discriminator(input_dim, dis_hidden_dim)\n",
    "  code_discriminator = CodeDiscriminator(latent_dim, dis_hidden_dim)\n",
    "\n",
    "  # Loss functions\n",
    "  adversarial_loss = nn.BCELoss()\n",
    "  reconstruction_loss = nn.MSELoss()\n",
    "  code_loss = nn.BCELoss()\n",
    "\n",
    "  # Optimizers\n",
    "  optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "  optimizer_D = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "  optimizer_Dis = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "  optimizer_CodeDis = optim.Adam(code_discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "  #scheduler_E = optim.lr_scheduler.StepLR(optimizer_E, step_size=step_size, gamma=gamma)\n",
    "  #scheduler_D = optim.lr_scheduler.StepLR(optimizer_D, step_size=step_size, gamma=gamma)\n",
    "  #scheduler_Dis = optim.lr_scheduler.StepLR(optimizer_Dis, step_size=step_size, gamma=gamma)\n",
    "  #scheduler_CodeDis = optim.lr_scheduler.StepLR(optimizer_CodeDis, step_size=step_size, gamma=gamma)\n",
    "\n",
    "  # Original data and labels\n",
    "  flist1 = [f for f in sub_flist_samples]\n",
    "  array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "  list_of_labels = array_of_labels.tolist()\n",
    "  list_of_data = []\n",
    "  for f in sub_flist_samples:\n",
    "    list_of_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "  data = torch.tensor(list_of_data)\n",
    "  labels = torch.tensor(list_of_labels)\n",
    "  #print(data.shape)\n",
    "  #print(labels.shape)\n",
    "\n",
    "\n",
    "  # DataLoader\n",
    "  dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    for real_data, _ in dataloader:\n",
    "        batch_size = real_data.size(0)\n",
    "\n",
    "        # Prepare real and fake labels\n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "        # ==========================\n",
    "        # Train the Discriminator\n",
    "        # ==========================\n",
    "        latent_code = encoder(real_data)\n",
    "        #print('latent code is' , 3)\n",
    "        reconstructed_data = decoder(latent_code)\n",
    "\n",
    "        outputs_real = discriminator(real_data)\n",
    "        outputs_fake = discriminator(reconstructed_data.detach())\n",
    "\n",
    "        dis_loss_real = adversarial_loss(outputs_real, real_labels)\n",
    "        dis_loss_fake = adversarial_loss(outputs_fake, fake_labels)\n",
    "\n",
    "        dis_loss = dis_loss_real + dis_loss_fake\n",
    "        optimizer_Dis.zero_grad()\n",
    "        dis_loss.backward(retain_graph=True)\n",
    "        optimizer_Dis.step()\n",
    "\n",
    "        # ==========================\n",
    "        # Train the Code Discriminator\n",
    "        # ==========================\n",
    "        outputs_code_real = code_discriminator(latent_code)\n",
    "        outputs_code_fake = code_discriminator(torch.randn(batch_size, latent_dim))\n",
    "\n",
    "        code_loss_real = code_loss(outputs_code_real, real_labels)\n",
    "        code_loss_fake = code_loss(outputs_code_fake, fake_labels)\n",
    "\n",
    "        code_dis_loss = code_loss_real + code_loss_fake\n",
    "        optimizer_CodeDis.zero_grad()\n",
    "        code_dis_loss.backward(retain_graph=True)\n",
    "        optimizer_CodeDis.step()\n",
    "\n",
    "        # ==========================\n",
    "        # Train the Encoder and Decoder (Generator)\n",
    "        # ==========================\n",
    "        outputs_fake = discriminator(reconstructed_data)\n",
    "        gen_loss = adversarial_loss(outputs_fake, real_labels)\n",
    "\n",
    "        rec_loss = reconstruction_loss(reconstructed_data, real_data)\n",
    "        total_gen_loss = gen_loss + rec_loss\n",
    "        optimizer_E.zero_grad()\n",
    "        optimizer_D.zero_grad()\n",
    "        total_gen_loss.backward(retain_graph=True)\n",
    "        optimizer_E.step()\n",
    "        optimizer_D.step()\n",
    "\n",
    "    #scheduler\n",
    "    #scheduler_E.step()\n",
    "    #scheduler_D.step()\n",
    "    #scheduler_Dis.step()\n",
    "    #scheduler_CodeDis.step()\n",
    "\n",
    "    #print(f'Epoch [{epoch}/{num_epochs}], Loss: {total_gen_loss.item()}')\n",
    "\n",
    "  # Generate additional samples\n",
    "  synthetic_data = generate_data(num_samples, decoder, latent_dim)\n",
    "\n",
    "  # Generating labels for synthetic data\n",
    "  # Here we are simply replicating the original labels\n",
    "  synthetic_labels = labels[:num_samples]\n",
    "\n",
    "  # Combine original and synthetic data\n",
    "  augmented_data = torch.cat([data, synthetic_data], dim=0)\n",
    "  augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "  # Create a TensorDataset\n",
    "  augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "  # DataLoader for training\n",
    "  augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  # Inspect the augmented dataset\n",
    "  print(augmented_data.shape)  # Should be (1568, 1935)\n",
    "  print(augmented_labels.shape)  # Should be (1568,)\n",
    "\n",
    "  return augmented_data , augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jE5pCcdoCwY"
   },
   "source": [
    "augment data alpha gan 2 layer with generating trainset * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJG7GQxBoKd-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "def augment_data_alpha_gan_2_layer_5_times(sub_flist_samples):\n",
    "    class Encoder(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "            super(Encoder, self).__init__()\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim1, hidden_dim2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim2, latent_dim)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.encoder(x)\n",
    "\n",
    "    class Decoder(nn.Module):\n",
    "        def __init__(self, latent_dim, hidden_dim2, hidden_dim1, output_dim):\n",
    "            super(Decoder, self).__init__()\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim2, hidden_dim1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim1, output_dim)\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            return self.decoder(z)\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim1, hidden_dim2):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.discriminator = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim1, hidden_dim2),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim2, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.discriminator(x)\n",
    "\n",
    "    class CodeDiscriminator(nn.Module):\n",
    "        def __init__(self, latent_dim, hidden_dim1, hidden_dim2):\n",
    "            super(CodeDiscriminator, self).__init__()\n",
    "            self.code_discriminator = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim1, hidden_dim2),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim2, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            return self.code_discriminator(z)\n",
    "\n",
    "    def generate_data(num_samples, decoder, latent_dim):\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            latent_codes = torch.randn(num_samples, latent_dim)\n",
    "            generated_data = decoder(latent_codes)\n",
    "        return generated_data\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_samples = len(sub_flist_samples) * 5\n",
    "    input_dim = 1935\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    latent_dim = 128\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "\n",
    "    # Instantiate models\n",
    "    encoder = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "    decoder = Decoder(latent_dim, hidden_dim2, hidden_dim1, input_dim)\n",
    "    discriminator = Discriminator(input_dim, hidden_dim1, hidden_dim2)\n",
    "    code_discriminator = CodeDiscriminator(latent_dim, hidden_dim1, hidden_dim2)\n",
    "\n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "    code_loss = nn.BCELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    optimizer_D = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    optimizer_Dis = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "    optimizer_CodeDis = optim.Adam(code_discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = []\n",
    "    for f in sub_flist_samples:\n",
    "        list_of_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(list_of_labels, dtype=torch.float32)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data, _ in dataloader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Prepare real and fake labels\n",
    "            real_labels = torch.ones(batch_size, 1)\n",
    "            fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "            # Train the Discriminator\n",
    "            latent_code = encoder(real_data)\n",
    "            reconstructed_data = decoder(latent_code)\n",
    "\n",
    "            outputs_real = discriminator(real_data)\n",
    "            outputs_fake = discriminator(reconstructed_data.detach())\n",
    "\n",
    "            dis_loss_real = adversarial_loss(outputs_real, real_labels)\n",
    "            dis_loss_fake = adversarial_loss(outputs_fake, fake_labels)\n",
    "            dis_loss = dis_loss_real + dis_loss_fake\n",
    "\n",
    "            optimizer_Dis.zero_grad()\n",
    "            dis_loss.backward(retain_graph=True)\n",
    "            optimizer_Dis.step()\n",
    "\n",
    "            # Train the Code Discriminator\n",
    "            outputs_code_real = code_discriminator(latent_code)\n",
    "            outputs_code_fake = code_discriminator(torch.randn(batch_size, latent_dim))\n",
    "\n",
    "            code_loss_real = code_loss(outputs_code_real, real_labels)\n",
    "            code_loss_fake = code_loss(outputs_code_fake, fake_labels)\n",
    "            code_dis_loss = code_loss_real + code_loss_fake\n",
    "\n",
    "            optimizer_CodeDis.zero_grad()\n",
    "            code_dis_loss.backward(retain_graph=True)\n",
    "            optimizer_CodeDis.step()\n",
    "\n",
    "            # Train the Encoder and Decoder (Generator)\n",
    "            outputs_fake = discriminator(reconstructed_data)\n",
    "            gen_loss = adversarial_loss(outputs_fake, real_labels)\n",
    "            rec_loss = reconstruction_loss(reconstructed_data, real_data)\n",
    "            total_gen_loss = gen_loss + rec_loss\n",
    "\n",
    "            optimizer_E.zero_grad()\n",
    "            optimizer_D.zero_grad()\n",
    "            total_gen_loss.backward(retain_graph=True)\n",
    "            optimizer_E.step()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_gen_loss.item()}')\n",
    "\n",
    "    # Generate additional samples\n",
    "    synthetic_data = generate_data(num_samples, decoder, latent_dim)\n",
    "\n",
    "    # Generating labels for synthetic data\n",
    "    synthetic_labels = labels[:num_samples]\n",
    "\n",
    "    # If necessary, expand synthetic labels to match synthetic data size\n",
    "    synthetic_labels = synthetic_labels.repeat(5)\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    augmented_labels_list = augmented_labels.tolist()\n",
    "    print('Number of ones is:', augmented_labels_list.count(1))\n",
    "    print('Number of zeros is:', augmented_labels_list.count(0))\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "    # DataLoader for training\n",
    "    augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inspect the augmented dataset\n",
    "    print(augmented_data.shape)\n",
    "    print(augmented_labels.shape)\n",
    "\n",
    "    return augmented_data, augmented_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiVHtVdAl244"
   },
   "source": [
    "using alph_gan 2 layer and additional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ss8Ghs4l8UL"
   },
   "outputs": [],
   "source": [
    "def augment_data_alpha_gan_2_layer_additional(sub_flist_samples):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    import numpy as np\n",
    "\n",
    "    class Encoder(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim):\n",
    "            super(Encoder, self).__init__()\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim1, hidden_dim2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim2, hidden_dim3),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim3, latent_dim)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.encoder(x)\n",
    "\n",
    "    class Decoder(nn.Module):\n",
    "        def __init__(self, latent_dim, hidden_dim3, hidden_dim2, hidden_dim1, output_dim):\n",
    "            super(Decoder, self).__init__()\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim3),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim3, hidden_dim2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim2, hidden_dim1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim1, output_dim)\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            return self.decoder(z)\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.discriminator = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim1, hidden_dim2),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim2, hidden_dim3),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim3, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.discriminator(x)\n",
    "\n",
    "    class CodeDiscriminator(nn.Module):\n",
    "        def __init__(self, latent_dim, hidden_dim1, hidden_dim2, hidden_dim3):\n",
    "            super(CodeDiscriminator, self).__init__()\n",
    "            self.code_discriminator = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim1, hidden_dim2),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim2, hidden_dim3),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim3, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            return self.code_discriminator(z)\n",
    "\n",
    "    def generate_data(num_samples, decoder, latent_dim):\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            latent_codes = torch.randn(num_samples, latent_dim)\n",
    "            generated_data = decoder(latent_codes)\n",
    "        return generated_data\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    input_dim = 1935\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    hidden_dim3 = 128\n",
    "    latent_dim = 64\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "\n",
    "    # Instantiate models\n",
    "    encoder = Encoder(input_dim, hidden_dim1, hidden_dim2, hidden_dim3, latent_dim)\n",
    "    decoder = Decoder(latent_dim, hidden_dim3, hidden_dim2, hidden_dim1, input_dim)\n",
    "    discriminator = Discriminator(input_dim, hidden_dim1, hidden_dim2, hidden_dim3)\n",
    "    code_discriminator = CodeDiscriminator(latent_dim, hidden_dim1, hidden_dim2, hidden_dim3)\n",
    "\n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "    code_loss = nn.BCELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    optimizer_D = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    optimizer_Dis = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "    optimizer_CodeDis = optim.Adam(code_discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = []\n",
    "    for f in sub_flist_samples:\n",
    "        list_of_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "    data = torch.tensor(list_of_data)\n",
    "    labels = torch.tensor(list_of_labels)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data, _ in dataloader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Prepare real and fake labels\n",
    "            real_labels = torch.ones(batch_size, 1)\n",
    "            fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "            # Train the Discriminator\n",
    "            latent_code = encoder(real_data)\n",
    "            reconstructed_data = decoder(latent_code)\n",
    "\n",
    "            outputs_real = discriminator(real_data)\n",
    "            outputs_fake = discriminator(reconstructed_data.detach())\n",
    "\n",
    "            dis_loss_real = adversarial_loss(outputs_real, real_labels)\n",
    "            dis_loss_fake = adversarial_loss(outputs_fake, fake_labels)\n",
    "            dis_loss = dis_loss_real + dis_loss_fake\n",
    "\n",
    "            optimizer_Dis.zero_grad()\n",
    "            dis_loss.backward(retain_graph=True)\n",
    "            optimizer_Dis.step()\n",
    "\n",
    "            # Train the Code Discriminator\n",
    "            outputs_code_real = code_discriminator(latent_code)\n",
    "            outputs_code_fake = code_discriminator(torch.randn(batch_size, latent_dim))\n",
    "\n",
    "            code_loss_real = code_loss(outputs_code_real, real_labels)\n",
    "            code_loss_fake = code_loss(outputs_code_fake, fake_labels)\n",
    "            code_dis_loss = code_loss_real + code_loss_fake\n",
    "\n",
    "            optimizer_CodeDis.zero_grad()\n",
    "            code_dis_loss.backward(retain_graph=True)\n",
    "            optimizer_CodeDis.step()\n",
    "\n",
    "            # Train the Encoder and Decoder (Generator)\n",
    "            outputs_fake = discriminator(reconstructed_data)\n",
    "            gen_loss = adversarial_loss(outputs_fake, real_labels)\n",
    "            rec_loss = reconstruction_loss(reconstructed_data, real_data)\n",
    "            total_gen_loss = gen_loss + rec_loss\n",
    "\n",
    "            optimizer_E.zero_grad()\n",
    "            optimizer_D.zero_grad()\n",
    "            total_gen_loss.backward(retain_graph=True)\n",
    "            optimizer_E.step()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_gen_loss.item()}')\n",
    "\n",
    "    # Generate additional samples\n",
    "    synthetic_data = generate_data(num_samples, decoder, latent_dim)\n",
    "\n",
    "    # Generating labels for synthetic data\n",
    "    synthetic_labels = labels[:num_samples]\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    augmented_labels_list = augmented_labels.tolist()\n",
    "    print('number of ones is:', augmented_labels_list.count(1))\n",
    "    print('number of zeros is:', augmented_labels_list.count(0))\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "    # DataLoader for training\n",
    "    augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inspect the augmented dataset\n",
    "    print(augmented_data.shape)  # Should be (2*num_samples, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (2*num_samples,)\n",
    "\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUFfqTVfOrp8"
   },
   "source": [
    "using alpha gan 2 layer with leakyrelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atQz7uBqOvez"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class Encoder1(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super(Encoder1, self).__init__()\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.BatchNorm1d(hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.BatchNorm1d(hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder1(x)\n",
    "\n",
    "class Decoder1(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim2, hidden_dim1, output_dim):\n",
    "        super(Decoder1, self).__init__()\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim2),\n",
    "            nn.BatchNorm1d(hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.BatchNorm1d(hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder1(z)\n",
    "\n",
    "class Discriminator1(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2):\n",
    "        super(Discriminator1, self).__init__()\n",
    "        self.discriminator1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.BatchNorm1d(hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.BatchNorm1d(hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discriminator1(x)\n",
    "\n",
    "class CodeDiscriminator1(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim1, hidden_dim2):\n",
    "        super(CodeDiscriminator1, self).__init__()\n",
    "        self.code_discriminator1 = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim1),\n",
    "            nn.BatchNorm1d(hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.BatchNorm1d(hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.code_discriminator1(z)\n",
    "\n",
    "def generate_data(num_samples, decoder, latent_dim):\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_codes = torch.randn(num_samples, latent_dim)\n",
    "        generated_data = decoder(latent_codes)\n",
    "    return generated_data\n",
    "\n",
    "def augment_data_alpha_gan_2_layer_leaky(sub_flist_samples):\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    input_dim = 1935\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    latent_dim = 128\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "\n",
    "    # Instantiate models\n",
    "    encoder = Encoder1(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "    decoder = Decoder1(latent_dim, hidden_dim2, hidden_dim1, input_dim)\n",
    "    discriminator = Discriminator1(input_dim, hidden_dim1, hidden_dim2)\n",
    "    code_discriminator = CodeDiscriminator1(latent_dim, hidden_dim1, hidden_dim2)\n",
    "\n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "    code_loss = nn.BCELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    optimizer_D = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    optimizer_Dis = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "    optimizer_CodeDis = optim.Adam(code_discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = []\n",
    "    for f in sub_flist_samples:\n",
    "      list_of_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(list_of_labels, dtype=torch.float32)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data, _ in dataloader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Prepare real and fake labels\n",
    "            real_labels = torch.ones(batch_size, 1)\n",
    "            fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "            # Train the Discriminator\n",
    "            latent_code = encoder(real_data)\n",
    "            reconstructed_data = decoder(latent_code)\n",
    "\n",
    "            outputs_real = discriminator(real_data)\n",
    "            outputs_fake = discriminator(reconstructed_data.detach())\n",
    "\n",
    "            dis_loss_real = adversarial_loss(outputs_real, real_labels)\n",
    "            dis_loss_fake = adversarial_loss(outputs_fake, fake_labels)\n",
    "            dis_loss = dis_loss_real + dis_loss_fake\n",
    "\n",
    "            optimizer_Dis.zero_grad()\n",
    "            dis_loss.backward(retain_graph=True)\n",
    "            optimizer_Dis.step()\n",
    "\n",
    "            # Train the Code Discriminator\n",
    "            outputs_code_real = code_discriminator(latent_code)\n",
    "            outputs_code_fake = code_discriminator(torch.randn(batch_size, latent_dim))\n",
    "\n",
    "            code_loss_real = code_loss(outputs_code_real, real_labels)\n",
    "            code_loss_fake = code_loss(outputs_code_fake, fake_labels)\n",
    "            code_dis_loss = code_loss_real + code_loss_fake\n",
    "\n",
    "            optimizer_CodeDis.zero_grad()\n",
    "            code_dis_loss.backward(retain_graph=True)\n",
    "            optimizer_CodeDis.step()\n",
    "\n",
    "            # Train the Encoder and Decoder (Generator)\n",
    "            outputs_fake = discriminator(reconstructed_data)\n",
    "            gen_loss = adversarial_loss(outputs_fake, real_labels)\n",
    "            rec_loss = reconstruction_loss(reconstructed_data, real_data)\n",
    "            total_gen_loss = gen_loss + rec_loss\n",
    "\n",
    "            optimizer_E.zero_grad()\n",
    "            optimizer_D.zero_grad()\n",
    "            total_gen_loss.backward(retain_graph=True)\n",
    "            optimizer_E.step()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_gen_loss.item()}')\n",
    "\n",
    "    # Generate additional samples\n",
    "    synthetic_data = generate_data(num_samples, decoder, latent_dim)\n",
    "\n",
    "    # Generating labels for synthetic data\n",
    "    synthetic_labels = labels[:num_samples]\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    augmented_labels_list = augmented_labels.tolist()\n",
    "    print('number of ones is:', augmented_labels_list.count(1))\n",
    "    print('number of zeros is:', augmented_labels_list.count(0))\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "    # DataLoader for training\n",
    "    augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inspect the augmented dataset\n",
    "    print(augmented_data.shape)  # Should be (2*num_samples, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (2*num_samples,)\n",
    "\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32KWZtTE6G9P"
   },
   "source": [
    "using alpha gan 2 layer with Conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCNOLXwu6N7C"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "def augment_data_alpha_gan_2_layer_conv(sub_flist_samples):\n",
    "\n",
    "    class Encoder(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "            super(Encoder, self).__init__()\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv1d(input_dim, hidden_dim1, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(hidden_dim1, hidden_dim2, kernel_size=3 , padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(hidden_dim2, latent_dim, kernel_size=3 , padding=1)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.unsqueeze(2)  # Add a channel dimension\n",
    "            x = self.encoder(x)\n",
    "            return x.squeeze(2)\n",
    "\n",
    "    class Decoder(nn.Module):\n",
    "        def __init__(self, latent_dim, hidden_dim2, hidden_dim1, output_dim):\n",
    "            super(Decoder, self).__init__()\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Conv1d(latent_dim, hidden_dim2, kernel_size=3 , padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(hidden_dim2, hidden_dim1, kernel_size=3 , padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(hidden_dim1, output_dim, kernel_size=3 , padding=1)\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            z = z.unsqueeze(2)  # Add a channel dimension\n",
    "            z = self.decoder(z)\n",
    "            return z.squeeze(2)\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim1, hidden_dim2):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.discriminator = nn.Sequential(\n",
    "                nn.Conv1d(input_dim, hidden_dim1, kernel_size=3 , padding=1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Conv1d(hidden_dim1, hidden_dim2, kernel_size=3 , padding=1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Conv1d(hidden_dim2, 1, kernel_size=3 , padding=1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.unsqueeze(2)  # Add a channel dimension\n",
    "            x = self.discriminator(x)\n",
    "            return x.squeeze(2)\n",
    "\n",
    "    class CodeDiscriminator(nn.Module):\n",
    "        def __init__(self, latent_dim, hidden_dim1, hidden_dim2):\n",
    "            super(CodeDiscriminator, self).__init__()\n",
    "            self.code_discriminator = nn.Sequential(\n",
    "                nn.Conv1d(latent_dim, hidden_dim1, kernel_size=3 , padding=1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Conv1d(hidden_dim1, hidden_dim2, kernel_size=3 , padding=1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Conv1d(hidden_dim2, 1, kernel_size=3 , padding=1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            z = z.unsqueeze(2)  # Add a channel dimension\n",
    "            z = self.code_discriminator(z)\n",
    "            return z.squeeze(2)\n",
    "\n",
    "    def generate_data(num_samples, decoder, latent_dim):\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            latent_codes = torch.randn(num_samples, latent_dim)\n",
    "            generated_data = decoder(latent_codes)\n",
    "        return generated_data\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    input_dim = 1935\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    latent_dim = 128\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "\n",
    "    # Instantiate models\n",
    "    encoder = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "    decoder = Decoder(latent_dim, hidden_dim2, hidden_dim1, input_dim)\n",
    "    discriminator = Discriminator(input_dim, hidden_dim1, hidden_dim2)\n",
    "    code_discriminator = CodeDiscriminator(latent_dim, hidden_dim1, hidden_dim2)\n",
    "\n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "    code_loss = nn.BCELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    optimizer_D = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    optimizer_Dis = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "    optimizer_CodeDis = optim.Adam(code_discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = []\n",
    "    for f in sub_flist_samples:\n",
    "        list_of_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(list_of_labels, dtype=torch.float32)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data, _ in dataloader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Prepare real and fake labels\n",
    "            real_labels = torch.ones(batch_size, 1)\n",
    "            fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "            # Train the Discriminator\n",
    "            latent_code = encoder(real_data)\n",
    "            reconstructed_data = decoder(latent_code)\n",
    "\n",
    "            outputs_real = discriminator(real_data)\n",
    "            outputs_fake = discriminator(reconstructed_data.detach())\n",
    "\n",
    "            dis_loss_real = adversarial_loss(outputs_real, real_labels)\n",
    "            dis_loss_fake = adversarial_loss(outputs_fake, fake_labels)\n",
    "            dis_loss = dis_loss_real + dis_loss_fake\n",
    "\n",
    "            optimizer_Dis.zero_grad()\n",
    "            dis_loss.backward(retain_graph=True)\n",
    "            optimizer_Dis.step()\n",
    "\n",
    "            # Train the Code Discriminator\n",
    "            outputs_code_real = code_discriminator(latent_code)\n",
    "            outputs_code_fake = code_discriminator(torch.randn(batch_size, latent_dim))\n",
    "\n",
    "            code_loss_real = code_loss(outputs_code_real, real_labels)\n",
    "            code_loss_fake = code_loss(outputs_code_fake, fake_labels)\n",
    "            code_dis_loss = code_loss_real + code_loss_fake\n",
    "\n",
    "            optimizer_CodeDis.zero_grad()\n",
    "            code_dis_loss.backward(retain_graph=True)\n",
    "            optimizer_CodeDis.step()\n",
    "\n",
    "            # Train the Encoder and Decoder (Generator)\n",
    "            outputs_fake = discriminator(reconstructed_data)\n",
    "            gen_loss = adversarial_loss(outputs_fake, real_labels)\n",
    "            rec_loss = reconstruction_loss(reconstructed_data, real_data)\n",
    "            total_gen_loss = gen_loss + rec_loss\n",
    "\n",
    "            optimizer_E.zero_grad()\n",
    "            optimizer_D.zero_grad()\n",
    "            total_gen_loss.backward(retain_graph=True)\n",
    "            optimizer_E.step()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_gen_loss.item()}')\n",
    "\n",
    "    # Generate additional samples\n",
    "    synthetic_data = generate_data(num_samples, decoder, latent_dim)\n",
    "\n",
    "    # Generating labels for synthetic data\n",
    "    synthetic_labels = labels[:num_samples]\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    augmented_labels_list = augmented_labels.tolist()\n",
    "    print('number of ones is:', augmented_labels_list.count(1))\n",
    "    print('number of zeros is:', augmented_labels_list.count(0))\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "    # DataLoader for training\n",
    "    augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inspect the augmented dataset\n",
    "    print(augmented_data.shape)  # Should be (2*num_samples, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (2*num_samples,)\n",
    "\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTS4lNRLM8BJ"
   },
   "source": [
    "using more complex 2 layer gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iwdcp3-nNCXg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Residual Block for deeper models\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features, in_features),\n",
    "            nn.BatchNorm1d(in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# Encoder with Residual Blocks\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim1),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim2),\n",
    "            nn.Linear(hidden_dim2, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Decoder with Residual Blocks\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim2, hidden_dim1, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim2),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim1),\n",
    "            nn.Linear(hidden_dim1, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "# More Powerful Discriminator with Residual Blocks and Increased Depth\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            ResidualBlock(hidden_dim1),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            ResidualBlock(hidden_dim2),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            ResidualBlock(hidden_dim3),\n",
    "            nn.Linear(hidden_dim3, hidden_dim4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            ResidualBlock(hidden_dim4),\n",
    "            nn.Linear(hidden_dim4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n",
    "\n",
    "# Code Discriminator with Residual Blocks\n",
    "class CodeDiscriminator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim1, hidden_dim2):\n",
    "        super(CodeDiscriminator, self).__init__()\n",
    "        self.code_discriminator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            ResidualBlock(hidden_dim1),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            ResidualBlock(hidden_dim2),\n",
    "            nn.Linear(hidden_dim2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.code_discriminator(z)\n",
    "\n",
    "# Gradient Penalty Calculation\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "    alpha = torch.rand(real_samples.size(0), 1).expand_as(real_samples)\n",
    "    interpolates = alpha * real_samples + ((1 - alpha) * fake_samples)\n",
    "    interpolates = interpolates.requires_grad_(True)\n",
    "\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "\n",
    "    fake = torch.ones(real_samples.size(0), 1).requires_grad_(False)\n",
    "\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "    return gradient_penalty\n",
    "\n",
    "# Function to generate synthetic data\n",
    "def generate_data(num_samples, decoder, latent_dim):\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        random_latent_vectors = torch.randn(num_samples, latent_dim)\n",
    "        synthetic_data = decoder(random_latent_vectors)\n",
    "    decoder.train()  # Set decoder back to training mode\n",
    "    return synthetic_data\n",
    "\n",
    "# Data Augmentation Function\n",
    "def augment_data_alpha_gan_2_layer_complex(sub_flist_samples):\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    input_dim = 1935\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    hidden_dim3 = 128  # Additional hidden layer dimension for a more powerful discriminator\n",
    "    hidden_dim4 = 64   # Another additional hidden layer dimension for a more powerful discriminator\n",
    "    latent_dim = 128\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "    lambda_gp = 10\n",
    "\n",
    "    # Instantiate models\n",
    "    encoder = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "    decoder = Decoder(latent_dim, hidden_dim2, hidden_dim1, input_dim)\n",
    "    discriminator = Discriminator(input_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4)\n",
    "    code_discriminator = CodeDiscriminator(latent_dim, hidden_dim1, hidden_dim2)\n",
    "\n",
    "    # Loss functions\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "    code_loss = nn.BCELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    optimizer_D = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    optimizer_Dis = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "    optimizer_CodeDis = optim.Adam(code_discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = [all_corr[f][0].tolist() for f in sub_flist_samples]\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(list_of_labels, dtype=torch.float32)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data, _ in dataloader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Prepare real and fake labels\n",
    "            real_labels = torch.ones(batch_size, 1)\n",
    "            fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "            # Train the Discriminator\n",
    "            latent_code = encoder(real_data)\n",
    "            reconstructed_data = decoder(latent_code)\n",
    "\n",
    "            outputs_real = discriminator(real_data)\n",
    "            outputs_fake = discriminator(reconstructed_data.detach())\n",
    "\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_data.data, reconstructed_data.data)\n",
    "            dis_loss = outputs_fake.mean() - outputs_real.mean() + lambda_gp * gradient_penalty\n",
    "\n",
    "            optimizer_Dis.zero_grad()\n",
    "            dis_loss.backward(retain_graph=True)\n",
    "            optimizer_Dis.step()\n",
    "\n",
    "            # Train the Code Discriminator\n",
    "            outputs_code_real = code_discriminator(latent_code)\n",
    "            outputs_code_fake = code_discriminator(torch.randn(batch_size, latent_dim))\n",
    "\n",
    "            code_loss_real = code_loss(outputs_code_real, real_labels)\n",
    "            code_loss_fake = code_loss(outputs_code_fake, fake_labels)\n",
    "            code_dis_loss = code_loss_real + code_loss_fake\n",
    "\n",
    "            optimizer_CodeDis.zero_grad()\n",
    "            code_dis_loss.backward(retain_graph=True)\n",
    "            optimizer_CodeDis.step()\n",
    "\n",
    "            # Train the Encoder and Decoder (Generator)\n",
    "            outputs_fake = discriminator(reconstructed_data)\n",
    "            gen_loss = outputs_fake.mean()\n",
    "            rec_loss = reconstruction_loss(reconstructed_data, real_data)\n",
    "            total_gen_loss = -gen_loss + rec_loss\n",
    "\n",
    "            optimizer_E.zero_grad()\n",
    "            optimizer_D.zero_grad()\n",
    "            total_gen_loss.backward(retain_graph=True)\n",
    "            optimizer_E.step()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_gen_loss.item()}')\n",
    "\n",
    "    # Generate additional samples\n",
    "    synthetic_data = generate_data(num_samples, decoder, latent_dim)\n",
    "\n",
    "    # Generating labels for synthetic data\n",
    "    synthetic_labels = labels[:num_samples]\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    augmented_labels_list = augmented_labels.tolist()\n",
    "    print('number of ones is:', augmented_labels_list.count(1))\n",
    "    print('number of zeros is:', augmented_labels_list.count(0))\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "    # DataLoader for training\n",
    "    augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inspect the augmented dataset\n",
    "    print(augmented_data.shape)  # Should be (2*num_samples, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (2*num_samples,)\n",
    "\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kj8sF2wuAHR9"
   },
   "source": [
    "using 2 layer GAN complex 2 without gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWZD2dX0ASno"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Residual Block for deeper models\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features, in_features),\n",
    "            nn.BatchNorm1d(in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# Encoder with Residual Blocks\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim1),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim2),\n",
    "            nn.Linear(hidden_dim2, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Decoder with Residual Blocks\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim2, hidden_dim1, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim2),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim1),\n",
    "            nn.Linear(hidden_dim1, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "# More Powerful Discriminator with Residual Blocks and Increased Depth\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            ResidualBlock(hidden_dim1),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            ResidualBlock(hidden_dim2),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            ResidualBlock(hidden_dim3),\n",
    "            nn.Linear(hidden_dim3, hidden_dim4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            ResidualBlock(hidden_dim4),\n",
    "            nn.Linear(hidden_dim4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n",
    "\n",
    "# Code Discriminator with Residual Blocks\n",
    "class CodeDiscriminator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim1, hidden_dim2):\n",
    "        super(CodeDiscriminator, self).__init__()\n",
    "        self.code_discriminator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            ResidualBlock(hidden_dim1),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            ResidualBlock(hidden_dim2),\n",
    "            nn.Linear(hidden_dim2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.code_discriminator(z)\n",
    "\n",
    "# Function to generate synthetic data\n",
    "def generate_data(num_samples, decoder, latent_dim):\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        random_latent_vectors = torch.randn(num_samples, latent_dim)\n",
    "        synthetic_data = decoder(random_latent_vectors)\n",
    "    decoder.train()  # Set decoder back to training mode\n",
    "    return synthetic_data\n",
    "\n",
    "# Data Augmentation Function\n",
    "def augment_data_alpha_gan_2_layer_complex2(sub_flist_samples):\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    input_dim = 1935\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    hidden_dim3 = 128  # Additional hidden layer dimension for a more powerful discriminator\n",
    "    hidden_dim4 = 64   # Another additional hidden layer dimension for a more powerful discriminator\n",
    "    latent_dim = 128\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "\n",
    "    # Instantiate models\n",
    "    encoder = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "    decoder = Decoder(latent_dim, hidden_dim2, hidden_dim1, input_dim)\n",
    "    discriminator = Discriminator(input_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4)\n",
    "    code_discriminator = CodeDiscriminator(latent_dim, hidden_dim1, hidden_dim2)\n",
    "\n",
    "    # Loss functions\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "    code_loss = nn.BCELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    optimizer_D = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    optimizer_Dis = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "    optimizer_CodeDis = optim.Adam(code_discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = [all_corr[f][0].tolist() for f in sub_flist_samples]\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(list_of_labels, dtype=torch.float32)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data, _ in dataloader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Prepare real and fake labels\n",
    "            real_labels = torch.ones(batch_size, 1)\n",
    "            fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "            # Train the Discriminator\n",
    "            latent_code = encoder(real_data)\n",
    "            reconstructed_data = decoder(latent_code)\n",
    "\n",
    "            outputs_real = discriminator(real_data)\n",
    "            outputs_fake = discriminator(reconstructed_data.detach())\n",
    "\n",
    "            dis_loss = outputs_fake.mean() - outputs_real.mean()\n",
    "\n",
    "            optimizer_Dis.zero_grad()\n",
    "            dis_loss.backward(retain_graph=True)\n",
    "            optimizer_Dis.step()\n",
    "\n",
    "            # Train the Code Discriminator\n",
    "            outputs_code_real = code_discriminator(latent_code)\n",
    "            outputs_code_fake = code_discriminator(torch.randn(batch_size, latent_dim))\n",
    "\n",
    "            code_loss_real = code_loss(outputs_code_real, real_labels)\n",
    "            code_loss_fake = code_loss(outputs_code_fake, fake_labels)\n",
    "            code_dis_loss = code_loss_real + code_loss_fake\n",
    "\n",
    "            optimizer_CodeDis.zero_grad()\n",
    "            code_dis_loss.backward(retain_graph=True)\n",
    "            optimizer_CodeDis.step()\n",
    "\n",
    "            # Train the Encoder and Decoder (Generator)\n",
    "            outputs_fake = discriminator(reconstructed_data)\n",
    "            gen_loss = outputs_fake.mean()\n",
    "            rec_loss = reconstruction_loss(reconstructed_data, real_data)\n",
    "            total_gen_loss = -gen_loss + rec_loss\n",
    "\n",
    "            optimizer_E.zero_grad()\n",
    "            optimizer_D.zero_grad()\n",
    "            total_gen_loss.backward(retain_graph=True)\n",
    "            optimizer_E.step()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_gen_loss.item()}')\n",
    "\n",
    "    # Generate additional samples\n",
    "    synthetic_data = generate_data(num_samples, decoder, latent_dim)\n",
    "\n",
    "    # Generating labels for synthetic data\n",
    "    synthetic_labels = labels[:num_samples]\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    augmented_labels_list = augmented_labels.tolist()\n",
    "    print('number of ones is:', augmented_labels_list.count(1))\n",
    "    print('number of zeros is:', augmented_labels_list.count(0))\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "    # DataLoader for training\n",
    "    augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inspect the augmented dataset\n",
    "    print(augmented_data.shape)  # Should be (2*num_samples, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (2*num_samples,)\n",
    "\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn-Of9sPTMNa"
   },
   "source": [
    "using 2 layer gan with out gradient penalty and residual block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-lSFHltTXSi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Encoder without Residual Blocks\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Decoder without Residual Blocks\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim2, hidden_dim1, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "# More Powerful Discriminator without Residual Blocks and Increased Depth\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, hidden_dim3),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim3, hidden_dim4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n",
    "\n",
    "# Code Discriminator without Residual Blocks\n",
    "class CodeDiscriminator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim1, hidden_dim2):\n",
    "        super(CodeDiscriminator, self).__init__()\n",
    "        self.code_discriminator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.code_discriminator(z)\n",
    "\n",
    "# Function to generate synthetic data\n",
    "def generate_data(num_samples, decoder, latent_dim):\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        random_latent_vectors = torch.randn(num_samples, latent_dim)\n",
    "        synthetic_data = decoder(random_latent_vectors)\n",
    "    decoder.train()  # Set decoder back to training mode\n",
    "    return synthetic_data\n",
    "\n",
    "# Data Augmentation Function\n",
    "def augment_data_alpha_gan_2_layer_complex3(sub_flist_samples):\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    input_dim = 1935\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    hidden_dim3 = 128  # Additional hidden layer dimension for a more powerful discriminator\n",
    "    hidden_dim4 = 64   # Another additional hidden layer dimension for a more powerful discriminator\n",
    "    latent_dim = 128\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "\n",
    "    # Instantiate models\n",
    "    encoder = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "    decoder = Decoder(latent_dim, hidden_dim2, hidden_dim1, input_dim)\n",
    "    discriminator = Discriminator(input_dim, hidden_dim1, hidden_dim2, hidden_dim3, hidden_dim4)\n",
    "    code_discriminator = CodeDiscriminator(latent_dim, hidden_dim1, hidden_dim2)\n",
    "\n",
    "    # Loss functions\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "    code_loss = nn.BCELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    optimizer_D = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    optimizer_Dis = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "    optimizer_CodeDis = optim.Adam(code_discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = [all_corr[f][0].tolist() for f in sub_flist_samples]\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(list_of_labels, dtype=torch.float32)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data, _ in dataloader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Prepare real and fake labels\n",
    "            real_labels = torch.ones(batch_size, 1)\n",
    "            fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "            # Train the Discriminator\n",
    "            latent_code = encoder(real_data)\n",
    "            reconstructed_data = decoder(latent_code)\n",
    "\n",
    "            outputs_real = discriminator(real_data)\n",
    "            outputs_fake = discriminator(reconstructed_data.detach())\n",
    "\n",
    "            dis_loss = outputs_fake.mean() - outputs_real.mean()\n",
    "\n",
    "            optimizer_Dis.zero_grad()\n",
    "            dis_loss.backward(retain_graph=True)\n",
    "            optimizer_Dis.step()\n",
    "\n",
    "            # Train the Code Discriminator\n",
    "            outputs_code_real = code_discriminator(latent_code)\n",
    "            outputs_code_fake = code_discriminator(torch.randn(batch_size, latent_dim))\n",
    "\n",
    "            code_loss_real = code_loss(outputs_code_real, real_labels)\n",
    "            code_loss_fake = code_loss(outputs_code_fake, fake_labels)\n",
    "            code_dis_loss = code_loss_real + code_loss_fake\n",
    "\n",
    "            optimizer_CodeDis.zero_grad()\n",
    "            code_dis_loss.backward(retain_graph=True)\n",
    "            optimizer_CodeDis.step()\n",
    "\n",
    "            # Train the Encoder and Decoder (Generator)\n",
    "            outputs_fake = discriminator(reconstructed_data)\n",
    "            gen_loss = outputs_fake.mean()\n",
    "            rec_loss = reconstruction_loss(reconstructed_data, real_data)\n",
    "            total_gen_loss = -gen_loss + rec_loss\n",
    "\n",
    "            optimizer_E.zero_grad()\n",
    "            optimizer_D.zero_grad()\n",
    "            total_gen_loss.backward(retain_graph=True)\n",
    "            optimizer_E.step()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_gen_loss.item()}')\n",
    "\n",
    "    # Generate additional samples\n",
    "    synthetic_data = generate_data(num_samples, decoder, latent_dim)\n",
    "\n",
    "    # Generating labels for synthetic data\n",
    "    synthetic_labels = labels[:num_samples]\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    augmented_labels_list = augmented_labels.tolist()\n",
    "    print('number of ones is:', augmented_labels_list.count(1))\n",
    "    print('number of zeros is:', augmented_labels_list.count(0))\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "    # DataLoader for training\n",
    "    augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inspect the augmented dataset\n",
    "    print(augmented_data.shape)  # Should be (2*num_samples, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (2*num_samples,)\n",
    "\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-QaY2-5YHXD"
   },
   "source": [
    "using Wasserstein Generative Adversarial Networks (WGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANA6GXVYYNiw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim2, hidden_dim1, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2):\n",
    "        super(Critic, self).__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "def gradient_penalty(critic, real_data, fake_data):\n",
    "    batch_size = real_data.size(0)\n",
    "    epsilon = torch.rand(batch_size, 1)\n",
    "    epsilon = epsilon.expand_as(real_data)\n",
    "\n",
    "    interpolated = epsilon * real_data + (1 - epsilon) * fake_data\n",
    "    interpolated = interpolated.requires_grad_(True)\n",
    "\n",
    "    interpolated_scores = critic(interpolated)\n",
    "\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=interpolated_scores,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=torch.ones_like(interpolated_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    return penalty\n",
    "\n",
    "def generate_data(num_samples, decoder, latent_dim):\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_codes = torch.randn(num_samples, latent_dim)\n",
    "        generated_data = decoder(latent_codes)\n",
    "    return generated_data\n",
    "\n",
    "def augment_data_wgan(sub_flist_samples):\n",
    "    # Hyperparameters\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    input_dim = 1935\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    latent_dim = 128\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "    lambda_gp = 10  # Gradient penalty coefficient\n",
    "\n",
    "    # Instantiate models\n",
    "    encoder = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "    decoder = Decoder(latent_dim, hidden_dim2, hidden_dim1, input_dim)\n",
    "    critic = Critic(input_dim, hidden_dim1, hidden_dim2)\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    optimizer_D = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    optimizer_Critic = optim.Adam(critic.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = []\n",
    "    for f in sub_flist_samples:\n",
    "        list_of_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(list_of_labels, dtype=torch.float32)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data, _ in dataloader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Train the Critic more frequently than the Generator\n",
    "            for _ in range(5):\n",
    "                latent_code = encoder(real_data)\n",
    "                reconstructed_data = decoder(latent_code)\n",
    "\n",
    "                real_score = critic(real_data)\n",
    "                fake_score = critic(reconstructed_data.detach())\n",
    "\n",
    "                gp = gradient_penalty(critic, real_data, reconstructed_data.detach())\n",
    "\n",
    "                critic_loss = fake_score.mean() - real_score.mean() + lambda_gp * gp\n",
    "\n",
    "                optimizer_Critic.zero_grad()\n",
    "                critic_loss.backward(retain_graph=True)\n",
    "                optimizer_Critic.step()\n",
    "\n",
    "            # Train the Encoder and Decoder (Generator)\n",
    "            latent_code = encoder(real_data)\n",
    "            reconstructed_data = decoder(latent_code)\n",
    "            fake_score = critic(reconstructed_data)\n",
    "\n",
    "            reconstruction_loss = nn.MSELoss()(reconstructed_data, real_data)\n",
    "            generator_loss = -fake_score.mean() + reconstruction_loss\n",
    "\n",
    "            optimizer_E.zero_grad()\n",
    "            optimizer_D.zero_grad()\n",
    "            generator_loss.backward(retain_graph=True)\n",
    "            optimizer_E.step()\n",
    "            optimizer_D.step()\n",
    "\n",
    "    # Generate additional samples\n",
    "    synthetic_data = generate_data(num_samples, decoder, latent_dim)\n",
    "\n",
    "    # Generating labels for synthetic data\n",
    "    synthetic_labels = labels[:num_samples]\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    augmented_labels_list = augmented_labels.tolist()\n",
    "    print('number of ones is:', augmented_labels_list.count(1))\n",
    "    print('number of zeros is:', augmented_labels_list.count(0))\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "    # DataLoader for training\n",
    "    augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inspect the augmented dataset\n",
    "    print(augmented_data.shape)  # Should be (2*num_samples, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (2*num_samples,)\n",
    "\n",
    "    return augmented_data, augmented_labels\n",
    "\n",
    "# Example usage\n",
    "# sub_flist_samples = your list of samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzm9Pe4qmq0C"
   },
   "source": [
    "using alph_gan 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qd79zkpcmuMC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 4, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 4, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim // 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discriminator(x)\n",
    "\n",
    "class CodeDiscriminator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim):\n",
    "        super(CodeDiscriminator, self).__init__()\n",
    "        self.code_discriminator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim // 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.code_discriminator(z)\n",
    "\n",
    "def generate_data(num_samples, decoder, latent_dim):\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_codes = torch.randn(num_samples, latent_dim)\n",
    "        generated_data = decoder(latent_codes)\n",
    "    return generated_data\n",
    "\n",
    "def augment_data_alpha_gan_4_layer(sub_flist_samples):\n",
    "\n",
    "  # Hyperparameters\n",
    "  num_samples = len(sub_flist_samples)\n",
    "  input_dim = 1935\n",
    "  hidden_dim = 512\n",
    "  latent_dim = 128\n",
    "  dis_hidden_dim = 256\n",
    "  num_epochs = 100\n",
    "  batch_size = 64\n",
    "  learning_rate = 0.0002\n",
    "  step_size = 2  # Number of epochs after which to decrease the learning rate\n",
    "  gamma = 0.5  # Multiplicative factor of learning rate decay\n",
    "\n",
    "  # Instantiate models\n",
    "  encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "  decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "  discriminator = Discriminator(input_dim, dis_hidden_dim)\n",
    "  code_discriminator = CodeDiscriminator(latent_dim, dis_hidden_dim)\n",
    "\n",
    "  # Loss functions\n",
    "  adversarial_loss = nn.BCELoss()\n",
    "  reconstruction_loss = nn.MSELoss()\n",
    "  code_loss = nn.BCELoss()\n",
    "\n",
    "  # Optimizers\n",
    "  optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "  optimizer_D = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "  optimizer_Dis = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "  optimizer_CodeDis = optim.Adam(code_discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "  # Original data and labels\n",
    "  flist1 = [f for f in sub_flist_samples]\n",
    "  array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "  list_of_labels = array_of_labels.tolist()\n",
    "  list_of_data = [all_corr[f][0].tolist() for f in sub_flist_samples]\n",
    "\n",
    "  data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "  labels = torch.tensor(list_of_labels, dtype=torch.float32)\n",
    "\n",
    "  # DataLoader\n",
    "  dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    for real_data, _ in dataloader:\n",
    "        batch_size = real_data.size(0)\n",
    "\n",
    "        # Prepare real and fake labels\n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "\n",
    "        # ==========================\n",
    "        # Train the Discriminator\n",
    "        # ==========================\n",
    "        latent_code = encoder(real_data)\n",
    "        reconstructed_data = decoder(latent_code)\n",
    "\n",
    "        outputs_real = discriminator(real_data)\n",
    "        outputs_fake = discriminator(reconstructed_data.detach())\n",
    "\n",
    "        dis_loss_real = adversarial_loss(outputs_real, real_labels)\n",
    "        dis_loss_fake = adversarial_loss(outputs_fake, fake_labels)\n",
    "\n",
    "        dis_loss = dis_loss_real + dis_loss_fake\n",
    "        optimizer_Dis.zero_grad()\n",
    "        dis_loss.backward(retain_graph=True)\n",
    "        optimizer_Dis.step()\n",
    "\n",
    "        # ==========================\n",
    "        # Train the Code Discriminator\n",
    "        # ==========================\n",
    "        outputs_code_real = code_discriminator(latent_code)\n",
    "        outputs_code_fake = code_discriminator(torch.randn(batch_size, latent_dim))\n",
    "\n",
    "        code_loss_real = code_loss(outputs_code_real, real_labels)\n",
    "        code_loss_fake = code_loss(outputs_code_fake, fake_labels)\n",
    "\n",
    "        code_dis_loss = code_loss_real + code_loss_fake\n",
    "        optimizer_CodeDis.zero_grad()\n",
    "        code_dis_loss.backward(retain_graph=True)\n",
    "        optimizer_CodeDis.step()\n",
    "\n",
    "        # ==========================\n",
    "        # Train the Encoder and Decoder (Generator)\n",
    "        # ==========================\n",
    "        outputs_fake = discriminator(reconstructed_data)\n",
    "        gen_loss = adversarial_loss(outputs_fake, real_labels)\n",
    "\n",
    "        rec_loss = reconstruction_loss(reconstructed_data, real_data)\n",
    "        total_gen_loss = gen_loss + rec_loss\n",
    "        optimizer_E.zero_grad()\n",
    "        optimizer_D.zero_grad()\n",
    "        total_gen_loss.backward(retain_graph=True)\n",
    "        optimizer_E.step()\n",
    "        optimizer_D.step()\n",
    "\n",
    "  # Generate additional samples\n",
    "  synthetic_data = generate_data(num_samples, decoder, latent_dim)\n",
    "\n",
    "  # Generating labels for synthetic data\n",
    "  synthetic_labels = labels[:num_samples]\n",
    "\n",
    "  # Combine original and synthetic data\n",
    "  augmented_data = torch.cat([data, synthetic_data], dim=0)\n",
    "  augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "  return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using AC_GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim2, hidden_dim1, output_dim, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim1),  # Additional layer\n",
    "            nn.ReLU(),                            # Additional layer\n",
    "            nn.Linear(hidden_dim1, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        z = torch.cat((z, labels), dim=1)\n",
    "        return self.decoder(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, hidden_dim2),  # Additional layer\n",
    "            nn.LeakyReLU(0.2),                    # Additional layer\n",
    "        )\n",
    "        self.adv_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.aux_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim2, num_classes),\n",
    "            nn.Softmax(dim=1)  # Adjusted to output probabilities over classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.discriminator(x)\n",
    "        validity = self.adv_layer(features)\n",
    "        label = self.aux_layer(features)\n",
    "        return validity, label\n",
    "\n",
    "class CodeDiscriminator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim1, hidden_dim2):\n",
    "        super(CodeDiscriminator, self).__init__()\n",
    "        self.code_discriminator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.code_discriminator(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "def generate_data(num_samples, decoder, latent_dim):\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_codes = torch.randn(num_samples, latent_dim)\n",
    "        labels = torch.randint(0, 2, (num_samples, 1)).float()\n",
    "        generated_data = decoder(latent_codes, labels)\n",
    "    return generated_data, labels\n",
    "\n",
    "def augment_data_ac_gan(sub_flist_samples):\n",
    "    # Hyperparameters\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    input_dim = 1935\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    latent_dim = 128\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "\n",
    "    # Instantiate models\n",
    "    encoder = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "    decoder = Decoder(latent_dim, hidden_dim2, hidden_dim1, input_dim,2)\n",
    "    discriminator = Discriminator(input_dim, hidden_dim1, hidden_dim2,2)\n",
    "    code_discriminator = CodeDiscriminator(latent_dim, hidden_dim1, hidden_dim2)\n",
    "\n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    auxiliary_loss = nn.BCELoss()\n",
    "    reconstruction_loss = nn.MSELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    optimizer_D = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    optimizer_Dis = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "    optimizer_CodeDis = optim.Adam(code_discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_labels = array_of_labels.tolist()\n",
    "    list_of_data = []\n",
    "    for f in sub_flist_samples:\n",
    "        list_of_data.append(all_corr[f][0].tolist())\n",
    "\n",
    "    data = torch.tensor(list_of_data).float()\n",
    "    labels = torch.tensor(list_of_labels).float().view(-1, 1)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data, real_labels in dataloader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Prepare real and fake labels\n",
    "            valid = torch.ones(batch_size, 1)\n",
    "            fake = torch.zeros(batch_size, 1)\n",
    "\n",
    "            # Train the Discriminator\n",
    "            optimizer_Dis.zero_grad()\n",
    "\n",
    "            # Sample noise and generate fake data\n",
    "            z = torch.randn(batch_size, latent_dim)\n",
    "            gen_labels = torch.randint(0, 2, (batch_size, 1)).float()\n",
    "            fake_data = decoder(z, gen_labels)\n",
    "\n",
    "            real_validity, real_aux = discriminator(real_data)\n",
    "            fake_validity, fake_aux = discriminator(fake_data.detach())\n",
    "\n",
    "            d_loss_real = adversarial_loss(real_validity, valid)\n",
    "            d_loss_fake = adversarial_loss(fake_validity, fake)\n",
    "            aux_loss_real = auxiliary_loss(real_aux, real_labels)\n",
    "            aux_loss_fake = auxiliary_loss(fake_aux, gen_labels)\n",
    "\n",
    "            d_loss = d_loss_real + d_loss_fake + aux_loss_real + aux_loss_fake\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_Dis.step()\n",
    "\n",
    "            # Train the Generator\n",
    "            optimizer_E.zero_grad()\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            gen_validity, gen_aux = discriminator(fake_data)\n",
    "            g_loss = adversarial_loss(gen_validity, valid) + auxiliary_loss(gen_aux, gen_labels)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_E.step()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train the Code Discriminator\n",
    "            optimizer_CodeDis.zero_grad()\n",
    "\n",
    "            code_validity_real = code_discriminator(z)\n",
    "            code_validity_fake = code_discriminator(torch.randn(batch_size, latent_dim))\n",
    "\n",
    "            code_loss_real = auxiliary_loss(code_validity_real, valid)\n",
    "            code_loss_fake = auxiliary_loss(code_validity_fake, fake)\n",
    "\n",
    "            code_dis_loss = code_loss_real + code_loss_fake\n",
    "\n",
    "            code_dis_loss.backward()\n",
    "            optimizer_CodeDis.step()\n",
    "\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {g_loss.item()}')\n",
    "\n",
    "    # Generate additional samples\n",
    "    synthetic_data, synthetic_labels = generate_data(num_samples, decoder, latent_dim)\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    augmented_labels_list = augmented_labels.tolist()\n",
    "    print('Number of ones is:', augmented_labels_list.count(1))\n",
    "    print('Number of zeros is:', augmented_labels_list.count(0))\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "    # DataLoader for training\n",
    "    augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inspect the augmented dataset\n",
    "    print(augmented_data.shape)  # Should be (2*num_samples, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (2*num_samples,)\n",
    "\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using cycle gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "def augment_cyclegan(sub_flist_samples):\n",
    "    class Encoder(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "            super(Encoder, self).__init__()\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim1, hidden_dim2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim2, latent_dim)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.encoder(x)\n",
    "\n",
    "    class Decoder(nn.Module):\n",
    "        def __init__(self, latent_dim, hidden_dim2, hidden_dim1, output_dim):\n",
    "            super(Decoder, self).__init__()\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim2, hidden_dim1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim1, output_dim)\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            return self.decoder(z)\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim1, hidden_dim2):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim1, hidden_dim2),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim2, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "    class CodeDiscriminator(nn.Module):\n",
    "        def __init__(self, latent_dim, hidden_dim1, hidden_dim2):\n",
    "            super(CodeDiscriminator, self).__init__()\n",
    "            self.code_discriminator = nn.Sequential(\n",
    "                nn.Linear(latent_dim, hidden_dim1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim1, hidden_dim2),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(hidden_dim2, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, z):\n",
    "            return self.code_discriminator(z)\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_samples = len(sub_flist_samples)\n",
    "    input_dim = 1935\n",
    "    hidden_dim1 = 512\n",
    "    hidden_dim2 = 256\n",
    "    latent_dim = 128\n",
    "    num_epochs = 100\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0002\n",
    "    lambda_cycle = 20.0\n",
    "\n",
    "    # Instantiate models\n",
    "    G_XY = Decoder(latent_dim, hidden_dim2, hidden_dim1, input_dim)\n",
    "    G_YX = Decoder(latent_dim, hidden_dim2, hidden_dim1, input_dim)\n",
    "    D_X = Discriminator(input_dim, hidden_dim1, hidden_dim2)\n",
    "    D_Y = Discriminator(input_dim, hidden_dim1, hidden_dim2)\n",
    "    E_X = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "    E_Y = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "    Code_D = CodeDiscriminator(latent_dim, hidden_dim1, hidden_dim2)\n",
    "\n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    cycle_consistency_loss = nn.L1Loss()\n",
    "    code_loss = nn.BCELoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(list(G_XY.parameters()) + list(G_YX.parameters()) + list(E_X.parameters()) + list(E_Y.parameters()), lr=learning_rate)\n",
    "    optimizer_D_X = optim.Adam(D_X.parameters(), lr=learning_rate)\n",
    "    optimizer_D_Y = optim.Adam(D_Y.parameters(), lr=learning_rate)\n",
    "    optimizer_Code_D = optim.Adam(Code_D.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Original data and labels\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_data = np.array([all_corr[f][0] for f in sub_flist_samples])\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32)\n",
    "    labels = torch.tensor(array_of_labels, dtype=torch.float32)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_X, _ in dataloader:\n",
    "            batch_size = real_X.size(0)\n",
    "            real_Y = real_X.clone()  # In this case, we use the same data for X and Y domains.\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = torch.ones(batch_size, 1)\n",
    "            fake = torch.zeros(batch_size, 1)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generators and Encoders\n",
    "            # ---------------------\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # GAN loss\n",
    "            encoded_X = E_X(real_X)\n",
    "            fake_Y = G_XY(encoded_X)\n",
    "            loss_GAN_XY = adversarial_loss(D_Y(fake_Y), valid)\n",
    "\n",
    "            encoded_Y = E_Y(real_Y)\n",
    "            fake_X = G_YX(encoded_Y)\n",
    "            loss_GAN_YX = adversarial_loss(D_X(fake_X), valid)\n",
    "\n",
    "            # Cycle consistency loss\n",
    "            recov_X = G_YX(E_Y(fake_Y))\n",
    "            loss_cycle_X = cycle_consistency_loss(recov_X, real_X)\n",
    "\n",
    "            recov_Y = G_XY(E_X(fake_X))\n",
    "            loss_cycle_Y = cycle_consistency_loss(recov_Y, real_Y)\n",
    "\n",
    "            # Total loss for generators and encoders\n",
    "            loss_G = loss_GAN_XY + loss_GAN_YX + lambda_cycle * (loss_cycle_X + loss_cycle_Y)\n",
    "            loss_G.backward(retain_graph=True)\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminators\n",
    "            # ---------------------\n",
    "\n",
    "            # Train D_X\n",
    "            optimizer_D_X.zero_grad()\n",
    "            loss_real_X = adversarial_loss(D_X(real_X), valid)\n",
    "            loss_fake_X = adversarial_loss(D_X(fake_X.detach()), fake)\n",
    "            loss_D_X = (loss_real_X + loss_fake_X) / 2\n",
    "            loss_D_X.backward(retain_graph=True)\n",
    "            optimizer_D_X.step()\n",
    "\n",
    "            # Train D_Y\n",
    "            optimizer_D_Y.zero_grad()\n",
    "            loss_real_Y = adversarial_loss(D_Y(real_Y), valid)\n",
    "            loss_fake_Y = adversarial_loss(D_Y(fake_Y.detach()), fake)\n",
    "            loss_D_Y = (loss_real_Y + loss_fake_Y) / 2\n",
    "            loss_D_Y.backward(retain_graph=True)\n",
    "            optimizer_D_Y.step()\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Code Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            optimizer_Code_D.zero_grad()\n",
    "            real_code = torch.randn(batch_size, latent_dim)\n",
    "            loss_code_real = code_loss(Code_D(real_code), valid)\n",
    "            loss_code_fake = code_loss(Code_D(encoded_X.detach()), fake)\n",
    "            loss_Code_D = (loss_code_real + loss_code_fake) / 2\n",
    "            loss_Code_D.backward()\n",
    "            optimizer_Code_D.step()\n",
    "\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}] Loss D_X: {loss_D_X.item()}, Loss D_Y: {loss_D_Y.item()}, Loss G: {loss_G.item()}')\n",
    "\n",
    "    # Generate synthetic samples\n",
    "    G_XY.eval()\n",
    "    E_X.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_X = E_X(data)\n",
    "        synthetic_data = G_XY(encoded_X).cpu()\n",
    "        synthetic_labels = labels.cpu()\n",
    "\n",
    "    # Combine original and synthetic data\n",
    "    augmented_data = torch.cat([data, synthetic_data], dim=0)\n",
    "    augmented_labels = torch.cat([labels, synthetic_labels], dim=0)\n",
    "\n",
    "    augmented_labels_list = augmented_labels.tolist()\n",
    "    print('number of ones is:', augmented_labels_list.count(1))\n",
    "    print('number of zeros is:', augmented_labels_list.count(0))\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    augmented_dataset = TensorDataset(augmented_data, augmented_labels)\n",
    "\n",
    "    # DataLoader for training\n",
    "    augmented_dataloader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Inspect the augmented dataset\n",
    "    print(augmented_data.shape)  # Should be (2*num_samples, 1935)\n",
    "    print(augmented_labels.shape)  # Should be (2*num_samples,)\n",
    "\n",
    "    return augmented_data, augmented_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using conditional+metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def augment_fmri_data_cw_meta_gp(sub_flist_samples):\n",
    "\n",
    "    num_epochs = 300\n",
    "    batch_size = 128\n",
    "    latent_dim = 128\n",
    "    lambda_gp = 20\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    class Generator(nn.Module):\n",
    "        def __init__(self, latent_dim, metadata_dim, output_dim):\n",
    "            super(Generator, self).__init__()\n",
    "            self.metadata_embedding = nn.Linear(metadata_dim, latent_dim)\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(latent_dim + latent_dim, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(512, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 2048),  # New layer\n",
    "                nn.ReLU(inplace=True),  # New layer\n",
    "                nn.Linear(2048, output_dim),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "        def forward(self, noise, metadata):\n",
    "            m = self.metadata_embedding(metadata)\n",
    "            x = torch.cat((noise, m), dim=1)\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "\n",
    "    class Discriminator(nn.Module):\n",
    "        def __init__(self, input_dim, metadata_dim):\n",
    "            super(Discriminator, self).__init__()\n",
    "            self.metadata_embedding = nn.Linear(metadata_dim, input_dim)\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_dim + input_dim, 1024),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Linear(256, 128),  # New layer\n",
    "                nn.LeakyReLU(0.2, inplace=True),  # New layer\n",
    "                nn.Linear(128, 1),\n",
    "            )\n",
    "\n",
    "        def forward(self, img, metadata):\n",
    "            m = self.metadata_embedding(metadata)\n",
    "            x = torch.cat((img, m), dim=1)\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "\n",
    "    def compute_gradient_penalty(D, real_samples, fake_samples, metadata, device):\n",
    "        alpha = torch.rand(real_samples.size(0), 1).to(device)\n",
    "        alpha = alpha.expand(real_samples.size())\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "        d_interpolates = D(interpolates, metadata)\n",
    "        fake = torch.ones(d_interpolates.size(), requires_grad=False).to(device)\n",
    "        gradients = grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "\n",
    "    input_dim = 1935\n",
    "    metadata_dim = 5\n",
    "    num_samples = len(sub_flist_samples)\n",
    "\n",
    "    # Instantiate models\n",
    "    generator = Generator(latent_dim, metadata_dim, input_dim).to(device)\n",
    "    discriminator = Discriminator(input_dim, metadata_dim).to(device)\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "\n",
    "    # Original data, labels, and metadata\n",
    "    flist1 = [f for f in sub_flist_samples]\n",
    "    array_of_labels = np.array([all_corr[f][1] for f in flist1])\n",
    "    list_of_data = np.array([all_corr[f][0] for f in flist1])\n",
    "    array_of_metadata = np.array([meta_data_dict[f] for f in flist1])\n",
    "\n",
    "    data = torch.tensor(list_of_data, dtype=torch.float32).to(device)\n",
    "    labels = torch.tensor(array_of_labels, dtype=torch.long).to(device)\n",
    "    metadata = torch.tensor(array_of_metadata, dtype=torch.float32).to(device)\n",
    "\n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(TensorDataset(data, metadata, labels), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_data, metadata, labels) in enumerate(dataloader):\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = torch.ones(batch_size, 1).to(device)\n",
    "            fake = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Sample noise and metadata as generator input\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            gen_metadata = metadata\n",
    "\n",
    "            # Generate a batch of data\n",
    "            gen_data = generator(z, gen_metadata)\n",
    "\n",
    "            # Real data\n",
    "            real_validity = discriminator(real_data, metadata)\n",
    "            # Fake data\n",
    "            fake_validity = discriminator(gen_data.detach(), gen_metadata)\n",
    "\n",
    "            # Gradient penalty\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_data.data, gen_data.data, gen_metadata.data, device)\n",
    "\n",
    "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate a batch of data\n",
    "            gen_data = generator(z, gen_metadata)\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            fake_validity = discriminator(gen_data, gen_metadata)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        # Print the progress every 10 epochs\n",
    "        #if (epoch + 1) % 10 == 0:\n",
    "         #   print(f\"Epoch [{epoch+1}/{num_epochs}] D loss: {d_loss.item()} G loss: {g_loss.item()}\")\n",
    "\n",
    "    # Generate synthetic samples\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        gen_metadata = torch.tensor(array_of_metadata, dtype=torch.float32).to(device)\n",
    "        synthetic_data = generator(z, gen_metadata).cpu().numpy()\n",
    "        synthetic_metadata = gen_metadata.cpu().numpy()\n",
    "        synthetic_labels = array_of_labels  # Each synthetic sample has the same label as its corresponding original sample\n",
    "        print(synthetic_labels.shape[0])\n",
    "\n",
    "    # Ensure synthetic_data has the correct shape (num_samples, input_dim)\n",
    "    if synthetic_data.shape[1] != input_dim:\n",
    "        raise ValueError(f\"Generated synthetic_data has incorrect shape: {synthetic_data.shape}. Expected ({num_samples}, {input_dim}).\")\n",
    "\n",
    "    # Concatenate data, metadata, and labels\n",
    "    augmented_data = np.vstack((data.cpu().numpy(), synthetic_data))\n",
    "    augmented_metadata = np.vstack((metadata.cpu().numpy(), synthetic_metadata))\n",
    "    augmented_labels = np.hstack((synthetic_labels, synthetic_labels))\n",
    "    print(augmented_data.shape)\n",
    "    print(augmented_labels.shape)\n",
    "\n",
    "    return augmented_data, augmented_labels\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
